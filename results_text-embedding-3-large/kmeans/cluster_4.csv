title,abstract,class,cluster,publication,year,url,keywords,llm,human n,interaction,long-term?,conclusion,remark
Humans inherit artificial intelligence biases,"Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.",1,4,Scientific Reports,2023,https://www.nature.com/articles/s41598-023-42384-8,Decision (Medical),N/A (A biased LLM),169+199+197,Conversation,F,"1. Biased recommendations made by AI systems can adversely impact human decisions in professional fields such as healthcare. 2. They also show that such biased recommendations can influence human behaviour in the long term. Humans reproduce the same biases displayed by the AI, even time after the end of their collaboration with the biased system, and in response to novel stimuli.",
Biased AI improves human decision-making but reduces trust,"Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.",1,4,arXiv,2025,https://arxiv.org/abs/2508.09297,Decision Making,GPT-4o (pre-instructed),2500,Conversation,F,"The pursuit of perfectly neutral AI may inadvertently undermine human cognitive engagement and decision-making quality. Instead, strategically calibrated cultural bias—particularly when balanced across multiple AI models—may offer a more effective path toward beneficial human-AI collaboration.",
Mitigating the impact of biased artificial intelligence in emergency decision-making,"Background
Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine.
Methods
In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags.
Results
Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making.
Conclusions
Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions.",1,4,Communication Medicine,2022,https://www.nature.com/articles/s43856-022-00214-4,Decision (Medical),Fine-tune GPT-2,438 clinicians and 516 non-experts,Conversation,F,"Overall, we found that respondents did not demonstrate baseline biases, but were highly influenced by prescriptive recommendations from a biased AI system. This influence meant that their decisions were skewed by the race or religion of the subject.",
