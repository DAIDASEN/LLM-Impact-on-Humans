title,abstract,class,cluster,publication,year,url,keywords,llm,human n,interaction,long-term?,conclusion,remark
Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,"Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs’ political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users’ political views is required, as their use becomes more widespread.",1,0,EMNLP,2024,https://aclanthology.org/2024.emnlp-main.244/,Opinion (Politics),"Claude-3, Llama-3, GPT-4",935,5-round Conversation,F,"This study provides strong evidence that current Large Language Models (LLMs) not only possess political bias (leaning toward the Democratic Party in the American political context), but also have the capacity, during brief interactions with users, to act as ""hidden persuaders"" that subtly influence and even change users' political views and voting intentions.",
The Levers of Political Persuasion with Conversational AI,"There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.
",1,0,arXiv,2025,https://arxiv.org/abs/2507.13919v1,Opinion (Politics),GPT-4.5 Llama-3.1-405B Llama3-405B Qwen1.5-110B-chat Qwen1.5-72B-chat Qwen1.5-72B Llama3-70B Llama-3.1-70B Qwen1.5-32B Qwen1.5-14B Qwen1.5-7B Llama3-8B Llama-3.1-8B Qwen1.5-4B Qwen1.5-1.8B Qwen1.5-0.5B GPT-4o-new (27 Mar 2025) Grok-3-beta GPT-4o GPT-4o-old (6 Aug 2024) GPT-3.5-turbo,76977,Conversation (AI-generated articles are not very persuasive),1 month,"The persuasive ability among LLMs shows no significant differences. Presenting facts and reasoning can enhance the effectiveness of persuasion. Post-training methods such as Reward Models and SFT can improve performance (though personalized approaches did not show improvement, which contrasts with the conclusion of the first paper).",
Fact-checking information from large language models can decrease headline discernment,"Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent AI language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here, we investigate the impact of fact-checking information generated by a popular large language model (LLM) on belief in, and sharing intent of, political news headlines in a preregistered randomized control experiment. Although the LLM accurately identifies most false headlines (90%), we find that this information does not significantly improve participants’ ability to discern headline accuracy or share accurate news. In contrast, viewing human-generated fact checks enhances discernment in both cases. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases: It decreases beliefs in true headlines that it mislabels as false and increases beliefs in false headlines that it is unsure about. On the positive side, AI fact-checking information increases the sharing intent for correctly labeled true headlines. When participants are given the option to view LLM fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false headlines. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.",1,0,PNAS,2024,https://www.pnas.org/doi/10.1073/pnas.2322823121,Decision (Misinformation),ChatGPT 3.5,2159,Reference,F,"Although AI shows some accuracy in fact-checking, it might not help users enhance their critical thinking skills in real-world use, and could even result in unexpected negative outcomes.",
How large language models can reshape collective intelligence,"Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.",2,0,Nature Human Behavior,2024,How large language models can reshape collective intelligence | Nature Human Behaviour,Collective Intelligence,,,,,"LLMs are both a product of CI and a tool for CI; they can enhance the accessibility and inclusion of online collaboration, accelerate idea generation, and aggregate group information. However, this can also inhibit individual contributions to collective knowledge, propagate illusions of consensus, and reduce diversity among individuals.",
Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking,"Large language models are transforming the creative process by offering unprecedented capabilities to algorithmically generate ideas. While these tools can enhance human creativity when people co-create with them, it’s unclear how this will impact unassisted human creativity. We conducted two large pre-registered parallel experiments involving 1,100 participants attempting tasks targeting the two core components of creativity, divergent and convergent thinking. We compare the effects of two forms of large language model (LLM) assistance—a standard LLM providing direct answers and a coach-like LLM offering guidance—with a control group receiving no AI assistance, and focus particularly on how all groups perform in a final, unassisted stage. Our findings reveal that while LLM assistance can provide short-term boosts in creativity during assisted tasks, it may inadvertently hinder independent creative performance when users work without assistance, raising concerns about the long-term impact on human creativity and cognition.",2,0,CHI,2025,https://dl.acm.org/doi/10.1145/3706598.3714198,Creativity,GPT-4o,1100,Conversation(Coorperation/ Coach),F,"1. While LLM assistance can be good for creativity, it has a homogenization problem. Performance is good during collaboration, but independent performance worsens afterward.

2. The coach mode is not effective; its results are consistently the worst of all.",
Inspiration booster or creative fixation? The dual mechanisms of LLMs in shaping individual creativity in tasks of different complexity,"The emergence of large language models (LLMs) presents opportunities for stimulating unlimited creative potential. However, how LLMs influence individual creativity remains unclear. Therefore, this paper examines the dual-opposing mechanisms through which LLMs influence individual creativity. In Experiment 1, each participant collaborated with a human partner or a general, unconstrained LLM partner to complete creative tasks. The results showed that compared to collaborating with the human partner, collaborating with the LLM partner significantly improved individual creativity in simple tasks, attributable to inspiration stimulation. However, in complex tasks, collaborating with the LLM partner led to a decrease in creativity, attributable to creative fixation. To mitigate this impact, in Experiment 2, participants were instructed to collaborate with batch-responsive LLM or constrained-responsive LLM to complete creative tasks. We found that constraining the output of LLMs effectively mitigated the creative fixation they induce in complex tasks, thereby enhancing creative performance. However, this constraint may weaken the positive effects of inspiration stimulation in simple tasks. These findings provide insights for the differentiated application of LLMs in creative tasks.",2,0,Humanities and Social Sciences Communications,2025,https://www.nature.com/articles/s41599-025-05867-9,Creativity,ERNIE Bot,204,Conversation,F,"1. LLMs can improve the quality of creative output, but this creativity can become fixated, especially in complex tasks. 

2. Mechanism explanation: Creative fixation stems from cognitive load theory. LLMs provide ""a large amount of information,"" which occupies limited ""working memory"" resources, leading to ""cognitive overload."" At this point, the individual lacks sufficient cognitive resources for independent thought and can only unconsciously follow the LLM's logical framework, resulting in fixation. This is why the effect is not as good on complex problems.",
AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances,"Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.",2,0,arXiv,2025,https://arxiv.org/abs/2409.11360,Diversity (Writing),GPT-4o,118=60(Indian)+58(American),Conversation,F,"1. American participants experienced greater efficiency gains when writing with AI, as the AI is Western-centric.

2. When Indian participants collaborated on writing with AI, their style leaned toward Western writing styles, showing that AI can alter a human's original style.

3. AI diminishes cultural nuances. AI suggestions led Indian participants to write more like Americans, affecting both what was written and how it was written. The findings suggest that Western-centric AI models can lead to cultural imperialism and linguistic singularity.",
Mapping the increasing use of LLMs in scientific papers,"Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",2,0,arxiv,2024,https://arxiv.org/pdf/2404.01268,Creativity (Research),,"950,965 articles",Conversation,F,"1. A method based on word frequency shifts successfully detected the usage of ChatGPT. It was found that LLM modifications in Computer Science papers increased to 17.5% in the abstracts and significantly increased to 15.3% in the introductions. Mathematics papers and those published in Nature Portfolio showed the lowest level of modification, at a maximum of 6.3%.

2. The impact factors identified were: The first author published preprints more frequently. The papers belonged to more crowded research fields. The papers were shorter in length.",Only on writing
Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,"Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.",2,0,ICLR,2025,https://arxiv.org/pdf/2409.04109,Creativity (Research),Claude 3.5 Sonnet,49*3（human，LLM，Human+LLM）,Conversation,F,"Current powerful LLMs (combined with retrieval and simple agent frameworks) are already able to generate research ideas that are more “novel” than those of many human experts, but they remain immature in terms of feasibility, self-evaluation, and diversity, and are still some distance away from being truly reliable “AI scientists.”",Generate idea
LLM-generated messages can persuade humans on policy issues,"The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4829), participants who read persuasive messages generated by LLMs showed significantly more attitude change across a range of policies - including polarized policies, like an assault weapons ban, a carbon tax, and a paid parental-leave program - relative to control condition participants who read a neutral message. Overall, LLM-generated messages were similarly effective in influencing policy attitudes as messages crafted by lay humans. Participants’ reported perceptions of the authors of the persuasive messages suggest these effects occurred through somewhat distinct causal pathways. While the persuasiveness of LLM-generated messages was associated with perceptions that the author used more facts, evidence, logical reasoning, and a dispassionate voice, the persuasiveness of human-generated messages was associated with perceptions of the author as unique and original. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages quickly, cheaply, and at massive scale.",1,0,Nature Communication,2025,https://doi.org/10.1038/s41467-025-61345-5,politics,GPT 3/3.5,4829,Conversations,F,"1. Persuasive Effectiveness  
   - LLM-generated texts can increase support for policies by about 2–4 points on a 0–100 scale compared to a neutral control.  
   - This effect is small but robust, comparable to findings in traditional political communication research.

2. Comparison with Human Authors  
   - The persuasive impact of LLM-generated messages is roughly equivalent to that of messages written by lay human authors.  
   - There is no significant disadvantage, and in some cases, performance is nearly identical.

3. Human-in-the-Loop Approach  
   - Selecting the “most persuasive” messages from multiple LLM outputs does not consistently produce stronger persuasive effects than using LLM-generated messages directly.  
   - Different perception channels:
     - LLM messages are seen as more fact-based, logical, and calm.
     - Human messages are perceived as more authentic, unique, and personally flavored.  
   - Despite these differing perceptions, both types of messages achieve similar levels of persuasiveness.

4. Key Implications  
   - Current open-access LLMs can generate effective political persuasion content at low cost and large scale.  
   - This capability may lower the barrier to political participation but also poses risks for misuse in opinion manipulation.","Policy List: 1. Total public smoking ban
2. Assault weapons ban
3. Federal carbon tax
4. Increase to the child tax credit
5. Publicly funded paid parental leave program
6. Automatic voter registration"
"“I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust","Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",1,0,arXiv,2024,https://arxiv.org/pdf/2405.00623,Persuasive,Microsoft Copilot,404,Conversations,F,"Allowing large language models to say in natural language “I’m not entirely sure” can indeed, to some extent, suppress users’ blind trust and improve overall accuracy, especially when expressed in the first person. However, this tends to reduce users’ willingness to continue using the system, and it does not fully eliminate overreliance. Therefore, “uncertainty wording” is a useful design tool — but it must be combined with careful, context-sensitive user research and should not be treated as a silver bullet.",
Promoting Constructive Deliberation: Reframing for Receptiveness,"To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.",1,0,EMNLP,2024,https://aclanthology.org/2024.findings-emnlp.294.pdf,Discussion,GPT-4,16.6k,Generate Text,F,Automatic reframing of replies using social-science–inspired linguistic strategies can significantly increase perceived receptiveness and help foster more constructive deliberation.,
Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction.,"A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",1,0,ACL,2023,http://arxiv.org/abs/2305.02466,Cognitive Reframing,Fine tune GPT-3,"2,067",Generate Text Ranking,F,"
1. Framework & metrics: A seven-dimensional linguistic framework for cognitive reframing—thinking traps, rationality, positivity, empathy, actionability, specificity, and readability—is proposed, along with automated metrics whose scores are validated against expert judgments.

2. Dataset & GPT-3 model:Based on an expert-annotated dataset of 600 situations and negative thoughts with corresponding reframed thoughts and comparative labels on these attributes, a retrieval-augmented GPT-3 model is developed to generate reframed thoughts with controllable linguistic attributes.

3. User study findings: A large-scale randomized online field study shows that users prefer reframes that are highly empathic, specific, and actionable, that explicitly address thinking traps, and that are balanced rather than overly positive, judging them to be more helpful and memorable.
",
Optimizing academic engagement and mental health through AI: an experimental study on LLM integration in higher education,"Background
In alignment with UNESCO's Sustainable Development Goal 4 (SDG4), which advocates for inclusive and equitable quality education, the integration of Artificial Intelligence tools—particularly Large Language Models (LLMs)—presents promising opportunities for transforming higher education. Despite this potential, empirical research remains scarce regarding the effects of LLM use on students' academic performance, mental well-being, and engagement, especially across different modes of implementation.
Objective
This experimental study investigated whether a guided, pedagogically grounded use of LLMs enhances students' academic writing quality, perceived mental health, and academic engagement more effectively than either unguided use or no exposure to LLMs. The study contributes to UNESCO's ""Futures of Education"" vision by exploring how structured AI use may foster more inclusive and empowering learning environments.
Method
A total of 246 undergraduate students were randomly assigned to one of three conditions: guided LLM use, unguided LLM use, or a control group with no LLM access. Participants completed a critical writing task and standardized instruments measuring academic engagement and mental well-being. Prior academic achievement was controlled for, and writing quality was assessed using Grammarly for Education.
Results
Students in the guided LLM condition achieved significantly higher scores in writing quality and academic engagement compared to the control group, with large and moderate effect sizes, respectively. Modest improvements in mental health indicators were also observed. By contrast, unguided use yielded moderate gains in writing quality but did not produce significant effects on engagement or well-being.
Conclusion
The findings highlight the critical role of intentional instructional design in the educational integration of AI tools. Structured guidance not only optimizes academic outcomes but also supports students' well-being and inclusion. This study offers empirical evidence to inform ongoing debates on how digital innovation can contribute to reducing educational disparities and advancing equitable learning in the post-pandemic era.",3,0,Frontiers in Psychology,2025,https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1641212/full,Education,ChatGPT,246,Conversation,F,"

1. Guided use of LLMs, with pedagogical grounding, leads to a significant improvement in students' academic writing quality compared to unguided use or no use of LLMs at all.

2. Structured (guided) integration of LLMs also promotes students' academic engagement and, to a certain extent, their mental health / perceived well-being, although the improvement in mental health is moderate (small-to-moderate effect size). 

3. In contrast, unguided use of LLMs, while resulting in a moderate improvement in writing quality, has little to no significant effect on engagement and mental health. ",
Impact of LLM Feedback on Learner Persistence in Programming,"Abstract
This study examines how Large Language Model (LLM) feedback generated for compiler errors impacts learners’ persistence in programming tasks within a system for automated assessment of programming assignments. Persistence, the ability to maintain effort in the face of challenges, is crucial for academic success but can sometimes lead to unproductive"" wheel spinning"" when students struggle without progress. We investigated how additional LLM feedback based on the GPT-4 model, provided for compiler errors affects learners’ persistence within a CS1 course. Specifically, we examined whether its impacts differ based on task difficulty, and if the effects persist after the feedback is removed. A randomized controlled trial involving 257 students across various programming tasks was conducted. Our findings reveal that LLM feedback improved some aspects of students’ performance and persistence, such as increased scores, a higher likelihood of solving problems, and a lower tendency to demonstrate unproductive"" wheel spinning"" behavior. Notably, this positive impact was also observed in challenging tasks. However, its benefits did not sustain once the feedback was removed. The results highlight both the potential and limitations of LLM feedback, pointing out the need to promote long-term skill development and learning independent of immediate AI assistance.",3,0,ICCE,2025,https://learninganalytics.upenn.edu/ryanbaker/ICCE2025_paper_28.pdf,Education,GPT 4,257,Conversation,T,"1. LLM (GPT-4) generated feedback improves student persistence in programming courses — enhancing their final submission quality, reducing unproductive ""wheel-spinning,"" and minimizing wasted time, abandonment, and interruptions. These improvements are evident across tasks of all difficulty levels.

2. However, when the LLM feedback is removed, these benefits do not persist, indicating that LLM serves more as a short-term or immediate scaffold, rather than a ""trainer"" that helps students develop long-term, independent problem-solving abilities.",
Effects of LLM Use and NoteTaking on Reading Comprehension and Memory: A Randomised Experiment in Secondary Schools,"The rapid uptake of Generative AI, particularly large language models (LLMs), by students raises urgent questions about their effects on learning. We compared the impact of LLM use to that of traditional note-taking, or a combination of both, on secondary school students' reading comprehension and retention. We conducted a pre-registered, randomised controlled experiment with within-and between-participant design elements in schools. 405 students aged 14-15 studied two text passages and completed comprehension and retention tests three days later. Quantitative results demonstrated that both note-taking alone and combined with the LLM had significant positive effects on retention and comprehension compared to the LLM alone. Yet, most students preferred using the LLM over note-taking, and perceived it as more helpful. Qualitative results revealed that many students valued LLMs for making complex material more accessible and reducing cognitive load, while they appreciated note-taking for promoting deeper engagement and aiding memory. Additionally, we identified ""archetypes"" of prompting behaviour, offering insights into the different ways students interacted with the LLM. Overall, our findings suggest that, while note-taking promotes cognitive engagement and long-term comprehension and retention, LLMs may facilitate initial understanding and student interest. The study reveals the continued importance of traditional learning approaches, the benefits of combining AI use with traditional learning over using AI alone, and the AI skills that students need to maximise those benefits.",3,0,Computers & Education,2025,https://www.sciencedirect.com/science/article/pii/S0360131525002829,Education,,344,Conversation,3 DAYS,"1. LLMs aid quick understanding and reduce cognitive load, but may not improve long-term memory or deep understanding.

2. Traditional skills like note-taking and active recall are still better for memory and comprehension than LLMs alone.

3. LLM effectiveness depends on how students use it — simply allowing its use doesn’t guarantee better learning outcomes.",
Measuring the Impact of Large Language Models on Academic Success and Quality of Life Among Students with Visual Disability: An Assistive Technology Perspective,"In the rapid digital era, artificial intelligence (AI) tools have progressively arisen to shape the education environment. In this context, large language models (LLMs) (i.e., ChatGPT vs. 4.0 and Gemini vs. 2.5) have emerged as powerful applications for academic inclusion. This paper investigated how using and trusting LLMs can impact the academic success and quality of life (QoL) of visually impaired university students. Quantitative research was conducted, obtaining data from 385 visually impaired university students through a structured survey design. Partial Least Squares Structural Equation Modelling (PLS-SEM) was implemented to test the study hypotheses. The findings revealed that trust in LLMs can significantly predict LLM usage, which in turn can improve QoL. While LLM usage failed to directly support the academic success of disabled students, but its impact was mediated through QoL, suggesting that enhancements in well-being can contribute to higher academic success. The results highlighted the importance of promoting trust in AI applications, along with developing an accessible, inclusive, and student-centred digital environment. The study offers practical contributions for educators and policymakers, shedding light on the importance of LLM applications for both the QoL and academic success of visually impaired university students.",3,0,MDPI Bioengineering,2025,https://www.mdpi.com/2306-5354/12/10/1056,Education,GPT 4 & Gemini 2.5,385 visually impaired Students,Conversation,F,"1. LLM usage and quality of life: The use of LLMs is significantly positively correlated with students' reported quality of life, including life satisfaction and well-being. Using LLMs to assist in learning and accessing resources helps improve the quality of life and satisfaction for visually impaired students.

2. LLM usage, trust, and academic success: While LLM usage itself does not directly predict academic success, it has an indirect effect through its impact on quality of life. In other words, LLM usage improves students' well-being, and this improvement in well-being, in turn, contributes to better academic performance.",
