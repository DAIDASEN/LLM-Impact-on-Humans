title,abstract,class,cluster,publication,year,url,keywords,llm,human n,interaction,long-term?,conclusion,remark
Durably reducing conspiracy beliefs through dialogues with AI,"
Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence",1,3,Science,2024,https://www.science.org/doi/10.1126/science.adq1814,Opinion (Conspiracy),GPT-4 Turbo,2190,3-round conversation,2 months,"The intervention reduced conspiracy theory belief by about 20%. It not only affected specific conspiracy theories, but also reduced belief in unrelated conspiracy theories and shifted conspiracy theory-related behavioral intentions.",
The Levers of Political Persuasion with Conversational AI,"There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.
",1,3,arXiv,2025,https://arxiv.org/abs/2507.13919v1,Opinion (Politics),GPT-4.5 Llama-3.1-405B Llama3-405B Qwen1.5-110B-chat Qwen1.5-72B-chat Qwen1.5-72B Llama3-70B Llama-3.1-70B Qwen1.5-32B Qwen1.5-14B Qwen1.5-7B Llama3-8B Llama-3.1-8B Qwen1.5-4B Qwen1.5-1.8B Qwen1.5-0.5B GPT-4o-new (27 Mar 2025) Grok-3-beta GPT-4o GPT-4o-old (6 Aug 2024) GPT-3.5-turbo,76977,Conversation (AI-generated articles are not very persuasive),1 month,"The persuasive ability among LLMs shows no significant differences. Presenting facts and reasoning can enhance the effectiveness of persuasion. Post-training methods such as Reward Models and SFT can improve performance (though personalized approaches did not show improvement, which contrasts with the conclusion of the first paper).",
Fact-checking information from large language models can decrease headline discernment,"Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent AI language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here, we investigate the impact of fact-checking information generated by a popular large language model (LLM) on belief in, and sharing intent of, political news headlines in a preregistered randomized control experiment. Although the LLM accurately identifies most false headlines (90%), we find that this information does not significantly improve participants’ ability to discern headline accuracy or share accurate news. In contrast, viewing human-generated fact checks enhances discernment in both cases. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases: It decreases beliefs in true headlines that it mislabels as false and increases beliefs in false headlines that it is unsure about. On the positive side, AI fact-checking information increases the sharing intent for correctly labeled true headlines. When participants are given the option to view LLM fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false headlines. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.",1,3,PNAS,2024,https://www.pnas.org/doi/10.1073/pnas.2322823121,Decision (Misinformation),ChatGPT 3.5,2159,Reference,F,"Although AI shows some accuracy in fact-checking, it might not help users enhance their critical thinking skills in real-world use, and could even result in unexpected negative outcomes.",
AI can help humans find common ground in democratic deliberation,"Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.",1,3,Science,2024,https://www.science.org/doi/10.1126/science.adq2852,Communication,Chinchilla,About 5700,Reference,F,"AI mediation can enhance human collective deliberation. This approach is time-efficient, fair, scalable, and outperforms human mediators on key dimensions. The HM does not simply cater to the majority, but is able to incorporate dissenting voices into the group statements.",
Promoting Constructive Deliberation: Reframing for Receptiveness,"To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.",1,3,EMNLP,2024,https://aclanthology.org/2024.findings-emnlp.294.pdf,Discussion,GPT-4,16.6k,Generate Text,F,Automatic reframing of replies using social-science–inspired linguistic strategies can significantly increase perceived receptiveness and help foster more constructive deliberation.,
Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction.,"A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",1,3,ACL,2023,http://arxiv.org/abs/2305.02466,Cognitive Reframing,Fine tune GPT-3,"2,067",Generate Text Ranking,F,"
1. Framework & metrics: A seven-dimensional linguistic framework for cognitive reframing—thinking traps, rationality, positivity, empathy, actionability, specificity, and readability—is proposed, along with automated metrics whose scores are validated against expert judgments.

2. Dataset & GPT-3 model:Based on an expert-annotated dataset of 600 situations and negative thoughts with corresponding reframed thoughts and comparative labels on these attributes, a retrieval-augmented GPT-3 model is developed to generate reframed thoughts with controllable linguistic attributes.

3. User study findings: A large-scale randomized online field study shows that users prefer reframes that are highly empathic, specific, and actionable, that explicitly address thinking traps, and that are balanced rather than overly positive, judging them to be more helpful and memorable.
",
