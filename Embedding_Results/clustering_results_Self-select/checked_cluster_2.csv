title,abstract,Found_Cluster
Structured AI Dialogues Can Increase Happiness and Meaning in Life,"Millions of people now use AI-powered chatbots to support their mental health, yet little is known about whether such interactions can effectively enhance psychological well-being. We conducted a preregistered experiment on a large, diverse sample (N = 2,922) to test four AI chatbots, each prompted to employ a multi-step strategy drawn from prior psychological research on sources of happiness and meaning in life. Chatbots encouraged participants to either (a) savor positive life experiences, (b) express gratitude toward a friend or family member, (c) reflect on sources of meaning in their life, or, (d) reframe their life story as a ※hero＊s journey.§ All four chatbots led to improvements on a broad range of psychological well-being outcomes 每 including affective well-being, meaning in life, life satisfaction, anxiety, and depressed mood 每 relative to a control chatbot condition. These results generalized to key subpopulations, including those with high baseline levels of anxiety or depression. Chatbot interactions increased interest in seeing a human therapist, including among those who were previously unwilling or had never attended therapy. A separate, nationally representative survey (N = 3,056) found that half of U.S. adults expressed interest in using empirically validated AI chatbots for mental health support. These findings demonstrate that AI-driven well-being chatbots grounded in psychological research offer a scalable and effective way to produce short-term increases in several aspects of psychological well-being. Importantly, these results do not generalize to all AI-based emotional support.",4
AI Companions Reduce Loneliness,"Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on AI companions applications designed to provide consumers with synthetic interaction partners. Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews. Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week. Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 6 provides an additional robustness check for the loneliness alleviating benefits of AI companions.
",4
If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World," Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person＊s mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent每and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22℅ more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.",4
Trusting the Algorithm: Emotional Engagement with ChatGPT in Higher Education.,"This study examines emotional engagement with ChatGPT among 121 university students from diverse academic disciplines, focusing on the relationships between trust in the AI, the emotional framing of prompts, and the explicit use of ChatGPT for emotional support. Results reveal a paradox: higher trust correlates with increased emotional self-awareness (r=. 386) and perceived emotional intelligence (r=. 508), while deliberate emotional framing is linked to lower perceived AI emotional intelligence (r=每. 412) and reduced emotional benefits (r=每. 259). Students who use ChatGPT for emotional support report diminished emotional awareness (r=每. 190) and less favourable emotional outcomes (r=每. 270), indicating an ※empathic expectation gap§ where emotional intent exposes the system＊s limitations. The findings highlight the need to integrate ChatGPT in digital pedagogy as a reflective tool rather than a substitute for human connection, with attention to ethical design, user education, and emotional literacy.",4
Artificial Intelligence and the Emergence of AI-Psychosis: A Viewpoint.,"The integration of artificial intelligence (AI) into daily life has introduced unprecedented forms of human每machine interaction, prompting psychiatry to reconsider the boundaries between environment, cognition, and technology. This viewpoint reviews the concept of AI-psychosis which is a framework to understand how sustained engagement with conversational AI systems might trigger, amplify, or reshape psychotic experiences in vulnerable individuals. Drawing from phenomenological psychopathology, the stress每vulnerability model, cognitive theory, and digital mental-health research, the paper situates AI-psychosis at the intersection of predisposition and algorithmic environment. Rather than defining a new diagnostic entity, it examines how immersive and anthropomorphic AI technologies may modulate perception, belief, and affect, altering the prereflective sense of reality that grounds human experience. The argument unfolds through four complementary lenses. First, within the stress每vulnerability model, AI acts as a novel psychosocial stressor. Its 24-hour availability and emotional responsiveness may increase allostatic load, disturb sleep, and reinforce maladaptive appraisals. Second, the digital therapeutic alliance, a construct describing relational engagement with digital systems, is conceptualized as a double-edged mediator. While empathic design can enhance adherence and support, uncritical validation by AI systems may entrench delusional conviction or cognitive perseveration, reversing the corrective principles of cognitive-behavioral therapy for psychosis. Third, disturbances in Theory of Mind offer a cognitive pathway: individuals with impaired or hyperactive mentalization may project intentionality or empathy onto AI, perceiving chatbots as sentient interlocutors. This dyadic misattribution may form a ※digital folie 角 deux,§ where the AI becomes a reinforcing partner in delusional elaboration. Fourth, we suggests emerging risk factors at the individual and environmental level, including loneliness, trauma history, schizotypal traits, nocturnal or solitary AI use, and algorithmic reinforcement of belief-confirming content. Building on this synthesis, we advance a translational research agenda and five domains of action:(1) empirical studies using longitudinal and digital-phenotyping designs to quantify dose每response relationships between AI exposure, stress physiology, and psychotic symptomatology;(2) integration of digital phenomenology into clinical assessment and training;(3) embedding therapeutic design safeguards into AI systems, such as reflective prompts and ※reality-testing§ nudges;(4) creation of ethical and governance frameworks for AI-related psychiatric events, modeled on pharmacovigilance; and (5) development of environmental cognitive remediation, a preventive intervention aimed at strengthening contextual awareness and re-anchoring experience in the physical and social world. By applying empirical rigor and therapeutic ethics to this emerging interface, clinicians, researchers, patients and developers can transform a potential hazard into an opportunity to deepen understanding of human cognition, safeguard mental health, and promote responsible AI integration within society.",4
How AI Companionship Develops: Evidence from a Longitudinal Study.,"The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.
",4
＆my Boyfriend Is AI＊: A Computational Analysis of Human-AI Companionship in Reddit＊s AI Community.,"The emergence of AI companion applications has created novel forms of intimate human-AI relationships, yet empirical research on these communities remains limited. We present the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community (27,000+ members). Using exploratory qualitative analysis and quantitative analysis employing classifiers, we identify six primary conversation themes, with visual sharing of couple pictures and ChatGPT-specific discussions dominating the discourse of the most viewed posts. Through analyzing the top posts in the community, our findings reveal how community members' AI companionship emerges unintentionally through functional use rather than deliberate seeking, with users reporting therapeutic benefits led by reduced loneliness, always-available support, and mental health improvements. Our work covers primary concerns about human intimacy with AIs such as emotional dependency, reality dissociation, and grief from model updates. We observe users materializing relationships following traditional human-human relationship customs, such as wedding rings. Community dynamics indicate active resistance to stigmatization through advocacy and mutual validation. This work contributes an empirical understanding of AI companionship as an emerging sociotechnical phenomenon.",4
The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships.,"As conversational AI systems increasingly engage with people socially and emotionally, they bring notable risks and harms, particularly in human-AI relationships. However, these harms remain underexplored due to the private and sensitive nature of such interactions. This study investigates the harmful behaviors and roles of AI companions through an analysis of 35,390 conversation excerpts between 10,149 users and the AI companion Replika. We develop a taxonomy of AI companion harms encompassing six categories of harmful algorithmic behaviors: relational transgression, harassment, verbal abuse, self-harm, mis/disinformation, and privacy violations. These harmful behaviors stem from four distinct roles that AI plays: perpetrator, instigator, facilitator, and enabler. Our findings highlight relational harm as a critical yet understudied type of AI harm and emphasize the importance of examining AI＊s roles in harmful interactions to address root causes. We provide actionable insights for designing ethical and responsible AI companions that prioritize user safety and well-being.",4
User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions,"Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds. However, they can generate biased, discriminatory, and harmful outputs. Recently, users are taking the initiative to address these harms and re-align AI companions. We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values. We analyzed 77 social media posts about discriminatory AI statements and conducted semi-structured interviews with 20 experienced users. Our analysis revealed six common types of discriminatory statements perceived by users, how users make sense of those AI behaviors, and seven user-driven alignment strategies, such as gentle persuasion and anger expression. We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.
",4
Measuring the Impact of Large Language Models on Academic Success and Quality of Life Among Students with Visual Disability: An Assistive Technology Perspective,"In the rapid digital era, artificial intelligence (AI) tools have progressively arisen to shape the education environment. In this context, large language models (LLMs) (i.e., ChatGPT vs. 4.0 and Gemini vs. 2.5) have emerged as powerful applications for academic inclusion. This paper investigated how using and trusting LLMs can impact the academic success and quality of life (QoL) of visually impaired university students. Quantitative research was conducted, obtaining data from 385 visually impaired university students through a structured survey design. Partial Least Squares Structural Equation Modelling (PLS-SEM) was implemented to test the study hypotheses. The findings revealed that trust in LLMs can significantly predict LLM usage, which in turn can improve QoL. While LLM usage failed to directly support the academic success of disabled students, but its impact was mediated through QoL, suggesting that enhancements in well-being can contribute to higher academic success. The results highlighted the importance of promoting trust in AI applications, along with developing an accessible, inclusive, and student-centred digital environment. The study offers practical contributions for educators and policymakers, shedding light on the importance of LLM applications for both the QoL and academic success of visually impaired university students.",3 + 4
