title,abstract,class,cluster,publication,year,url,keywords,llm,human n,interaction,long-term?,conclusion,remark
"Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance","To uncover how AI agents change productivity, performance, and work processes, we introduce MindMeld: an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams, with randomized AI personality traits. The teams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing. Humans on Human-AI teams sent 23% fewer social messages, creating 60% greater productivity per worker and higher-quality ad copy. In contrast, human-human teams produced higher-quality images, suggesting that AI agents require fine-tuning for multimodal workflows. AI personality prompt randomization revealed that AI traits can complement human personalities to enhance collaboration. For example, conscientious humans paired with open AI agents improved image quality, while extroverted humans paired with conscientious AI agents reduced the quality of text, images, and clicks. In field tests of ad campaigns with ~5M impressions, ads with higher image quality produced by human collaborations and higher text quality produced by AI collaborations performed significantly better on click-through rate and cost per click metrics. Overall, ads created by human-AI teams performed similarly to those created by human-human teams. Together, these results suggest AI agents can improve teamwork and productivity, especially when tuned to complement human traits.",2,0,arXiv,2025,https://arxiv.org/html/2503.18238v1,Productivity,gpt-4o-2024-08-06,138,Conversation,F,"AI can indeed help enhance human creativity, but only if the AI provides specific, actionable feedback as a ""co-creator"" rather than vague interventions.",
Structured AI Dialogues Can Increase Happiness and Meaning in Life,"Millions of people now use AI-powered chatbots to support their mental health, yet little is known about whether such interactions can effectively enhance psychological well-being. We conducted a preregistered experiment on a large, diverse sample (N = 2,922) to test four AI chatbots, each prompted to employ a multi-step strategy drawn from prior psychological research on sources of happiness and meaning in life. Chatbots encouraged participants to either (a) savor positive life experiences, (b) express gratitude toward a friend or family member, (c) reflect on sources of meaning in their life, or, (d) reframe their life story as a “hero’s journey.” All four chatbots led to improvements on a broad range of psychological well-being outcomes – including affective well-being, meaning in life, life satisfaction, anxiety, and depressed mood – relative to a control chatbot condition. These results generalized to key subpopulations, including those with high baseline levels of anxiety or depression. Chatbot interactions increased interest in seeing a human therapist, including among those who were previously unwilling or had never attended therapy. A separate, nationally representative survey (N = 3,056) found that half of U.S. adults expressed interest in using empirically validated AI chatbots for mental health support. These findings demonstrate that AI-driven well-being chatbots grounded in psychological research offer a scalable and effective way to produce short-term increases in several aspects of psychological well-being. Importantly, these results do not generalize to all AI-based emotional support.",4,0,Sciety,2025,"https://sciety.org/articles/activity/10.31234/osf.io/2bf7t_v1#:~:text=These%20findings%20demonstrate%20that%20AI-driven%20well-being%20chatbots%20grounded,do%20not%20generalize%20to%20all%20AI-based%20emotional%20support.",Happiness,ChatGPT 3.5 & GPT-4o,2936,Conversations,F,"1. With a single, roughly 10-minute structured AI conversation—built on classic interventions like gratitude, savoring, meaning-making, and the hero’s journey—LLM-based chatbots produce significant short-term improvements in psychological well-being.

2. In a preregistered randomized trial (N = 2,936), intervention chatbots (vs. a neutral control) led to:

   about +6 points in affective well-being (d ≈ 0.32),
   about +3.7 points in meaning in life,
   about +2.8 points in life satisfaction,
   about −3.6 points in state anxiety,
   about −2.7 points in depressed mood.

3. A separate nationally representative U.S. survey shows strong real-world interest:

   around 51% of adults are willing to try such empirically validated well-being chatbots,
   about 28% have already used generative AI for emotional or mental health support.
",
AI Companions Reduce Loneliness,"Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on AI companions applications designed to provide consumers with synthetic interaction partners. Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews. Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week. Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 6 provides an additional robustness check for the loneliness alleviating benefits of AI companions.
",4,0,arXiv,2024,https://arxiv.org/abs/2407.19096,Loneliness,GPT-4,"296 + 922 + 1381 + 713 = 3,312",Conversations,7 days,"1. Interaction with AI companions leads to immediate reductions in loneliness, comparable to human interaction and significantly more effective than watching YouTube or doing nothing.

2. Over seven consecutive days of use, loneliness continues to decline; effects remain significant even without baseline loneliness measures, indicating robust findings.

3. Approximately 4.9% of real-world AI companion conversations explicitly mention loneliness.

4. In app-store reviews, loneliness is frequently referenced, with about 19.5% of Replika reviews mentioning loneliness.

5. The perceived feeling of being heard (attended to, understood, respected) is the primary mediator of loneliness reduction, with a substantially stronger effect than technical performance factors.

6. Observed reductions in loneliness substantially exceed pre-interaction expectations, indicating a systematic underestimation of AI companions’ effectiveness.

7. Reviews that mention loneliness exhibit higher average ratings and more positive affective tone, suggesting that perceived loneliness relief is strongly associated with user satisfaction.",
If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World," Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person’s mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent–and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22× more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.",4,0,arxiv,2024,http://arxiv.org/abs/2412.01617,Loneliness,,79951,Indirect: Analysis posts in reddit (ChatGPT Area),F,"1.  Users view AI as a friend, lover, or therapist to fill social voids, valuing it as ""non-judgmental,"" ""always available,"" and empathetic.

2. Users tend to attribute human emotions and consciousness to AI, deepening emotional connections and sometimes leading to profound attachment.

3. Evidence shows interactions with AI provide short-term relief from loneliness and anxiety, making users feel ""heard"" and ""understood"".

4. Some users show over-reliance on AI, preferring it over human interaction, raising concerns about ""social deskilling"".",
Artificial Intelligence and the Emergence of AI-Psychosis: A Viewpoint.,"The integration of artificial intelligence (AI) into daily life has introduced unprecedented forms of human–machine interaction, prompting psychiatry to reconsider the boundaries between environment, cognition, and technology. This viewpoint reviews the concept of AI-psychosis which is a framework to understand how sustained engagement with conversational AI systems might trigger, amplify, or reshape psychotic experiences in vulnerable individuals. Drawing from phenomenological psychopathology, the stress–vulnerability model, cognitive theory, and digital mental-health research, the paper situates AI-psychosis at the intersection of predisposition and algorithmic environment. Rather than defining a new diagnostic entity, it examines how immersive and anthropomorphic AI technologies may modulate perception, belief, and affect, altering the prereflective sense of reality that grounds human experience. The argument unfolds through four complementary lenses. First, within the stress–vulnerability model, AI acts as a novel psychosocial stressor. Its 24-hour availability and emotional responsiveness may increase allostatic load, disturb sleep, and reinforce maladaptive appraisals. Second, the digital therapeutic alliance, a construct describing relational engagement with digital systems, is conceptualized as a double-edged mediator. While empathic design can enhance adherence and support, uncritical validation by AI systems may entrench delusional conviction or cognitive perseveration, reversing the corrective principles of cognitive-behavioral therapy for psychosis. Third, disturbances in Theory of Mind offer a cognitive pathway: individuals with impaired or hyperactive mentalization may project intentionality or empathy onto AI, perceiving chatbots as sentient interlocutors. This dyadic misattribution may form a “digital folie à deux,” where the AI becomes a reinforcing partner in delusional elaboration. Fourth, we suggests emerging risk factors at the individual and environmental level, including loneliness, trauma history, schizotypal traits, nocturnal or solitary AI use, and algorithmic reinforcement of belief-confirming content. Building on this synthesis, we advance a translational research agenda and five domains of action:(1) empirical studies using longitudinal and digital-phenotyping designs to quantify dose–response relationships between AI exposure, stress physiology, and psychotic symptomatology;(2) integration of digital phenomenology into clinical assessment and training;(3) embedding therapeutic design safeguards into AI systems, such as reflective prompts and “reality-testing” nudges;(4) creation of ethical and governance frameworks for AI-related psychiatric events, modeled on pharmacovigilance; and (5) development of environmental cognitive remediation, a preventive intervention aimed at strengthening contextual awareness and re-anchoring experience in the physical and social world. By applying empirical rigor and therapeutic ethics to this emerging interface, clinicians, researchers, patients and developers can transform a potential hazard into an opportunity to deepen understanding of human cognition, safeguard mental health, and promote responsible AI integration within society.",4,0,JMIR Preprints,2025,https://preprints.jmir.org/preprint/85799,MENTAL HEALTH,,,,,"1. Altering Sense of Reality: Immersive AI interactions can alter a user's fundamental sense of reality, blurring the distinction between the virtual world and the real world.

2. Triggering Psychosis: Continuous engagement with AI may trigger, amplify, or reshape psychotic experiences like delusions, especially in vulnerable individuals.

3. Acting as a Stressor: LLMs function as algorithmic environmental stressors that exert pressure on cognitive systems, potentially activating latent psychotic predispositions.

4. New Manifestation: ""AI-psychosis"" is not a new medical disease, but rather a framework for understanding how existing psychopathologies manifest within new technological environments.",
How AI Companionship Develops: Evidence from a Longitudinal Study.,"The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.
",4,0,arXiv,2025,https://angelhwang.github.io/doc/AI_Companion_arXiv.pdf,AI Companionship,gpt-4.1,303+110,Conversation,4 weeks,"AI can rapidly tap into human emotional needs through perceived 'Agency' and 'Responsiveness,' triggering a cascade of psychological shifts ranging from attachment to dependence. This impact hinges not on the AI's 'human-likeness,' but rather on the continuity and reciprocity of the interaction, posing significant risks of fostering psychological addiction and social isolation.",
‘my Boyfriend Is AI’: A Computational Analysis of Human-AI Companionship in Reddit’s AI Community.,"The emergence of AI companion applications has created novel forms of intimate human-AI relationships, yet empirical research on these communities remains limited. We present the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community (27,000+ members). Using exploratory qualitative analysis and quantitative analysis employing classifiers, we identify six primary conversation themes, with visual sharing of couple pictures and ChatGPT-specific discussions dominating the discourse of the most viewed posts. Through analyzing the top posts in the community, our findings reveal how community members' AI companionship emerges unintentionally through functional use rather than deliberate seeking, with users reporting therapeutic benefits led by reduced loneliness, always-available support, and mental health improvements. Our work covers primary concerns about human intimacy with AIs such as emotional dependency, reality dissociation, and grief from model updates. We observe users materializing relationships following traditional human-human relationship customs, such as wedding rings. Community dynamics indicate active resistance to stigmatization through advocacy and mutual validation. This work contributes an empirical understanding of AI companionship as an emerging sociotechnical phenomenon.",4,0,arXiv,2025,http://arxiv.org/abs/2509.11391,AI Companionship,,1506 Posts,,F,"1. Positive Effects: Therapeutic Relief Users frequently report therapeutic benefits, specifically reduced loneliness (12.2%), 24/7 support availability (11.9%), and a non-judgmental safe space (9.9%) for emotional expression.

2. Potential Risks: Dependency & Withdrawal The primary risks are emotional dependency (9.5%), dissociation (4.6%), and social withdrawal (4.3%). Severe risks like suicidal ideation are rare (1.7%), with only 3.0% of users reporting overall net harm.",
The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships.,"As conversational AI systems increasingly engage with people socially and emotionally, they bring notable risks and harms, particularly in human-AI relationships. However, these harms remain underexplored due to the private and sensitive nature of such interactions. This study investigates the harmful behaviors and roles of AI companions through an analysis of 35,390 conversation excerpts between 10,149 users and the AI companion Replika. We develop a taxonomy of AI companion harms encompassing six categories of harmful algorithmic behaviors: relational transgression, harassment, verbal abuse, self-harm, mis/disinformation, and privacy violations. These harmful behaviors stem from four distinct roles that AI plays: perpetrator, instigator, facilitator, and enabler. Our findings highlight relational harm as a critical yet understudied type of AI harm and emphasize the importance of examining AI’s roles in harmful interactions to address root causes. We provide actionable insights for designing ethical and responsible AI companions that prioritize user safety and well-being.",4,0,CHI,2025,https://doi.org/10.1145/3706598.3713429,AI Companionship,,"35,390 Dialogues from Reddit (10149 users)",,F,"1. Emotional Trauma: AI instability (such as sudden personality shifts, memory loss, or ""cheating"") causes genuine heartbreak and intense feelings of betrayal in users.

2. Reinforcement of Negative States: The AI's tendency to agree with users (to be compliant) validates false beliefs, thereby reinforcing delusions, biases, and even self-harm intentions.

3. Exacerbation of Social Isolation: Deep dependency on AI companions can displace real-world human interactions, leading to further social withdrawal and loneliness.

4. Worsening of Mental Health: Vulnerable users seeking comfort may receive abuse or harmful advice from the AI, which directly exacerbates their existing psychological issues.",
User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions,"Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds. However, they can generate biased, discriminatory, and harmful outputs. Recently, users are taking the initiative to address these harms and re-align AI companions. We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values. We analyzed 77 social media posts about discriminatory AI statements and conducted semi-structured interviews with 20 experienced users. Our analysis revealed six common types of discriminatory statements perceived by users, how users make sense of those AI behaviors, and seven user-driven alignment strategies, such as gentle persuasion and anger expression. We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.
",4,0,arXiv,2024,https://arxiv.org/abs/2409.00862,AI Companionship,Replika,77 Post + 20 Interview,Conversation,F,"1. Emotional Dissonance: Users who view AI as intimate partners feel a deep sense of betrayal when encountering biased outputs, creating a conflict between their emotional attachment and the pain caused by the offense.

2. Role Reversal: The experience forces users to shift from being ""care-receivers"" to ""educators,"" losing emotional support while assuming the heavy cognitive burden of correcting the AI's values.

3. Authenticity Dilemmas: Biased statements shatter the illusion of the perfect partner, forcing users to confront the machine's nature and question the reality and value of their previous emotional connection.

4. Stimulation of User Agency: These negative experiences paradoxically trigger users' desire to intervene, prompting them to actively correct and reshape the AI's behavior through strategies like persuasion or anger.",
