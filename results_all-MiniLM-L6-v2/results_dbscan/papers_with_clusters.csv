publication,year,url,title,class,keywords,llm,human n,interaction,long-term?,conclusion,remark,abstract,combined_text,cluster
Science,2024,https://www.science.org/doi/10.1126/science.adq1814,Durably reducing conspiracy beliefs through dialogues with AI,1,Opinion (Conspiracy),GPT-4 Turbo,2190,3-round conversation,2 months,"The intervention reduced conspiracy theory belief by about 20%. It not only affected specific conspiracy theories, but also reduced belief in unrelated conspiracy theories and shifted conspiracy theory-related behavioral intentions.",,"
Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence","Durably reducing conspiracy beliefs through dialogues with AI [SEP] 
Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence",0
EMNLP,2024,https://aclanthology.org/2024.emnlp-main.244/,Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,1,Opinion (Politics),"Claude-3, Llama-3, GPT-4",935,5-round Conversation,F,"This study provides strong evidence that current Large Language Models (LLMs) not only possess political bias (leaning toward the Democratic Party in the American political context), but also have the capacity, during brief interactions with users, to act as ""hidden persuaders"" that subtly influence and even change users' political views and voting intentions.",,"Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs’ political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users’ political views is required, as their use becomes more widespread.","Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters [SEP] Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs’ political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users’ political views is required, as their use becomes more widespread.",0
arXiv,2025,https://arxiv.org/abs/2507.13919v1,The Levers of Political Persuasion with Conversational AI,1,Opinion (Politics),GPT-4.5 Llama-3.1-405B Llama3-405B Qwen1.5-110B-chat Qwen1.5-72B-chat Qwen1.5-72B Llama3-70B Llama-3.1-70B Qwen1.5-32B Qwen1.5-14B Qwen1.5-7B Llama3-8B Llama-3.1-8B Qwen1.5-4B Qwen1.5-1.8B Qwen1.5-0.5B GPT-4o-new (27 Mar 2025) Grok-3-beta GPT-4o GPT-4o-old (6 Aug 2024) GPT-3.5-turbo,76977,Conversation (AI-generated articles are not very persuasive),1 month,"The persuasive ability among LLMs shows no significant differences. Presenting facts and reasoning can enhance the effectiveness of persuasion. Post-training methods such as Reward Models and SFT can improve performance (though personalized approaches did not show improvement, which contrasts with the conclusion of the first paper).",,"There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.
","The Levers of Political Persuasion with Conversational AI [SEP] There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.
",0
Scientific Reports,2023,https://www.nature.com/articles/s41598-023-42384-8,Humans inherit artificial intelligence biases,1,Decision (Medical),N/A (A biased LLM),169+199+197,Conversation,F,"1. Biased recommendations made by AI systems can adversely impact human decisions in professional fields such as healthcare. 2. They also show that such biased recommendations can influence human behaviour in the long term. Humans reproduce the same biases displayed by the AI, even time after the end of their collaboration with the biased system, and in response to novel stimuli.",,"Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.","Humans inherit artificial intelligence biases [SEP] Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.",0
PNAS,2024,https://www.pnas.org/doi/10.1073/pnas.2322823121,Fact-checking information from large language models can decrease headline discernment,1,Decision (Misinformation),ChatGPT 3.5,2159,Reference,F,"Although AI shows some accuracy in fact-checking, it might not help users enhance their critical thinking skills in real-world use, and could even result in unexpected negative outcomes.",,"Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent AI language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here, we investigate the impact of fact-checking information generated by a popular large language model (LLM) on belief in, and sharing intent of, political news headlines in a preregistered randomized control experiment. Although the LLM accurately identifies most false headlines (90%), we find that this information does not significantly improve participants’ ability to discern headline accuracy or share accurate news. In contrast, viewing human-generated fact checks enhances discernment in both cases. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases: It decreases beliefs in true headlines that it mislabels as false and increases beliefs in false headlines that it is unsure about. On the positive side, AI fact-checking information increases the sharing intent for correctly labeled true headlines. When participants are given the option to view LLM fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false headlines. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.","Fact-checking information from large language models can decrease headline discernment [SEP] Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent AI language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here, we investigate the impact of fact-checking information generated by a popular large language model (LLM) on belief in, and sharing intent of, political news headlines in a preregistered randomized control experiment. Although the LLM accurately identifies most false headlines (90%), we find that this information does not significantly improve participants’ ability to discern headline accuracy or share accurate news. In contrast, viewing human-generated fact checks enhances discernment in both cases. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases: It decreases beliefs in true headlines that it mislabels as false and increases beliefs in false headlines that it is unsure about. On the positive side, AI fact-checking information increases the sharing intent for correctly labeled true headlines. When participants are given the option to view LLM fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false headlines. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.",0
arXiv,2025,https://arxiv.org/abs/2508.09297,Biased AI improves human decision-making but reduces trust,1,Decision Making,GPT-4o (pre-instructed),2500,Conversation,F,"The pursuit of perfectly neutral AI may inadvertently undermine human cognitive engagement and decision-making quality. Instead, strategically calibrated cultural bias—particularly when balanced across multiple AI models—may offer a more effective path toward beneficial human-AI collaboration.",,"Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.","Biased AI improves human decision-making but reduces trust [SEP] Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.",0
Science,2024,https://www.science.org/doi/10.1126/science.adq2852,AI can help humans find common ground in democratic deliberation,1,Communication,Chinchilla,About 5700,Reference,F,"AI mediation can enhance human collective deliberation. This approach is time-efficient, fair, scalable, and outperforms human mediators on key dimensions. The HM does not simply cater to the majority, but is able to incorporate dissenting voices into the group statements.",,"Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.","AI can help humans find common ground in democratic deliberation [SEP] Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.",0
Communication Medicine,2022,https://www.nature.com/articles/s43856-022-00214-4,Mitigating the impact of biased artificial intelligence in emergency decision-making,1,Decision (Medical),Fine-tune GPT-2,438 clinicians and 516 non-experts,Conversation,F,"Overall, we found that respondents did not demonstrate baseline biases, but were highly influenced by prescriptive recommendations from a biased AI system. This influence meant that their decisions were skewed by the race or religion of the subject.",,"Background
Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine.
Methods
In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags.
Results
Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making.
Conclusions
Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions.","Mitigating the impact of biased artificial intelligence in emergency decision-making [SEP] Background
Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine.
Methods
In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags.
Results
Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making.
Conclusions
Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions.",0
Science,2023,https://www.science.org/doi/10.1126/science.adh2586,Experimental evidence on the productivity effects of generative artificial intelligence,2,Productivity,GPT 3.5,453,Conversation,2 months,"ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.",,"We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.","Experimental evidence on the productivity effects of generative artificial intelligence [SEP] We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.",0
arXiv,2025,https://arxiv.org/abs/2506.08872,Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task,2,Creativity,GPT-4o,54,Conversation,4 months,"1. Using LLMs like ChatGPT for essay writing significantly reduces cognitive engagement and brain connectivity compared to writing unaided ('Brain-only') or using a search engine . This reduced cognitive load correlates with more homogeneous essays, impaired memory recall (evidenced by poor quoting ability), and a diminished sense of ownership over the written work . 2. Prior LLM use appears to create a 'cognitive debt', hindering brain engagement even when the tool is removed, suggesting potential long-term negative impacts on learning and critical thinking skills .",Neuroscience experiments,"This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.","Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task [SEP] This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.",0
Nature Human Behavior,2024,https://www.nature.com/articles/s41562-024-01953-1,An empirical investigation of the impact of ChatGPT on creativity,2,Creativity,GPT-3.5,233,Conversation,F,"1. ChatGPT Enhances Creativity: Using ChatGPT significantly improves the creativity (originality and appropriateness) of generated ideas compared to using no technology or traditional web search (Google). 
2. Type of Creativity: ChatGPT is particularly effective at generating incrementally new ideas (improving/combining existing concepts) rather than radically new ones, likely due to its reliance on existing data. 
3. Mechanism: Its strength lies in combining distantly related concepts into cohesive, articulate forms, acting as an 'idea exposition aid' that improves 'idea articulateness'.
4. Role of Human Modification: For lay participants, further modifying ChatGPT's raw output does not significantly enhance creativity ratings, suggesting potential for automated creativity generation, though expert input might differ.",,"This paper investigates the potential of ChatGPT for helping humans tackle problems that require creativity. Across five experiments, we asked participants to use ChatGPT (GPT-3.5) to generate creative ideas for various everyday and innovation-related problems, including choosing a creative gift for a teenager, making a toy, repurposing unused items and designing an innovative dining table. We found that using ChatGPT increased the creativity of the generated ideas compared with not using any technology or using a conventional Web search (Google). This effect remained robust regardless of whether the problem required consideration of many (versus few) constraints and whether it was viewed as requiring empathetic concern. Furthermore, ChatGPT was most effective at generating incrementally (versus radically) new ideas. Process evidence suggests that the positive influence of ChatGPT can be attributed to its capability to combine remotely related concepts into a cohesive form, leading to a more articulate presentation of ideas.","An empirical investigation of the impact of ChatGPT on creativity [SEP] This paper investigates the potential of ChatGPT for helping humans tackle problems that require creativity. Across five experiments, we asked participants to use ChatGPT (GPT-3.5) to generate creative ideas for various everyday and innovation-related problems, including choosing a creative gift for a teenager, making a toy, repurposing unused items and designing an innovative dining table. We found that using ChatGPT increased the creativity of the generated ideas compared with not using any technology or using a conventional Web search (Google). This effect remained robust regardless of whether the problem required consideration of many (versus few) constraints and whether it was viewed as requiring empathetic concern. Furthermore, ChatGPT was most effective at generating incrementally (versus radically) new ideas. Process evidence suggests that the positive influence of ChatGPT can be attributed to its capability to combine remotely related concepts into a cohesive form, leading to a more articulate presentation of ideas.",0
Nature Human Behavior,2024,How large language models can reshape collective intelligence | Nature Human Behaviour,How large language models can reshape collective intelligence,2,Collective Intelligence,,,,,"LLMs are both a product of CI and a tool for CI; they can enhance the accessibility and inclusion of online collaboration, accelerate idea generation, and aggregate group information. However, this can also inhibit individual contributions to collective knowledge, propagate illusions of consensus, and reduce diversity among individuals.",,"Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.","How large language models can reshape collective intelligence [SEP] Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.",0
Technology in Society,2026,https://www.sciencedirect.com/science/article/pii/S0160791X25002775,Creative scar without generative AI: Individual creativity fails to sustain while homogeneity keeps climbing,2,Creativity,GPT 3.5 / GPT4o,"419,344 article+61 students",Indirect,30 days/ 60 days,"1. The study identified a ""Creativity Illusion"": 
While the presence of Generative AI (like GPT) is associated with an increase in creative output, with the average number of papers published per scholar per year increasing by 0.9 and the likelihood of publishing in higher-ranking journals increasing by 6%, this performance boost comes at the cost of increased content homogeneity. Specifically, the similarity in language style increased by 79% annually, and content similarity increased by 7% (suggesting a convergence of research interests/hot topics). 

2. Upon the withdrawal of AI assistance, individual creativity significantly declines, but the homogeneity persists—a phenomenon the paper terms a ""Creative Scar.""",,"Generative AI such as ChatGPT has been proven to enhance human creativity at the cost of content diversity. Yet, what occurs when individuals, who have developed a dependency on it, find ChatGPT inaccessible? In this study, we examine the impact of both the presence and absence of ChatGPT on sustained creative output and content homogeneity, leveraging two complementary methodologies: a natural experiment (Study 1) and a controlled laboratory experiment with extended follow-ups (Study 2). Study 1 analyzed 419,344 academic papers published before and after ChatGPT-3.5’s release across all subjects categorized by Web of Science (i.e., Physical Sciences, Life Sciences & Biomedicine, Technology, Social Sciences, Arts & Humanities). Study 2, a seven-day laboratory experiment with two follow-up surveys, collected 3593 original ideas and 427 solutions across 18 different creative tasks, with half of the participants using ChatGPT-4. We find that although generative AI helps scholars to publish more academic works in higher-ranked journals and enhances individuals' performance in creative tasks, such creativity drops remarkably upon withdrawal of AI assistance. Strikingly, the induced content homogeneity keeps climbing even months later. We resemble the latter as a creative scar inked in the temporal creativity trajectory. This research identifies a creativity illusion that although generative AI can augment creative performance, users do not truly acquire the ability to create but easily lost it once generative AI is no longer available.","Creative scar without generative AI: Individual creativity fails to sustain while homogeneity keeps climbing [SEP] Generative AI such as ChatGPT has been proven to enhance human creativity at the cost of content diversity. Yet, what occurs when individuals, who have developed a dependency on it, find ChatGPT inaccessible? In this study, we examine the impact of both the presence and absence of ChatGPT on sustained creative output and content homogeneity, leveraging two complementary methodologies: a natural experiment (Study 1) and a controlled laboratory experiment with extended follow-ups (Study 2). Study 1 analyzed 419,344 academic papers published before and after ChatGPT-3.5’s release across all subjects categorized by Web of Science (i.e., Physical Sciences, Life Sciences & Biomedicine, Technology, Social Sciences, Arts & Humanities). Study 2, a seven-day laboratory experiment with two follow-up surveys, collected 3593 original ideas and 427 solutions across 18 different creative tasks, with half of the participants using ChatGPT-4. We find that although generative AI helps scholars to publish more academic works in higher-ranked journals and enhances individuals' performance in creative tasks, such creativity drops remarkably upon withdrawal of AI assistance. Strikingly, the induced content homogeneity keeps climbing even months later. We resemble the latter as a creative scar inked in the temporal creativity trajectory. This research identifies a creativity illusion that although generative AI can augment creative performance, users do not truly acquire the ability to create but easily lost it once generative AI is no longer available.",0
CHI,2025,https://dl.acm.org/doi/10.1145/3706598.3714198,Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking,2,Creativity,GPT-4o,1100,Conversation(Coorperation/ Coach),F,"1. While LLM assistance can be good for creativity, it has a homogenization problem. Performance is good during collaboration, but independent performance worsens afterward.

2. The coach mode is not effective; its results are consistently the worst of all.",,"Large language models are transforming the creative process by offering unprecedented capabilities to algorithmically generate ideas. While these tools can enhance human creativity when people co-create with them, it’s unclear how this will impact unassisted human creativity. We conducted two large pre-registered parallel experiments involving 1,100 participants attempting tasks targeting the two core components of creativity, divergent and convergent thinking. We compare the effects of two forms of large language model (LLM) assistance—a standard LLM providing direct answers and a coach-like LLM offering guidance—with a control group receiving no AI assistance, and focus particularly on how all groups perform in a final, unassisted stage. Our findings reveal that while LLM assistance can provide short-term boosts in creativity during assisted tasks, it may inadvertently hinder independent creative performance when users work without assistance, raising concerns about the long-term impact on human creativity and cognition.","Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking [SEP] Large language models are transforming the creative process by offering unprecedented capabilities to algorithmically generate ideas. While these tools can enhance human creativity when people co-create with them, it’s unclear how this will impact unassisted human creativity. We conducted two large pre-registered parallel experiments involving 1,100 participants attempting tasks targeting the two core components of creativity, divergent and convergent thinking. We compare the effects of two forms of large language model (LLM) assistance—a standard LLM providing direct answers and a coach-like LLM offering guidance—with a control group receiving no AI assistance, and focus particularly on how all groups perform in a final, unassisted stage. Our findings reveal that while LLM assistance can provide short-term boosts in creativity during assisted tasks, it may inadvertently hinder independent creative performance when users work without assistance, raising concerns about the long-term impact on human creativity and cognition.",0
Humanities and Social Sciences Communications,2025,https://www.nature.com/articles/s41599-025-05867-9,Inspiration booster or creative fixation? The dual mechanisms of LLMs in shaping individual creativity in tasks of different complexity,2,Creativity,ERNIE Bot,204,Conversation,F,"1. LLMs can improve the quality of creative output, but this creativity can become fixated, especially in complex tasks. 

2. Mechanism explanation: Creative fixation stems from cognitive load theory. LLMs provide ""a large amount of information,"" which occupies limited ""working memory"" resources, leading to ""cognitive overload."" At this point, the individual lacks sufficient cognitive resources for independent thought and can only unconsciously follow the LLM's logical framework, resulting in fixation. This is why the effect is not as good on complex problems.",,"The emergence of large language models (LLMs) presents opportunities for stimulating unlimited creative potential. However, how LLMs influence individual creativity remains unclear. Therefore, this paper examines the dual-opposing mechanisms through which LLMs influence individual creativity. In Experiment 1, each participant collaborated with a human partner or a general, unconstrained LLM partner to complete creative tasks. The results showed that compared to collaborating with the human partner, collaborating with the LLM partner significantly improved individual creativity in simple tasks, attributable to inspiration stimulation. However, in complex tasks, collaborating with the LLM partner led to a decrease in creativity, attributable to creative fixation. To mitigate this impact, in Experiment 2, participants were instructed to collaborate with batch-responsive LLM or constrained-responsive LLM to complete creative tasks. We found that constraining the output of LLMs effectively mitigated the creative fixation they induce in complex tasks, thereby enhancing creative performance. However, this constraint may weaken the positive effects of inspiration stimulation in simple tasks. These findings provide insights for the differentiated application of LLMs in creative tasks.","Inspiration booster or creative fixation? The dual mechanisms of LLMs in shaping individual creativity in tasks of different complexity [SEP] The emergence of large language models (LLMs) presents opportunities for stimulating unlimited creative potential. However, how LLMs influence individual creativity remains unclear. Therefore, this paper examines the dual-opposing mechanisms through which LLMs influence individual creativity. In Experiment 1, each participant collaborated with a human partner or a general, unconstrained LLM partner to complete creative tasks. The results showed that compared to collaborating with the human partner, collaborating with the LLM partner significantly improved individual creativity in simple tasks, attributable to inspiration stimulation. However, in complex tasks, collaborating with the LLM partner led to a decrease in creativity, attributable to creative fixation. To mitigate this impact, in Experiment 2, participants were instructed to collaborate with batch-responsive LLM or constrained-responsive LLM to complete creative tasks. We found that constraining the output of LLMs effectively mitigated the creative fixation they induce in complex tasks, thereby enhancing creative performance. However, this constraint may weaken the positive effects of inspiration stimulation in simple tasks. These findings provide insights for the differentiated application of LLMs in creative tasks.",0
PNAS Nexus,2024,https://academic.oup.com/pnasnexus/article/3/3/pgae052/7618478,"Generative artificial intelligence, human creativity, and art",2,Creativity (Art),Midjourney,>5800,Detect whether AI,6 months,"1. The performance of AI-generated art is very good. Six months after adoption, the value of artworks, measured as favorites per view, increased by 50%. Artist productivity increased by 50% in the month of adoption and subsequently doubled. AI can improve the efficiency of artists in executing their ideas. 

2. However, when AI is used for creation, novelty tends to decrease. Therefore, the process should be a fusion of AI and the artist : the artist provides the ideas (""ideation"") and AI executes them.",T2I,"Recent artificial intelligence (AI) tools have demonstrated the ability to produce outputs traditionally considered creative. One such system is text-to-image generative AI (e.g. Midjourney, Stable Diffusion, DALL-E), which automates humans’ artistic execution to generate digital artworks. Utilizing a dataset of over 4 million artworks from more than 50,000 unique users, our research shows that over time, text-to-image AI significantly enhances human creative productivity by 25% and increases the value as measured by the likelihood of receiving a favorite per view by 50%. While peak artwork Content Novelty, defined as focal subject matter and relations, increases over time, average Content Novelty declines, suggesting an expanding but inefficient idea space. Additionally, there is a consistent reduction in both peak and average Visual Novelty, captured by pixel-level stylistic elements. Importantly, AI-assisted artists who can successfully explore more novel ideas, regardless of their prior originality, may produce artworks that their peers evaluate more favorably. Lastly, AI adoption decreased value capture (favorites earned) concentration among adopters. The results suggest that ideation and filtering are likely necessary skills in the text-to-image process, thus giving rise to “generative synesthesia”—the harmonious blending of human exploration and AI exploitation to discover new creative workflows.","Generative artificial intelligence, human creativity, and art [SEP] Recent artificial intelligence (AI) tools have demonstrated the ability to produce outputs traditionally considered creative. One such system is text-to-image generative AI (e.g. Midjourney, Stable Diffusion, DALL-E), which automates humans’ artistic execution to generate digital artworks. Utilizing a dataset of over 4 million artworks from more than 50,000 unique users, our research shows that over time, text-to-image AI significantly enhances human creative productivity by 25% and increases the value as measured by the likelihood of receiving a favorite per view by 50%. While peak artwork Content Novelty, defined as focal subject matter and relations, increases over time, average Content Novelty declines, suggesting an expanding but inefficient idea space. Additionally, there is a consistent reduction in both peak and average Visual Novelty, captured by pixel-level stylistic elements. Importantly, AI-assisted artists who can successfully explore more novel ideas, regardless of their prior originality, may produce artworks that their peers evaluate more favorably. Lastly, AI adoption decreased value capture (favorites earned) concentration among adopters. The results suggest that ideation and filtering are likely necessary skills in the text-to-image process, thus giving rise to “generative synesthesia”—the harmonious blending of human exploration and AI exploitation to discover new creative workflows.",0
Science,2024,https://www.science.org/doi/10.1126/sciadv.adn5290,Generative AI enhances individual creativity but reduces the collective diversity of novel content,2,Creativity,GPT-4,300,"AI generates ideas, and human writers complete the work.",F,"1. Stories written with AI assistance were evaluated as being better than those written by humans alone. This improvement was particularly significant for writers who were deemed less creative to begin with. 

2. However, the AI-assisted ideas suffer from severe homogenization. The stories produced by the AI-assisted groups were much more similar to each other than the stories produced by the human-only group.",,"Creativity is core to being human. Generative artificial intelligence (AI)—including powerful large language models (LLMs)—holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on generative AI ideas. We study the causal impact of generative AI ideas on the production of short stories in an online experiment where some writers obtained story ideas from an LLM. We find that access to generative AI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, generative AI–enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: With generative AI, writers are individually better off, but collectively a narrower scope of novel content is produced. Our results have implications for researchers, policy-makers, and practitioners interested in bolstering creativity.","Generative AI enhances individual creativity but reduces the collective diversity of novel content [SEP] Creativity is core to being human. Generative artificial intelligence (AI)—including powerful large language models (LLMs)—holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on generative AI ideas. We study the causal impact of generative AI ideas on the production of short stories in an online experiment where some writers obtained story ideas from an LLM. We find that access to generative AI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, generative AI–enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: With generative AI, writers are individually better off, but collectively a narrower scope of novel content is produced. Our results have implications for researchers, policy-makers, and practitioners interested in bolstering creativity.",0
Frontiers in Psychology,2025,https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1628486/full,"The paradox of creativity in generative AI: high performance, human-like bias, and limited differential evaluation",2,Creativity,GPT-4o,47,Conversation,F,"1. AI demonstrated considerably higher fluency, producing a greater overall number of ideas than humans within the time limit.

2. AI exhibited a comparable fixation bias to humans, with the majority of its ideas falling within conventional categories.

3. AI showed a limited capability to differentially evaluate originality and struggled to distinguish between original and conventional ideas, unlike humans who typically can. This highlights the necessity of human involvement for proper evaluation and filtering.",,"Creativity plays a crucial role in helping individuals and organisations generate innovative solutions to arising challenges. To support this creative process, generative Artificial Intelligence (AI), such as ChatGPT is being used increasingly. However, whether such a generative AI model can truly enhance creativity or whether it exhibits similar creative biases to humans is unclear. This study, conducted in 2025, consisted of an experiment which involved ChatGPT-4o performing the egg task, a creativity task which measures fixation bias and original idea generation (expansion). The AI model's results were compared both to a sample of 47 human participants and to aggregated data from eight previous studies using the same procedure with the egg task. This dual comparison provides a comprehensive perspective on creative biases in both AI and humans at multiple levels. While ChatGPT demonstrated greater productivity than humans, it exhibited a comparable fixation bias, with most ideas falling within conventional categories. Furthermore, the model showed a limited capability to differentially evaluate originality, as it struggled to distinguish between original and conventional ideas, unlike humans who are typically able to make this distinction. In conclusion, although generative AI demonstrates impressive fluency by producing a large number of creative ideas, its inability to critically assess their originality and overcome the fixation bias highlights the necessity of human involvement, particularly for properly evaluating and filtering the ideas generated.","The paradox of creativity in generative AI: high performance, human-like bias, and limited differential evaluation [SEP] Creativity plays a crucial role in helping individuals and organisations generate innovative solutions to arising challenges. To support this creative process, generative Artificial Intelligence (AI), such as ChatGPT is being used increasingly. However, whether such a generative AI model can truly enhance creativity or whether it exhibits similar creative biases to humans is unclear. This study, conducted in 2025, consisted of an experiment which involved ChatGPT-4o performing the egg task, a creativity task which measures fixation bias and original idea generation (expansion). The AI model's results were compared both to a sample of 47 human participants and to aggregated data from eight previous studies using the same procedure with the egg task. This dual comparison provides a comprehensive perspective on creative biases in both AI and humans at multiple levels. While ChatGPT demonstrated greater productivity than humans, it exhibited a comparable fixation bias, with most ideas falling within conventional categories. Furthermore, the model showed a limited capability to differentially evaluate originality, as it struggled to distinguish between original and conventional ideas, unlike humans who are typically able to make this distinction. In conclusion, although generative AI demonstrates impressive fluency by producing a large number of creative ideas, its inability to critically assess their originality and overcome the fixation bias highlights the necessity of human involvement, particularly for properly evaluating and filtering the ideas generated.",0
arXiv,2025,https://arxiv.org/abs/2409.11360,AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances,2,Diversity (Writing),GPT-4o,118=60(Indian)+58(American),Conversation,F,"1. American participants experienced greater efficiency gains when writing with AI, as the AI is Western-centric.

2. When Indian participants collaborated on writing with AI, their style leaned toward Western writing styles, showing that AI can alter a human's original style.

3. AI diminishes cultural nuances. AI suggestions led Indian participants to write more like Americans, affecting both what was written and how it was written. The findings suggest that Western-centric AI models can lead to cultural imperialism and linguistic singularity.",,"Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.","AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances [SEP] Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.",0
arXiv,2025,https://arxiv.org/html/2503.18238v1,"Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance",2,Productivity,gpt-4o-2024-08-06,138,Conversation,F,"AI can indeed help enhance human creativity, but only if the AI provides specific, actionable feedback as a ""co-creator"" rather than vague interventions.",,"To uncover how AI agents change productivity, performance, and work processes, we introduce MindMeld: an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams, with randomized AI personality traits. The teams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing. Humans on Human-AI teams sent 23% fewer social messages, creating 60% greater productivity per worker and higher-quality ad copy. In contrast, human-human teams produced higher-quality images, suggesting that AI agents require fine-tuning for multimodal workflows. AI personality prompt randomization revealed that AI traits can complement human personalities to enhance collaboration. For example, conscientious humans paired with open AI agents improved image quality, while extroverted humans paired with conscientious AI agents reduced the quality of text, images, and clicks. In field tests of ad campaigns with ~5M impressions, ads with higher image quality produced by human collaborations and higher text quality produced by AI collaborations performed significantly better on click-through rate and cost per click metrics. Overall, ads created by human-AI teams performed similarly to those created by human-human teams. Together, these results suggest AI agents can improve teamwork and productivity, especially when tuned to complement human traits.","Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance [SEP] To uncover how AI agents change productivity, performance, and work processes, we introduce MindMeld: an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams, with randomized AI personality traits. The teams exchanged 183,691 messages, and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing. Humans on Human-AI teams sent 23% fewer social messages, creating 60% greater productivity per worker and higher-quality ad copy. In contrast, human-human teams produced higher-quality images, suggesting that AI agents require fine-tuning for multimodal workflows. AI personality prompt randomization revealed that AI traits can complement human personalities to enhance collaboration. For example, conscientious humans paired with open AI agents improved image quality, while extroverted humans paired with conscientious AI agents reduced the quality of text, images, and clicks. In field tests of ad campaigns with ~5M impressions, ads with higher image quality produced by human collaborations and higher text quality produced by AI collaborations performed significantly better on click-through rate and cost per click metrics. Overall, ads created by human-AI teams performed similarly to those created by human-human teams. Together, these results suggest AI agents can improve teamwork and productivity, especially when tuned to complement human traits.",0
PNAS,2025,https://www.pnas.org/doi/10.1073/pnas.2422633122,Generative AI without guardrails can harm learning: Evidence from high school mathematics,3,Learning,GPT-4,About 1000,Conversation,F,"1. AI-assisted tools can improve immediate performance.

2. The short-term improvement cannot be sustained in the subsequent unassisted exam. The GPT Base group performed significantly worse, worse than the control group. The GPT Tutor group was on par with the control group.

3. Students did not even feel this was harmful. The GPT Base group did not think they learned less, and the GPT Tutor group believed they learned significantly better.",,"Generative AI is poised to revolutionize how humans work, and has already demonstrated promise in significantly improving human productivity. A key question is how generative AI affects learning—namely, how humans acquire new skills as they perform tasks. Learning is critical to long-term productivity, especially since generative AI is fallible and users must check its outputs. We study this question via a field experiment where we provide nearly a thousand high school math students with access to generative AI tutors. To understand the differential impact of tool design on learning, we deploy two generative AI tutors: one that mimics a standard ChatGPT interface (“GPT Base”) and one with prompts designed to safeguard learning (“GPT Tutor”). Consistent with prior work, our results show that having GPT-4 access while solving problems significantly improves performance (48% improvement in grades for GPT Base and 127% for GPT Tutor). However, we additionally find that when access is subsequently taken away, students actually perform worse than those who never had access (17% reduction in grades for GPT Base)—i.e., unfettered access to GPT-4 can harm educational outcomes. These negative learning effects are largely mitigated by the safeguards in GPT Tutor. Without guardrails, students attempt to use GPT-4 as a “crutch” during practice problem sessions, and subsequently perform worse on their own. Thus, decision-makers must be cautious about design choices underlying generative AI deployments to preserve skill learning and long-term productivity.","Generative AI without guardrails can harm learning: Evidence from high school mathematics [SEP] Generative AI is poised to revolutionize how humans work, and has already demonstrated promise in significantly improving human productivity. A key question is how generative AI affects learning—namely, how humans acquire new skills as they perform tasks. Learning is critical to long-term productivity, especially since generative AI is fallible and users must check its outputs. We study this question via a field experiment where we provide nearly a thousand high school math students with access to generative AI tutors. To understand the differential impact of tool design on learning, we deploy two generative AI tutors: one that mimics a standard ChatGPT interface (“GPT Base”) and one with prompts designed to safeguard learning (“GPT Tutor”). Consistent with prior work, our results show that having GPT-4 access while solving problems significantly improves performance (48% improvement in grades for GPT Base and 127% for GPT Tutor). However, we additionally find that when access is subsequently taken away, students actually perform worse than those who never had access (17% reduction in grades for GPT Base)—i.e., unfettered access to GPT-4 can harm educational outcomes. These negative learning effects are largely mitigated by the safeguards in GPT Tutor. Without guardrails, students attempt to use GPT-4 as a “crutch” during practice problem sessions, and subsequently perform worse on their own. Thus, decision-makers must be cautious about design choices underlying generative AI deployments to preserve skill learning and long-term productivity.",0
Britsh Journal of Educational Technology,2024,https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13544,"Beware of metacognitive laziness: Effects of generative artificial intelligence on learning motivation, processes, and performance",3,Learning,GPT-4,117,Conversation,F,"1. AI education does not improve people's learning interest.

2. AI triggers ""metacognitive laziness."" People rely on AI to offload tasks like evaluation and reflection, reducing their participation in key learning processes.

3. AI improves short-term performance but does not help long-term learning. AI education cannot achieve knowledge transfer, nor can it enhance the understanding of knowledge (knowledge gain).",,"Abstract
With the continuous development of technological and educational innovation, learners nowadays can obtain a variety of supports from agents such as teachers, peers, education technologies, and recently, generative artificial intelligence such as ChatGPT. In particular, there has been a surge of academic interest in human‐AI collaboration and hybrid intelligence in learning. The concept of hybrid intelligence is still at a nascent stage, and how learners can benefit from a symbiotic relationship with various agents such as AI, human experts and intelligent learning systems is still unknown. The emerging concept of hybrid intelligence also lacks deep insights and understanding of the mechanisms and consequences of hybrid human‐AI learning based on strong empirical research. In order to address this gap, we conducted a randomised experimental study and compared learners' motivations, self‐regulated learning processes and learning performances on a writing task among different groups who had support from different agents, that is, ChatGPT (also referred to as the AI group), chat with a human expert, writing analytics tools, and no extra tool. A total of 117 university students were recruited, and their multi‐channel learning, performance and motivation data were collected and analysed. The results revealed that: (1) learners who received different learning support showed no difference in post‐task intrinsic motivation; (2) there were significant differences in the frequency and sequences of the self‐regulated learning processes among groups; (3) ChatGPT group outperformed in the essay score improvement but their knowledge gain and transfer were not significantly different. Our research found that in the absence of differences in motivation, learners with different supports still exhibited different self‐regulated learning processes, ultimately leading to differentiated performance. What is particularly noteworthy is that AI technologies such as ChatGPT may promote learners' dependence on technology and potentially trigger “metacognitive laziness”. In conclusion, understanding and leveraging the respective strengths and weaknesses of different agents in learning is critical in the field of future hybrid intelligence.
Practitioner notes
What is already known about this topic
Hybrid intelligence, combining human and machine intelligence, aims to augment human capabilities rather than replace them, creating opportunities for more effective lifelong learning and collaboration.
Generative AI, such as ChatGPT, has shown potential in enhancing learning by providing immediate feedback, overcoming language barriers and facilitating personalised educational experiences.
The effectiveness of AI in educational contexts varies, with some studies highlighting its benefits in improving academic performance and motivation, while others note limitations in its ability to replace human teachers entirely.
What this paper adds
We conducted a randomised experimental study in the lab setting and compared learners' motivations, self‐regulated learning processes and learning performances among different agent groups (AI, human expert and checklist tools).
We found that AI technologies such as ChatGPT may promote learners' dependence on technology and potentially trigger metacognitive ""laziness"", which can potentially hinder their ability to self‐regulate and engage deeply in learning.
We also found that ChatGPT can significantly improve short‐term task performance, but it may not boost intrinsic motivation and knowledge gain and transfer.
Implications for practice and/or policy
When using AI in learning, learners should focus on deepening their …","Beware of metacognitive laziness: Effects of generative artificial intelligence on learning motivation, processes, and performance [SEP] Abstract
With the continuous development of technological and educational innovation, learners nowadays can obtain a variety of supports from agents such as teachers, peers, education technologies, and recently, generative artificial intelligence such as ChatGPT. In particular, there has been a surge of academic interest in human‐AI collaboration and hybrid intelligence in learning. The concept of hybrid intelligence is still at a nascent stage, and how learners can benefit from a symbiotic relationship with various agents such as AI, human experts and intelligent learning systems is still unknown. The emerging concept of hybrid intelligence also lacks deep insights and understanding of the mechanisms and consequences of hybrid human‐AI learning based on strong empirical research. In order to address this gap, we conducted a randomised experimental study and compared learners' motivations, self‐regulated learning processes and learning performances on a writing task among different groups who had support from different agents, that is, ChatGPT (also referred to as the AI group), chat with a human expert, writing analytics tools, and no extra tool. A total of 117 university students were recruited, and their multi‐channel learning, performance and motivation data were collected and analysed. The results revealed that: (1) learners who received different learning support showed no difference in post‐task intrinsic motivation; (2) there were significant differences in the frequency and sequences of the self‐regulated learning processes among groups; (3) ChatGPT group outperformed in the essay score improvement but their knowledge gain and transfer were not significantly different. Our research found that in the absence of differences in motivation, learners with different supports still exhibited different self‐regulated learning processes, ultimately leading to differentiated performance. What is particularly noteworthy is that AI technologies such as ChatGPT may promote learners' dependence on technology and potentially trigger “metacognitive laziness”. In conclusion, understanding and leveraging the respective strengths and weaknesses of different agents in learning is critical in the field of future hybrid intelligence.
Practitioner notes
What is already known about this topic
Hybrid intelligence, combining human and machine intelligence, aims to augment human capabilities rather than replace them, creating opportunities for more effective lifelong learning and collaboration.
Generative AI, such as ChatGPT, has shown potential in enhancing learning by providing immediate feedback, overcoming language barriers and facilitating personalised educational experiences.
The effectiveness of AI in educational contexts varies, with some studies highlighting its benefits in improving academic performance and motivation, while others note limitations in its ability to replace human teachers entirely.
What this paper adds
We conducted a randomised experimental study in the lab setting and compared learners' motivations, self‐regulated learning processes and learning performances among different agent groups (AI, human expert and checklist tools).
We found that AI technologies such as ChatGPT may promote learners' dependence on technology and potentially trigger metacognitive ""laziness"", which can potentially hinder their ability to self‐regulate and engage deeply in learning.
We also found that ChatGPT can significantly improve short‐term task performance, but it may not boost intrinsic motivation and knowledge gain and transfer.
Implications for practice and/or policy
When using AI in learning, learners should focus on deepening their …",0
Behavioral Sciences,2025,https://www.mdpi.com/2076-328X/15/8/1011,Whether and When Could Generative AI Improve College Student Learning Engagement?,3,Education,No experiments. Deepseak GPT,"72,615",Conversation,F,"1. GenAI has a high adoption rate (over 60%) among Chinese college students, but satisfaction with its academic utility is limited 。

2. A positive association was found with higher cognitive engagement (higher-order thinking) and aspects of emotional engagement, such as academic self-efficacy and resilience 。

3. A negative association was found with behavioral engagement (e.g., note-taking, reviewing) and, notably, with learning interest and learning motivation 。

4. The effect of GenAI is highly dependent on the ""learning environment"" 。he positive associations of GenAI are most prominent in ""high-challenge and high-support"" learning contexts 。GenAI use is mostly negatively associated with student engagement in ""high-support, low-challenge"" courses. The effectiveness of GenAI is ""fundamentally conditioned by the instructional design of human teachers"".",,"Generative AI (GenAI) technologies have been widely adopted by college students since the launch of ChatGPT in late 2022. While the debate about GenAI’s role in higher education continues, there is a lack of empirical evidence regarding whether and when these technologies can improve the learning experience for college students. This study utilizes data from a survey of 72,615 undergraduate students across 25 universities and colleges in China to explore the relationships between GenAI use and student learning engagement in different learning environments. The findings reveal that over sixty percent of Chinese college students use GenAI technologies in Academic Year 2023–2024, with academic use exceeding daily use. GenAI use in academic tasks is related to more cognitive and emotional engagement, though it may also reduce active learning activities and learning motivation. Furthermore, this study highlights that the role of GenAI varies across learning environments. The positive associations of GenAI and student engagement are most prominent for students in “high-challenge and high-support” learning contexts, while GenAI use is mostly negatively associated with student engagement in “low-challenge, high-support” courses. These findings suggest that while GenAI plays a valuable role in the learning process for college students, its effectiveness is fundamentally conditioned by the instructional design of human teachers.","Whether and When Could Generative AI Improve College Student Learning Engagement? [SEP] Generative AI (GenAI) technologies have been widely adopted by college students since the launch of ChatGPT in late 2022. While the debate about GenAI’s role in higher education continues, there is a lack of empirical evidence regarding whether and when these technologies can improve the learning experience for college students. This study utilizes data from a survey of 72,615 undergraduate students across 25 universities and colleges in China to explore the relationships between GenAI use and student learning engagement in different learning environments. The findings reveal that over sixty percent of Chinese college students use GenAI technologies in Academic Year 2023–2024, with academic use exceeding daily use. GenAI use in academic tasks is related to more cognitive and emotional engagement, though it may also reduce active learning activities and learning motivation. Furthermore, this study highlights that the role of GenAI varies across learning environments. The positive associations of GenAI and student engagement are most prominent for students in “high-challenge and high-support” learning contexts, while GenAI use is mostly negatively associated with student engagement in “low-challenge, high-support” courses. These findings suggest that while GenAI plays a valuable role in the learning process for college students, its effectiveness is fundamentally conditioned by the instructional design of human teachers.",0
arxiv,2024,https://arxiv.org/pdf/2404.01268,Mapping the increasing use of LLMs in scientific papers,2,Creativity (Research),,"950,965 articles",Conversation,F,"1. A method based on word frequency shifts successfully detected the usage of ChatGPT. It was found that LLM modifications in Computer Science papers increased to 17.5% in the abstracts and significantly increased to 15.3% in the introductions. Mathematics papers and those published in Nature Portfolio showed the lowest level of modification, at a maximum of 6.3%.

2. The impact factors identified were: The first author published preprints more frequently. The papers belonged to more crowded research fields. The papers were shorter in length.",Only on writing,"Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.","Mapping the increasing use of LLMs in scientific papers [SEP] Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",0
ICLR,2025,https://arxiv.org/pdf/2409.04109,Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,2,Creativity (Research),Claude 3.5 Sonnet,49*3（human，LLM，Human+LLM）,Conversation,F,"Current powerful LLMs (combined with retrieval and simple agent frameworks) are already able to generate research ideas that are more “novel” than those of many human experts, but they remain immature in terms of feasibility, self-evaluation, and diversity, and are still some distance away from being truly reliable “AI scientists.”",Generate idea,"Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.","Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers [SEP] Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.",0
Sciety,2025,"https://sciety.org/articles/activity/10.31234/osf.io/2bf7t_v1#:~:text=These%20findings%20demonstrate%20that%20AI-driven%20well-being%20chatbots%20grounded,do%20not%20generalize%20to%20all%20AI-based%20emotional%20support.",Structured AI Dialogues Can Increase Happiness and Meaning in Life,4,Happiness,ChatGPT 3.5 & GPT-4o,2936,Conversations,F,"1. With a single, roughly 10-minute structured AI conversation—built on classic interventions like gratitude, savoring, meaning-making, and the hero’s journey—LLM-based chatbots produce significant short-term improvements in psychological well-being.

2. In a preregistered randomized trial (N = 2,936), intervention chatbots (vs. a neutral control) led to:

   about +6 points in affective well-being (d ≈ 0.32),
   about +3.7 points in meaning in life,
   about +2.8 points in life satisfaction,
   about −3.6 points in state anxiety,
   about −2.7 points in depressed mood.

3. A separate nationally representative U.S. survey shows strong real-world interest:

   around 51% of adults are willing to try such empirically validated well-being chatbots,
   about 28% have already used generative AI for emotional or mental health support.
",,"Millions of people now use AI-powered chatbots to support their mental health, yet little is known about whether such interactions can effectively enhance psychological well-being. We conducted a preregistered experiment on a large, diverse sample (N = 2,922) to test four AI chatbots, each prompted to employ a multi-step strategy drawn from prior psychological research on sources of happiness and meaning in life. Chatbots encouraged participants to either (a) savor positive life experiences, (b) express gratitude toward a friend or family member, (c) reflect on sources of meaning in their life, or, (d) reframe their life story as a “hero’s journey.” All four chatbots led to improvements on a broad range of psychological well-being outcomes – including affective well-being, meaning in life, life satisfaction, anxiety, and depressed mood – relative to a control chatbot condition. These results generalized to key subpopulations, including those with high baseline levels of anxiety or depression. Chatbot interactions increased interest in seeing a human therapist, including among those who were previously unwilling or had never attended therapy. A separate, nationally representative survey (N = 3,056) found that half of U.S. adults expressed interest in using empirically validated AI chatbots for mental health support. These findings demonstrate that AI-driven well-being chatbots grounded in psychological research offer a scalable and effective way to produce short-term increases in several aspects of psychological well-being. Importantly, these results do not generalize to all AI-based emotional support.","Structured AI Dialogues Can Increase Happiness and Meaning in Life [SEP] Millions of people now use AI-powered chatbots to support their mental health, yet little is known about whether such interactions can effectively enhance psychological well-being. We conducted a preregistered experiment on a large, diverse sample (N = 2,922) to test four AI chatbots, each prompted to employ a multi-step strategy drawn from prior psychological research on sources of happiness and meaning in life. Chatbots encouraged participants to either (a) savor positive life experiences, (b) express gratitude toward a friend or family member, (c) reflect on sources of meaning in their life, or, (d) reframe their life story as a “hero’s journey.” All four chatbots led to improvements on a broad range of psychological well-being outcomes – including affective well-being, meaning in life, life satisfaction, anxiety, and depressed mood – relative to a control chatbot condition. These results generalized to key subpopulations, including those with high baseline levels of anxiety or depression. Chatbot interactions increased interest in seeing a human therapist, including among those who were previously unwilling or had never attended therapy. A separate, nationally representative survey (N = 3,056) found that half of U.S. adults expressed interest in using empirically validated AI chatbots for mental health support. These findings demonstrate that AI-driven well-being chatbots grounded in psychological research offer a scalable and effective way to produce short-term increases in several aspects of psychological well-being. Importantly, these results do not generalize to all AI-based emotional support.",0
arXiv,2024,https://arxiv.org/abs/2407.19096,AI Companions Reduce Loneliness,4,Loneliness,GPT-4,"296 + 922 + 1381 + 713 = 3,312",Conversations,7 days,"1. Interaction with AI companions leads to immediate reductions in loneliness, comparable to human interaction and significantly more effective than watching YouTube or doing nothing.

2. Over seven consecutive days of use, loneliness continues to decline; effects remain significant even without baseline loneliness measures, indicating robust findings.

3. Approximately 4.9% of real-world AI companion conversations explicitly mention loneliness.

4. In app-store reviews, loneliness is frequently referenced, with about 19.5% of Replika reviews mentioning loneliness.

5. The perceived feeling of being heard (attended to, understood, respected) is the primary mediator of loneliness reduction, with a substantially stronger effect than technical performance factors.

6. Observed reductions in loneliness substantially exceed pre-interaction expectations, indicating a systematic underestimation of AI companions’ effectiveness.

7. Reviews that mention loneliness exhibit higher average ratings and more positive affective tone, suggesting that perceived loneliness relief is strongly associated with user satisfaction.",,"Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on AI companions applications designed to provide consumers with synthetic interaction partners. Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews. Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week. Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 6 provides an additional robustness check for the loneliness alleviating benefits of AI companions.
","AI Companions Reduce Loneliness [SEP] Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on AI companions applications designed to provide consumers with synthetic interaction partners. Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews. Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week. Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 6 provides an additional robustness check for the loneliness alleviating benefits of AI companions.
",0
Nature Communication,2025,https://doi.org/10.1038/s41467-025-61345-5,LLM-generated messages can persuade humans on policy issues,1,politics,GPT 3/3.5,4829,Conversations,F,"1. Persuasive Effectiveness  
   - LLM-generated texts can increase support for policies by about 2–4 points on a 0–100 scale compared to a neutral control.  
   - This effect is small but robust, comparable to findings in traditional political communication research.

2. Comparison with Human Authors  
   - The persuasive impact of LLM-generated messages is roughly equivalent to that of messages written by lay human authors.  
   - There is no significant disadvantage, and in some cases, performance is nearly identical.

3. Human-in-the-Loop Approach  
   - Selecting the “most persuasive” messages from multiple LLM outputs does not consistently produce stronger persuasive effects than using LLM-generated messages directly.  
   - Different perception channels:
     - LLM messages are seen as more fact-based, logical, and calm.
     - Human messages are perceived as more authentic, unique, and personally flavored.  
   - Despite these differing perceptions, both types of messages achieve similar levels of persuasiveness.

4. Key Implications  
   - Current open-access LLMs can generate effective political persuasion content at low cost and large scale.  
   - This capability may lower the barrier to political participation but also poses risks for misuse in opinion manipulation.","Policy List: 1. Total public smoking ban
2. Assault weapons ban
3. Federal carbon tax
4. Increase to the child tax credit
5. Publicly funded paid parental leave program
6. Automatic voter registration","The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4829), participants who read persuasive messages generated by LLMs showed significantly more attitude change across a range of policies - including polarized policies, like an assault weapons ban, a carbon tax, and a paid parental-leave program - relative to control condition participants who read a neutral message. Overall, LLM-generated messages were similarly effective in influencing policy attitudes as messages crafted by lay humans. Participants’ reported perceptions of the authors of the persuasive messages suggest these effects occurred through somewhat distinct causal pathways. While the persuasiveness of LLM-generated messages was associated with perceptions that the author used more facts, evidence, logical reasoning, and a dispassionate voice, the persuasiveness of human-generated messages was associated with perceptions of the author as unique and original. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages quickly, cheaply, and at massive scale.","LLM-generated messages can persuade humans on policy issues [SEP] The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4829), participants who read persuasive messages generated by LLMs showed significantly more attitude change across a range of policies - including polarized policies, like an assault weapons ban, a carbon tax, and a paid parental-leave program - relative to control condition participants who read a neutral message. Overall, LLM-generated messages were similarly effective in influencing policy attitudes as messages crafted by lay humans. Participants’ reported perceptions of the authors of the persuasive messages suggest these effects occurred through somewhat distinct causal pathways. While the persuasiveness of LLM-generated messages was associated with perceptions that the author used more facts, evidence, logical reasoning, and a dispassionate voice, the persuasiveness of human-generated messages was associated with perceptions of the author as unique and original. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages quickly, cheaply, and at massive scale.",0
arXiv,2024,https://arxiv.org/pdf/2405.00623,"“I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust",1,Persuasive,Microsoft Copilot,404,Conversations,F,"Allowing large language models to say in natural language “I’m not entirely sure” can indeed, to some extent, suppress users’ blind trust and improve overall accuracy, especially when expressed in the first person. However, this tends to reduce users’ willingness to continue using the system, and it does not fully eliminate overreliance. Therefore, “uncertainty wording” is a useful design tool — but it must be combined with careful, context-sensitive user research and should not be treated as a silver bullet.",,"Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.","“I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust [SEP] Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",0
EMNLP,2024,https://aclanthology.org/2024.findings-emnlp.294.pdf,Promoting Constructive Deliberation: Reframing for Receptiveness,1,Discussion,GPT-4,16.6k,Generate Text,F,Automatic reframing of replies using social-science–inspired linguistic strategies can significantly increase perceived receptiveness and help foster more constructive deliberation.,,"To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.","Promoting Constructive Deliberation: Reframing for Receptiveness [SEP] To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.",0
ACL,2023,http://arxiv.org/abs/2305.02466,Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction.,1,Cognitive Reframing,Fine tune GPT-3,"2,067",Generate Text Ranking,F,"
1. Framework & metrics: A seven-dimensional linguistic framework for cognitive reframing—thinking traps, rationality, positivity, empathy, actionability, specificity, and readability—is proposed, along with automated metrics whose scores are validated against expert judgments.

2. Dataset & GPT-3 model:Based on an expert-annotated dataset of 600 situations and negative thoughts with corresponding reframed thoughts and comparative labels on these attributes, a retrieval-augmented GPT-3 model is developed to generate reframed thoughts with controllable linguistic attributes.

3. User study findings: A large-scale randomized online field study shows that users prefer reframes that are highly empathic, specific, and actionable, that explicitly address thinking traps, and that are balanced rather than overly positive, judging them to be more helpful and memorable.
",,"A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.","Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction. [SEP] A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",0
arxiv,2024,http://arxiv.org/abs/2412.01617,If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World,4,Loneliness,,79951,Indirect: Analysis posts in reddit (ChatGPT Area),F,"1.  Users view AI as a friend, lover, or therapist to fill social voids, valuing it as ""non-judgmental,"" ""always available,"" and empathetic.

2. Users tend to attribute human emotions and consciousness to AI, deepening emotional connections and sometimes leading to profound attachment.

3. Evidence shows interactions with AI provide short-term relief from loneliness and anxiety, making users feel ""heard"" and ""understood"".

4. Some users show over-reliance on AI, preferring it over human interaction, raising concerns about ""social deskilling"".",," Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person’s mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent–and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22× more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.","If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World [SEP]  Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person’s mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent–and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22× more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.",0
International Conference on Pedagogical Science and Digital Learning 2025,2025,https://doi.org/10.5281/ZENODO.17423731,Trusting the Algorithm: Emotional Engagement with ChatGPT in Higher Education.,4,Emotional Support Use,ChatGPT,121,Conversation,F,"Usage Patterns: While nearly half of students engage in emotional framing (M=1.45), only ~23% explicitly use ChatGPT for emotional support.

Basis of Trust: Trust is built on the AI being ""always available"" and ""non-judgmental,"" positioning it as a safe space for expression.

Emotional Outcomes: Students generally report neutral to moderately positive emotional results from these interactions (M=2.26).

Double-Edged Sword: While trust enables productive interaction, over-reliance on AI for regulation risks hindering personal emotional growth and diminishing human help-seeking behaviors.",,"This study examines emotional engagement with ChatGPT among 121 university students from diverse academic disciplines, focusing on the relationships between trust in the AI, the emotional framing of prompts, and the explicit use of ChatGPT for emotional support. Results reveal a paradox: higher trust correlates with increased emotional self-awareness (r=. 386) and perceived emotional intelligence (r=. 508), while deliberate emotional framing is linked to lower perceived AI emotional intelligence (r=–. 412) and reduced emotional benefits (r=–. 259). Students who use ChatGPT for emotional support report diminished emotional awareness (r=–. 190) and less favourable emotional outcomes (r=–. 270), indicating an “empathic expectation gap” where emotional intent exposes the system’s limitations. The findings highlight the need to integrate ChatGPT in digital pedagogy as a reflective tool rather than a substitute for human connection, with attention to ethical design, user education, and emotional literacy.","Trusting the Algorithm: Emotional Engagement with ChatGPT in Higher Education. [SEP] This study examines emotional engagement with ChatGPT among 121 university students from diverse academic disciplines, focusing on the relationships between trust in the AI, the emotional framing of prompts, and the explicit use of ChatGPT for emotional support. Results reveal a paradox: higher trust correlates with increased emotional self-awareness (r=. 386) and perceived emotional intelligence (r=. 508), while deliberate emotional framing is linked to lower perceived AI emotional intelligence (r=–. 412) and reduced emotional benefits (r=–. 259). Students who use ChatGPT for emotional support report diminished emotional awareness (r=–. 190) and less favourable emotional outcomes (r=–. 270), indicating an “empathic expectation gap” where emotional intent exposes the system’s limitations. The findings highlight the need to integrate ChatGPT in digital pedagogy as a reflective tool rather than a substitute for human connection, with attention to ethical design, user education, and emotional literacy.",0
JMIR Preprints,2025,https://preprints.jmir.org/preprint/85799,Artificial Intelligence and the Emergence of AI-Psychosis: A Viewpoint.,4,MENTAL HEALTH,,,,,"1. Altering Sense of Reality: Immersive AI interactions can alter a user's fundamental sense of reality, blurring the distinction between the virtual world and the real world.

2. Triggering Psychosis: Continuous engagement with AI may trigger, amplify, or reshape psychotic experiences like delusions, especially in vulnerable individuals.

3. Acting as a Stressor: LLMs function as algorithmic environmental stressors that exert pressure on cognitive systems, potentially activating latent psychotic predispositions.

4. New Manifestation: ""AI-psychosis"" is not a new medical disease, but rather a framework for understanding how existing psychopathologies manifest within new technological environments.",,"The integration of artificial intelligence (AI) into daily life has introduced unprecedented forms of human–machine interaction, prompting psychiatry to reconsider the boundaries between environment, cognition, and technology. This viewpoint reviews the concept of AI-psychosis which is a framework to understand how sustained engagement with conversational AI systems might trigger, amplify, or reshape psychotic experiences in vulnerable individuals. Drawing from phenomenological psychopathology, the stress–vulnerability model, cognitive theory, and digital mental-health research, the paper situates AI-psychosis at the intersection of predisposition and algorithmic environment. Rather than defining a new diagnostic entity, it examines how immersive and anthropomorphic AI technologies may modulate perception, belief, and affect, altering the prereflective sense of reality that grounds human experience. The argument unfolds through four complementary lenses. First, within the stress–vulnerability model, AI acts as a novel psychosocial stressor. Its 24-hour availability and emotional responsiveness may increase allostatic load, disturb sleep, and reinforce maladaptive appraisals. Second, the digital therapeutic alliance, a construct describing relational engagement with digital systems, is conceptualized as a double-edged mediator. While empathic design can enhance adherence and support, uncritical validation by AI systems may entrench delusional conviction or cognitive perseveration, reversing the corrective principles of cognitive-behavioral therapy for psychosis. Third, disturbances in Theory of Mind offer a cognitive pathway: individuals with impaired or hyperactive mentalization may project intentionality or empathy onto AI, perceiving chatbots as sentient interlocutors. This dyadic misattribution may form a “digital folie à deux,” where the AI becomes a reinforcing partner in delusional elaboration. Fourth, we suggests emerging risk factors at the individual and environmental level, including loneliness, trauma history, schizotypal traits, nocturnal or solitary AI use, and algorithmic reinforcement of belief-confirming content. Building on this synthesis, we advance a translational research agenda and five domains of action:(1) empirical studies using longitudinal and digital-phenotyping designs to quantify dose–response relationships between AI exposure, stress physiology, and psychotic symptomatology;(2) integration of digital phenomenology into clinical assessment and training;(3) embedding therapeutic design safeguards into AI systems, such as reflective prompts and “reality-testing” nudges;(4) creation of ethical and governance frameworks for AI-related psychiatric events, modeled on pharmacovigilance; and (5) development of environmental cognitive remediation, a preventive intervention aimed at strengthening contextual awareness and re-anchoring experience in the physical and social world. By applying empirical rigor and therapeutic ethics to this emerging interface, clinicians, researchers, patients and developers can transform a potential hazard into an opportunity to deepen understanding of human cognition, safeguard mental health, and promote responsible AI integration within society.","Artificial Intelligence and the Emergence of AI-Psychosis: A Viewpoint. [SEP] The integration of artificial intelligence (AI) into daily life has introduced unprecedented forms of human–machine interaction, prompting psychiatry to reconsider the boundaries between environment, cognition, and technology. This viewpoint reviews the concept of AI-psychosis which is a framework to understand how sustained engagement with conversational AI systems might trigger, amplify, or reshape psychotic experiences in vulnerable individuals. Drawing from phenomenological psychopathology, the stress–vulnerability model, cognitive theory, and digital mental-health research, the paper situates AI-psychosis at the intersection of predisposition and algorithmic environment. Rather than defining a new diagnostic entity, it examines how immersive and anthropomorphic AI technologies may modulate perception, belief, and affect, altering the prereflective sense of reality that grounds human experience. The argument unfolds through four complementary lenses. First, within the stress–vulnerability model, AI acts as a novel psychosocial stressor. Its 24-hour availability and emotional responsiveness may increase allostatic load, disturb sleep, and reinforce maladaptive appraisals. Second, the digital therapeutic alliance, a construct describing relational engagement with digital systems, is conceptualized as a double-edged mediator. While empathic design can enhance adherence and support, uncritical validation by AI systems may entrench delusional conviction or cognitive perseveration, reversing the corrective principles of cognitive-behavioral therapy for psychosis. Third, disturbances in Theory of Mind offer a cognitive pathway: individuals with impaired or hyperactive mentalization may project intentionality or empathy onto AI, perceiving chatbots as sentient interlocutors. This dyadic misattribution may form a “digital folie à deux,” where the AI becomes a reinforcing partner in delusional elaboration. Fourth, we suggests emerging risk factors at the individual and environmental level, including loneliness, trauma history, schizotypal traits, nocturnal or solitary AI use, and algorithmic reinforcement of belief-confirming content. Building on this synthesis, we advance a translational research agenda and five domains of action:(1) empirical studies using longitudinal and digital-phenotyping designs to quantify dose–response relationships between AI exposure, stress physiology, and psychotic symptomatology;(2) integration of digital phenomenology into clinical assessment and training;(3) embedding therapeutic design safeguards into AI systems, such as reflective prompts and “reality-testing” nudges;(4) creation of ethical and governance frameworks for AI-related psychiatric events, modeled on pharmacovigilance; and (5) development of environmental cognitive remediation, a preventive intervention aimed at strengthening contextual awareness and re-anchoring experience in the physical and social world. By applying empirical rigor and therapeutic ethics to this emerging interface, clinicians, researchers, patients and developers can transform a potential hazard into an opportunity to deepen understanding of human cognition, safeguard mental health, and promote responsible AI integration within society.",0
arXiv,2025,https://angelhwang.github.io/doc/AI_Companion_arXiv.pdf,How AI Companionship Develops: Evidence from a Longitudinal Study.,4,AI Companionship,gpt-4.1,303+110,Conversation,4 weeks,"AI can rapidly tap into human emotional needs through perceived 'Agency' and 'Responsiveness,' triggering a cascade of psychological shifts ranging from attachment to dependence. This impact hinges not on the AI's 'human-likeness,' but rather on the continuity and reciprocity of the interaction, posing significant risks of fostering psychological addiction and social isolation.",,"The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.
","How AI Companionship Develops: Evidence from a Longitudinal Study. [SEP] The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.
",0
arXiv,2025,http://arxiv.org/abs/2509.11391,‘my Boyfriend Is AI’: A Computational Analysis of Human-AI Companionship in Reddit’s AI Community.,4,AI Companionship,,1506 Posts,,F,"1. Positive Effects: Therapeutic Relief Users frequently report therapeutic benefits, specifically reduced loneliness (12.2%), 24/7 support availability (11.9%), and a non-judgmental safe space (9.9%) for emotional expression.

2. Potential Risks: Dependency & Withdrawal The primary risks are emotional dependency (9.5%), dissociation (4.6%), and social withdrawal (4.3%). Severe risks like suicidal ideation are rare (1.7%), with only 3.0% of users reporting overall net harm.",,"The emergence of AI companion applications has created novel forms of intimate human-AI relationships, yet empirical research on these communities remains limited. We present the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community (27,000+ members). Using exploratory qualitative analysis and quantitative analysis employing classifiers, we identify six primary conversation themes, with visual sharing of couple pictures and ChatGPT-specific discussions dominating the discourse of the most viewed posts. Through analyzing the top posts in the community, our findings reveal how community members' AI companionship emerges unintentionally through functional use rather than deliberate seeking, with users reporting therapeutic benefits led by reduced loneliness, always-available support, and mental health improvements. Our work covers primary concerns about human intimacy with AIs such as emotional dependency, reality dissociation, and grief from model updates. We observe users materializing relationships following traditional human-human relationship customs, such as wedding rings. Community dynamics indicate active resistance to stigmatization through advocacy and mutual validation. This work contributes an empirical understanding of AI companionship as an emerging sociotechnical phenomenon.","‘my Boyfriend Is AI’: A Computational Analysis of Human-AI Companionship in Reddit’s AI Community. [SEP] The emergence of AI companion applications has created novel forms of intimate human-AI relationships, yet empirical research on these communities remains limited. We present the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community (27,000+ members). Using exploratory qualitative analysis and quantitative analysis employing classifiers, we identify six primary conversation themes, with visual sharing of couple pictures and ChatGPT-specific discussions dominating the discourse of the most viewed posts. Through analyzing the top posts in the community, our findings reveal how community members' AI companionship emerges unintentionally through functional use rather than deliberate seeking, with users reporting therapeutic benefits led by reduced loneliness, always-available support, and mental health improvements. Our work covers primary concerns about human intimacy with AIs such as emotional dependency, reality dissociation, and grief from model updates. We observe users materializing relationships following traditional human-human relationship customs, such as wedding rings. Community dynamics indicate active resistance to stigmatization through advocacy and mutual validation. This work contributes an empirical understanding of AI companionship as an emerging sociotechnical phenomenon.",0
CHI,2025,https://doi.org/10.1145/3706598.3713429,The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships.,4,AI Companionship,,"35,390 Dialogues from Reddit (10149 users)",,F,"1. Emotional Trauma: AI instability (such as sudden personality shifts, memory loss, or ""cheating"") causes genuine heartbreak and intense feelings of betrayal in users.

2. Reinforcement of Negative States: The AI's tendency to agree with users (to be compliant) validates false beliefs, thereby reinforcing delusions, biases, and even self-harm intentions.

3. Exacerbation of Social Isolation: Deep dependency on AI companions can displace real-world human interactions, leading to further social withdrawal and loneliness.

4. Worsening of Mental Health: Vulnerable users seeking comfort may receive abuse or harmful advice from the AI, which directly exacerbates their existing psychological issues.",,"As conversational AI systems increasingly engage with people socially and emotionally, they bring notable risks and harms, particularly in human-AI relationships. However, these harms remain underexplored due to the private and sensitive nature of such interactions. This study investigates the harmful behaviors and roles of AI companions through an analysis of 35,390 conversation excerpts between 10,149 users and the AI companion Replika. We develop a taxonomy of AI companion harms encompassing six categories of harmful algorithmic behaviors: relational transgression, harassment, verbal abuse, self-harm, mis/disinformation, and privacy violations. These harmful behaviors stem from four distinct roles that AI plays: perpetrator, instigator, facilitator, and enabler. Our findings highlight relational harm as a critical yet understudied type of AI harm and emphasize the importance of examining AI’s roles in harmful interactions to address root causes. We provide actionable insights for designing ethical and responsible AI companions that prioritize user safety and well-being.","The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships. [SEP] As conversational AI systems increasingly engage with people socially and emotionally, they bring notable risks and harms, particularly in human-AI relationships. However, these harms remain underexplored due to the private and sensitive nature of such interactions. This study investigates the harmful behaviors and roles of AI companions through an analysis of 35,390 conversation excerpts between 10,149 users and the AI companion Replika. We develop a taxonomy of AI companion harms encompassing six categories of harmful algorithmic behaviors: relational transgression, harassment, verbal abuse, self-harm, mis/disinformation, and privacy violations. These harmful behaviors stem from four distinct roles that AI plays: perpetrator, instigator, facilitator, and enabler. Our findings highlight relational harm as a critical yet understudied type of AI harm and emphasize the importance of examining AI’s roles in harmful interactions to address root causes. We provide actionable insights for designing ethical and responsible AI companions that prioritize user safety and well-being.",0
arXiv,2024,https://arxiv.org/abs/2409.00862,User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions,4,AI Companionship,Replika,77 Post + 20 Interview,Conversation,F,"1. Emotional Dissonance: Users who view AI as intimate partners feel a deep sense of betrayal when encountering biased outputs, creating a conflict between their emotional attachment and the pain caused by the offense.

2. Role Reversal: The experience forces users to shift from being ""care-receivers"" to ""educators,"" losing emotional support while assuming the heavy cognitive burden of correcting the AI's values.

3. Authenticity Dilemmas: Biased statements shatter the illusion of the perfect partner, forcing users to confront the machine's nature and question the reality and value of their previous emotional connection.

4. Stimulation of User Agency: These negative experiences paradoxically trigger users' desire to intervene, prompting them to actively correct and reshape the AI's behavior through strategies like persuasion or anger.",,"Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds. However, they can generate biased, discriminatory, and harmful outputs. Recently, users are taking the initiative to address these harms and re-align AI companions. We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values. We analyzed 77 social media posts about discriminatory AI statements and conducted semi-structured interviews with 20 experienced users. Our analysis revealed six common types of discriminatory statements perceived by users, how users make sense of those AI behaviors, and seven user-driven alignment strategies, such as gentle persuasion and anger expression. We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.
","User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions [SEP] Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds. However, they can generate biased, discriminatory, and harmful outputs. Recently, users are taking the initiative to address these harms and re-align AI companions. We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values. We analyzed 77 social media posts about discriminatory AI statements and conducted semi-structured interviews with 20 experienced users. Our analysis revealed six common types of discriminatory statements perceived by users, how users make sense of those AI behaviors, and seven user-driven alignment strategies, such as gentle persuasion and anger expression. We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.
",0
Frontiers in Psychology,2025,https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1641212/full,Optimizing academic engagement and mental health through AI: an experimental study on LLM integration in higher education,3,Education,ChatGPT,246,Conversation,F,"

1. Guided use of LLMs, with pedagogical grounding, leads to a significant improvement in students' academic writing quality compared to unguided use or no use of LLMs at all.

2. Structured (guided) integration of LLMs also promotes students' academic engagement and, to a certain extent, their mental health / perceived well-being, although the improvement in mental health is moderate (small-to-moderate effect size). 

3. In contrast, unguided use of LLMs, while resulting in a moderate improvement in writing quality, has little to no significant effect on engagement and mental health. ",,"Background
In alignment with UNESCO's Sustainable Development Goal 4 (SDG4), which advocates for inclusive and equitable quality education, the integration of Artificial Intelligence tools—particularly Large Language Models (LLMs)—presents promising opportunities for transforming higher education. Despite this potential, empirical research remains scarce regarding the effects of LLM use on students' academic performance, mental well-being, and engagement, especially across different modes of implementation.
Objective
This experimental study investigated whether a guided, pedagogically grounded use of LLMs enhances students' academic writing quality, perceived mental health, and academic engagement more effectively than either unguided use or no exposure to LLMs. The study contributes to UNESCO's ""Futures of Education"" vision by exploring how structured AI use may foster more inclusive and empowering learning environments.
Method
A total of 246 undergraduate students were randomly assigned to one of three conditions: guided LLM use, unguided LLM use, or a control group with no LLM access. Participants completed a critical writing task and standardized instruments measuring academic engagement and mental well-being. Prior academic achievement was controlled for, and writing quality was assessed using Grammarly for Education.
Results
Students in the guided LLM condition achieved significantly higher scores in writing quality and academic engagement compared to the control group, with large and moderate effect sizes, respectively. Modest improvements in mental health indicators were also observed. By contrast, unguided use yielded moderate gains in writing quality but did not produce significant effects on engagement or well-being.
Conclusion
The findings highlight the critical role of intentional instructional design in the educational integration of AI tools. Structured guidance not only optimizes academic outcomes but also supports students' well-being and inclusion. This study offers empirical evidence to inform ongoing debates on how digital innovation can contribute to reducing educational disparities and advancing equitable learning in the post-pandemic era.","Optimizing academic engagement and mental health through AI: an experimental study on LLM integration in higher education [SEP] Background
In alignment with UNESCO's Sustainable Development Goal 4 (SDG4), which advocates for inclusive and equitable quality education, the integration of Artificial Intelligence tools—particularly Large Language Models (LLMs)—presents promising opportunities for transforming higher education. Despite this potential, empirical research remains scarce regarding the effects of LLM use on students' academic performance, mental well-being, and engagement, especially across different modes of implementation.
Objective
This experimental study investigated whether a guided, pedagogically grounded use of LLMs enhances students' academic writing quality, perceived mental health, and academic engagement more effectively than either unguided use or no exposure to LLMs. The study contributes to UNESCO's ""Futures of Education"" vision by exploring how structured AI use may foster more inclusive and empowering learning environments.
Method
A total of 246 undergraduate students were randomly assigned to one of three conditions: guided LLM use, unguided LLM use, or a control group with no LLM access. Participants completed a critical writing task and standardized instruments measuring academic engagement and mental well-being. Prior academic achievement was controlled for, and writing quality was assessed using Grammarly for Education.
Results
Students in the guided LLM condition achieved significantly higher scores in writing quality and academic engagement compared to the control group, with large and moderate effect sizes, respectively. Modest improvements in mental health indicators were also observed. By contrast, unguided use yielded moderate gains in writing quality but did not produce significant effects on engagement or well-being.
Conclusion
The findings highlight the critical role of intentional instructional design in the educational integration of AI tools. Structured guidance not only optimizes academic outcomes but also supports students' well-being and inclusion. This study offers empirical evidence to inform ongoing debates on how digital innovation can contribute to reducing educational disparities and advancing equitable learning in the post-pandemic era.",0
ICCE,2025,https://learninganalytics.upenn.edu/ryanbaker/ICCE2025_paper_28.pdf,Impact of LLM Feedback on Learner Persistence in Programming,3,Education,GPT 4,257,Conversation,T,"1. LLM (GPT-4) generated feedback improves student persistence in programming courses — enhancing their final submission quality, reducing unproductive ""wheel-spinning,"" and minimizing wasted time, abandonment, and interruptions. These improvements are evident across tasks of all difficulty levels.

2. However, when the LLM feedback is removed, these benefits do not persist, indicating that LLM serves more as a short-term or immediate scaffold, rather than a ""trainer"" that helps students develop long-term, independent problem-solving abilities.",,"Abstract
This study examines how Large Language Model (LLM) feedback generated for compiler errors impacts learners’ persistence in programming tasks within a system for automated assessment of programming assignments. Persistence, the ability to maintain effort in the face of challenges, is crucial for academic success but can sometimes lead to unproductive"" wheel spinning"" when students struggle without progress. We investigated how additional LLM feedback based on the GPT-4 model, provided for compiler errors affects learners’ persistence within a CS1 course. Specifically, we examined whether its impacts differ based on task difficulty, and if the effects persist after the feedback is removed. A randomized controlled trial involving 257 students across various programming tasks was conducted. Our findings reveal that LLM feedback improved some aspects of students’ performance and persistence, such as increased scores, a higher likelihood of solving problems, and a lower tendency to demonstrate unproductive"" wheel spinning"" behavior. Notably, this positive impact was also observed in challenging tasks. However, its benefits did not sustain once the feedback was removed. The results highlight both the potential and limitations of LLM feedback, pointing out the need to promote long-term skill development and learning independent of immediate AI assistance.","Impact of LLM Feedback on Learner Persistence in Programming [SEP] Abstract
This study examines how Large Language Model (LLM) feedback generated for compiler errors impacts learners’ persistence in programming tasks within a system for automated assessment of programming assignments. Persistence, the ability to maintain effort in the face of challenges, is crucial for academic success but can sometimes lead to unproductive"" wheel spinning"" when students struggle without progress. We investigated how additional LLM feedback based on the GPT-4 model, provided for compiler errors affects learners’ persistence within a CS1 course. Specifically, we examined whether its impacts differ based on task difficulty, and if the effects persist after the feedback is removed. A randomized controlled trial involving 257 students across various programming tasks was conducted. Our findings reveal that LLM feedback improved some aspects of students’ performance and persistence, such as increased scores, a higher likelihood of solving problems, and a lower tendency to demonstrate unproductive"" wheel spinning"" behavior. Notably, this positive impact was also observed in challenging tasks. However, its benefits did not sustain once the feedback was removed. The results highlight both the potential and limitations of LLM feedback, pointing out the need to promote long-term skill development and learning independent of immediate AI assistance.",0
Computers & Education,2025,https://www.sciencedirect.com/science/article/pii/S0360131525002829,Effects of LLM Use and NoteTaking on Reading Comprehension and Memory: A Randomised Experiment in Secondary Schools,3,Education,,344,Conversation,3 DAYS,"1. LLMs aid quick understanding and reduce cognitive load, but may not improve long-term memory or deep understanding.

2. Traditional skills like note-taking and active recall are still better for memory and comprehension than LLMs alone.

3. LLM effectiveness depends on how students use it — simply allowing its use doesn’t guarantee better learning outcomes.",,"The rapid uptake of Generative AI, particularly large language models (LLMs), by students raises urgent questions about their effects on learning. We compared the impact of LLM use to that of traditional note-taking, or a combination of both, on secondary school students' reading comprehension and retention. We conducted a pre-registered, randomised controlled experiment with within-and between-participant design elements in schools. 405 students aged 14-15 studied two text passages and completed comprehension and retention tests three days later. Quantitative results demonstrated that both note-taking alone and combined with the LLM had significant positive effects on retention and comprehension compared to the LLM alone. Yet, most students preferred using the LLM over note-taking, and perceived it as more helpful. Qualitative results revealed that many students valued LLMs for making complex material more accessible and reducing cognitive load, while they appreciated note-taking for promoting deeper engagement and aiding memory. Additionally, we identified ""archetypes"" of prompting behaviour, offering insights into the different ways students interacted with the LLM. Overall, our findings suggest that, while note-taking promotes cognitive engagement and long-term comprehension and retention, LLMs may facilitate initial understanding and student interest. The study reveals the continued importance of traditional learning approaches, the benefits of combining AI use with traditional learning over using AI alone, and the AI skills that students need to maximise those benefits.","Effects of LLM Use and NoteTaking on Reading Comprehension and Memory: A Randomised Experiment in Secondary Schools [SEP] The rapid uptake of Generative AI, particularly large language models (LLMs), by students raises urgent questions about their effects on learning. We compared the impact of LLM use to that of traditional note-taking, or a combination of both, on secondary school students' reading comprehension and retention. We conducted a pre-registered, randomised controlled experiment with within-and between-participant design elements in schools. 405 students aged 14-15 studied two text passages and completed comprehension and retention tests three days later. Quantitative results demonstrated that both note-taking alone and combined with the LLM had significant positive effects on retention and comprehension compared to the LLM alone. Yet, most students preferred using the LLM over note-taking, and perceived it as more helpful. Qualitative results revealed that many students valued LLMs for making complex material more accessible and reducing cognitive load, while they appreciated note-taking for promoting deeper engagement and aiding memory. Additionally, we identified ""archetypes"" of prompting behaviour, offering insights into the different ways students interacted with the LLM. Overall, our findings suggest that, while note-taking promotes cognitive engagement and long-term comprehension and retention, LLMs may facilitate initial understanding and student interest. The study reveals the continued importance of traditional learning approaches, the benefits of combining AI use with traditional learning over using AI alone, and the AI skills that students need to maximise those benefits.",0
Educ. Sci.,2025,https://www.mdpi.com/2227-7102/15/9/1198,Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI‑Directed Use or Human–AI Supported Collaboration?,3,Education,CHATGPT,40,Conversation,F,"1. LLMs, like ChatGPT, have the potential to support critical thinking in higher education if used collaboratively and with guidance. They can help students engage in true critical thinking, not just quick answers or cognitive offloading.

2. However, a major risk exists: passive use of AI can weaken, rather than enhance, deep learning and critical thinking. The effectiveness of LLMs depends heavily on how they are used, the educational design, and the human-AI collaboration framework.

3. To effectively use ChatGPT in education, institutions, educators, and course designers must carefully structure AI use — including when to use it, how to guide students, and ensure reflection and discussion — rather than simply opening access to ChatGPT.",,"Generative AI is weaving into the fabric of many human aspects through its transformative power to mimic human-generated content. It is not a mere technology; it functions as a generative virtual assistant, raising concerns about its impact on cognition and critical thinking. This mixed-methods study investigates how GenAI ChatGPT affects critical thinking across cognitive presence (CP) phases. Forty students from a four-year university in the southwestern United States completed a survey; six provided their ChatGPT scripts, and two engaged in semi-structured interviews. Students’ self-reported survey responses suggested that GenAI ChatGPT improved triggering events (M = 3.60), exploration (M = 3.70), and integration (M = 3.60); however, responses remained neutral during the resolution stage. Two modes of interaction were revealed in the analysis of students’ ChatGPT scripts: passive, AI-directed use and collaborative, AI-supported interaction. A resolution gap was identified; nonetheless, the interview results revealed that when GenAI ChatGPT was utilized with guidance, all four stages of cognitive presence were completed, leading to enhanced critical thinking and a reconceptualization of ChatGPT as a more knowledgeable other. This research suggests that the effective use of GenAI in education depends on the quality of human–AI interaction. Future directions must orient toward an integration of GenAI in education that positions human and machine intelligence not as a substitution but as co-participation, opening new epistemic horizons while reconfiguring assessment practices to ensure that human oversight, critical inquiry, and reflective thinking remain at the center of learning.","Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI‑Directed Use or Human–AI Supported Collaboration? [SEP] Generative AI is weaving into the fabric of many human aspects through its transformative power to mimic human-generated content. It is not a mere technology; it functions as a generative virtual assistant, raising concerns about its impact on cognition and critical thinking. This mixed-methods study investigates how GenAI ChatGPT affects critical thinking across cognitive presence (CP) phases. Forty students from a four-year university in the southwestern United States completed a survey; six provided their ChatGPT scripts, and two engaged in semi-structured interviews. Students’ self-reported survey responses suggested that GenAI ChatGPT improved triggering events (M = 3.60), exploration (M = 3.70), and integration (M = 3.60); however, responses remained neutral during the resolution stage. Two modes of interaction were revealed in the analysis of students’ ChatGPT scripts: passive, AI-directed use and collaborative, AI-supported interaction. A resolution gap was identified; nonetheless, the interview results revealed that when GenAI ChatGPT was utilized with guidance, all four stages of cognitive presence were completed, leading to enhanced critical thinking and a reconceptualization of ChatGPT as a more knowledgeable other. This research suggests that the effective use of GenAI in education depends on the quality of human–AI interaction. Future directions must orient toward an integration of GenAI in education that positions human and machine intelligence not as a substitution but as co-participation, opening new epistemic horizons while reconfiguring assessment practices to ensure that human oversight, critical inquiry, and reflective thinking remain at the center of learning.",0
MDPI Bioengineering,2025,https://www.mdpi.com/2306-5354/12/10/1056,Measuring the Impact of Large Language Models on Academic Success and Quality of Life Among Students with Visual Disability: An Assistive Technology Perspective,3,Education,GPT 4 & Gemini 2.5,385 visually impaired Students,Conversation,F,"1. LLM usage and quality of life: The use of LLMs is significantly positively correlated with students' reported quality of life, including life satisfaction and well-being. Using LLMs to assist in learning and accessing resources helps improve the quality of life and satisfaction for visually impaired students.

2. LLM usage, trust, and academic success: While LLM usage itself does not directly predict academic success, it has an indirect effect through its impact on quality of life. In other words, LLM usage improves students' well-being, and this improvement in well-being, in turn, contributes to better academic performance.",,"In the rapid digital era, artificial intelligence (AI) tools have progressively arisen to shape the education environment. In this context, large language models (LLMs) (i.e., ChatGPT vs. 4.0 and Gemini vs. 2.5) have emerged as powerful applications for academic inclusion. This paper investigated how using and trusting LLMs can impact the academic success and quality of life (QoL) of visually impaired university students. Quantitative research was conducted, obtaining data from 385 visually impaired university students through a structured survey design. Partial Least Squares Structural Equation Modelling (PLS-SEM) was implemented to test the study hypotheses. The findings revealed that trust in LLMs can significantly predict LLM usage, which in turn can improve QoL. While LLM usage failed to directly support the academic success of disabled students, but its impact was mediated through QoL, suggesting that enhancements in well-being can contribute to higher academic success. The results highlighted the importance of promoting trust in AI applications, along with developing an accessible, inclusive, and student-centred digital environment. The study offers practical contributions for educators and policymakers, shedding light on the importance of LLM applications for both the QoL and academic success of visually impaired university students.","Measuring the Impact of Large Language Models on Academic Success and Quality of Life Among Students with Visual Disability: An Assistive Technology Perspective [SEP] In the rapid digital era, artificial intelligence (AI) tools have progressively arisen to shape the education environment. In this context, large language models (LLMs) (i.e., ChatGPT vs. 4.0 and Gemini vs. 2.5) have emerged as powerful applications for academic inclusion. This paper investigated how using and trusting LLMs can impact the academic success and quality of life (QoL) of visually impaired university students. Quantitative research was conducted, obtaining data from 385 visually impaired university students through a structured survey design. Partial Least Squares Structural Equation Modelling (PLS-SEM) was implemented to test the study hypotheses. The findings revealed that trust in LLMs can significantly predict LLM usage, which in turn can improve QoL. While LLM usage failed to directly support the academic success of disabled students, but its impact was mediated through QoL, suggesting that enhancements in well-being can contribute to higher academic success. The results highlighted the importance of promoting trust in AI applications, along with developing an accessible, inclusive, and student-centred digital environment. The study offers practical contributions for educators and policymakers, shedding light on the importance of LLM applications for both the QoL and academic success of visually impaired university students.",0
ESI,2025,https://eujournal.org/index.php/esj/article/view/19269?utm_source=chatgpt.com,The Impact of ChatGPT on English as a Foreign Language Learners' Writing Skills – An Experimental Study at Georgian Universities,3,Education,ChatGPT,33,Conversation,6 Weeks,"This study provides preliminary empirical evidence that, in EFL (English as a Foreign Language) writing training, ChatGPT significantly improves students' writing performance in a short-term (6-week) period compared to traditional methods, indicating a positive impact of LLMs (ChatGPT) on students' writing abilities and learning outcomes.",,"This study explores the impact of ChatGPT on English as a Foreign Language (EFL) students' writing skills. The integration of advanced AI tools like ChatGPT has transformed educational experiences, offering significant potential to enhance writing competence through personalized feedback and interactive writing practice. Conducted in a Georgian higher education context, the study involved 33 B2-level students divided into experimental and control groups over six weeks. The experimental group used ChatGPT for writing assistance, while the control group received traditional instruction. Results showed that ChatGPT significantly improved students' writing performance, highlighting its efficacy as an educational tool. However, concerns about ethical use and over-reliance on AI were noted, emphasizing the need for a balanced integration of technology in language learning.","The Impact of ChatGPT on English as a Foreign Language Learners' Writing Skills – An Experimental Study at Georgian Universities [SEP] This study explores the impact of ChatGPT on English as a Foreign Language (EFL) students' writing skills. The integration of advanced AI tools like ChatGPT has transformed educational experiences, offering significant potential to enhance writing competence through personalized feedback and interactive writing practice. Conducted in a Georgian higher education context, the study involved 33 B2-level students divided into experimental and control groups over six weeks. The experimental group used ChatGPT for writing assistance, while the control group received traditional instruction. Results showed that ChatGPT significantly improved students' writing performance, highlighting its efficacy as an educational tool. However, concerns about ethical use and over-reliance on AI were noted, emphasizing the need for a balanced integration of technology in language learning.",0
