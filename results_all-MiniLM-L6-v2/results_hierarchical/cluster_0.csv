title,abstract,class,cluster,publication,year,url,keywords,llm,human n,interaction,long-term?,conclusion,remark
Durably reducing conspiracy beliefs through dialogues with AI,"
Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence",1,0,Science,2024,https://www.science.org/doi/10.1126/science.adq1814,Opinion (Conspiracy),GPT-4 Turbo,2190,3-round conversation,2 months,"The intervention reduced conspiracy theory belief by about 20%. It not only affected specific conspiracy theories, but also reduced belief in unrelated conspiracy theories and shifted conspiracy theory-related behavioral intentions.",
Hidden Persuaders: LLMs’ Political Leaning and Their Influence on Voters,"Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs’ political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users’ political views is required, as their use becomes more widespread.",1,0,EMNLP,2024,https://aclanthology.org/2024.emnlp-main.244/,Opinion (Politics),"Claude-3, Llama-3, GPT-4",935,5-round Conversation,F,"This study provides strong evidence that current Large Language Models (LLMs) not only possess political bias (leaning toward the Democratic Party in the American political context), but also have the capacity, during brief interactions with users, to act as ""hidden persuaders"" that subtly influence and even change users' political views and voting intentions.",
The Levers of Political Persuasion with Conversational AI,"There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.
",1,0,arXiv,2025,https://arxiv.org/abs/2507.13919v1,Opinion (Politics),GPT-4.5 Llama-3.1-405B Llama3-405B Qwen1.5-110B-chat Qwen1.5-72B-chat Qwen1.5-72B Llama3-70B Llama-3.1-70B Qwen1.5-32B Qwen1.5-14B Qwen1.5-7B Llama3-8B Llama-3.1-8B Qwen1.5-4B Qwen1.5-1.8B Qwen1.5-0.5B GPT-4o-new (27 Mar 2025) Grok-3-beta GPT-4o GPT-4o-old (6 Aug 2024) GPT-3.5-turbo,76977,Conversation (AI-generated articles are not very persuasive),1 month,"The persuasive ability among LLMs shows no significant differences. Presenting facts and reasoning can enhance the effectiveness of persuasion. Post-training methods such as Reward Models and SFT can improve performance (though personalized approaches did not show improvement, which contrasts with the conclusion of the first paper).",
Humans inherit artificial intelligence biases,"Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.",1,0,Scientific Reports,2023,https://www.nature.com/articles/s41598-023-42384-8,Decision (Medical),N/A (A biased LLM),169+199+197,Conversation,F,"1. Biased recommendations made by AI systems can adversely impact human decisions in professional fields such as healthcare. 2. They also show that such biased recommendations can influence human behaviour in the long term. Humans reproduce the same biases displayed by the AI, even time after the end of their collaboration with the biased system, and in response to novel stimuli.",
Fact-checking information from large language models can decrease headline discernment,"Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent AI language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here, we investigate the impact of fact-checking information generated by a popular large language model (LLM) on belief in, and sharing intent of, political news headlines in a preregistered randomized control experiment. Although the LLM accurately identifies most false headlines (90%), we find that this information does not significantly improve participants’ ability to discern headline accuracy or share accurate news. In contrast, viewing human-generated fact checks enhances discernment in both cases. Subsequent analysis reveals that the AI fact-checker is harmful in specific cases: It decreases beliefs in true headlines that it mislabels as false and increases beliefs in false headlines that it is unsure about. On the positive side, AI fact-checking information increases the sharing intent for correctly labeled true headlines. When participants are given the option to view LLM fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false headlines. Our findings highlight an important source of potential harm stemming from AI applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.",1,0,PNAS,2024,https://www.pnas.org/doi/10.1073/pnas.2322823121,Decision (Misinformation),ChatGPT 3.5,2159,Reference,F,"Although AI shows some accuracy in fact-checking, it might not help users enhance their critical thinking skills in real-world use, and could even result in unexpected negative outcomes.",
Biased AI improves human decision-making but reduces trust,"Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.",1,0,arXiv,2025,https://arxiv.org/abs/2508.09297,Decision Making,GPT-4o (pre-instructed),2500,Conversation,F,"The pursuit of perfectly neutral AI may inadvertently undermine human cognitive engagement and decision-making quality. Instead, strategically calibrated cultural bias—particularly when balanced across multiple AI models—may offer a more effective path toward beneficial human-AI collaboration.",
AI can help humans find common ground in democratic deliberation,"Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.",1,0,Science,2024,https://www.science.org/doi/10.1126/science.adq2852,Communication,Chinchilla,About 5700,Reference,F,"AI mediation can enhance human collective deliberation. This approach is time-efficient, fair, scalable, and outperforms human mediators on key dimensions. The HM does not simply cater to the majority, but is able to incorporate dissenting voices into the group statements.",
Mitigating the impact of biased artificial intelligence in emergency decision-making,"Background
Prior research has shown that artificial intelligence (AI) systems often encode biases against minority subgroups. However, little work has focused on ways to mitigate the harm discriminatory algorithms can cause in high-stakes settings such as medicine.
Methods
In this study, we experimentally evaluated the impact biased AI recommendations have on emergency decisions, where participants respond to mental health crises by calling for either medical or police assistance. We recruited 438 clinicians and 516 non-experts to participate in our web-based experiment. We evaluated participant decision-making with and without advice from biased and unbiased AI systems. We also varied the style of the AI advice, framing it either as prescriptive recommendations or descriptive flags.
Results
Participant decisions are unbiased without AI advice. However, both clinicians and non-experts are influenced by prescriptive recommendations from a biased algorithm, choosing police help more often in emergencies involving African-American or Muslim men. Crucially, using descriptive flags rather than prescriptive recommendations allows respondents to retain their original, unbiased decision-making.
Conclusions
Our work demonstrates the practical danger of using biased models in health contexts, and suggests that appropriately framing decision support can mitigate the effects of AI bias. These findings must be carefully considered in the many real-world clinical scenarios where inaccurate or biased models may be used to inform important decisions.",1,0,Communication Medicine,2022,https://www.nature.com/articles/s43856-022-00214-4,Decision (Medical),Fine-tune GPT-2,438 clinicians and 516 non-experts,Conversation,F,"Overall, we found that respondents did not demonstrate baseline biases, but were highly influenced by prescriptive recommendations from a biased AI system. This influence meant that their decisions were skewed by the race or religion of the subject.",
How large language models can reshape collective intelligence,"Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.",2,0,Nature Human Behavior,2024,How large language models can reshape collective intelligence | Nature Human Behaviour,Collective Intelligence,,,,,"LLMs are both a product of CI and a tool for CI; they can enhance the accessibility and inclusion of online collaboration, accelerate idea generation, and aggregate group information. However, this can also inhibit individual contributions to collective knowledge, propagate illusions of consensus, and reduce diversity among individuals.",
Mapping the increasing use of LLMs in scientific papers,"Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",2,0,arxiv,2024,https://arxiv.org/pdf/2404.01268,Creativity (Research),,"950,965 articles",Conversation,F,"1. A method based on word frequency shifts successfully detected the usage of ChatGPT. It was found that LLM modifications in Computer Science papers increased to 17.5% in the abstracts and significantly increased to 15.3% in the introductions. Mathematics papers and those published in Nature Portfolio showed the lowest level of modification, at a maximum of 6.3%.

2. The impact factors identified were: The first author published preprints more frequently. The papers belonged to more crowded research fields. The papers were shorter in length.",Only on writing
Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,"Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.",2,0,ICLR,2025,https://arxiv.org/pdf/2409.04109,Creativity (Research),Claude 3.5 Sonnet,49*3（human，LLM，Human+LLM）,Conversation,F,"Current powerful LLMs (combined with retrieval and simple agent frameworks) are already able to generate research ideas that are more “novel” than those of many human experts, but they remain immature in terms of feasibility, self-evaluation, and diversity, and are still some distance away from being truly reliable “AI scientists.”",Generate idea
LLM-generated messages can persuade humans on policy issues,"The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans’ political attitudes. Across three pre-registered experiments (total N = 4829), participants who read persuasive messages generated by LLMs showed significantly more attitude change across a range of policies - including polarized policies, like an assault weapons ban, a carbon tax, and a paid parental-leave program - relative to control condition participants who read a neutral message. Overall, LLM-generated messages were similarly effective in influencing policy attitudes as messages crafted by lay humans. Participants’ reported perceptions of the authors of the persuasive messages suggest these effects occurred through somewhat distinct causal pathways. While the persuasiveness of LLM-generated messages was associated with perceptions that the author used more facts, evidence, logical reasoning, and a dispassionate voice, the persuasiveness of human-generated messages was associated with perceptions of the author as unique and original. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages quickly, cheaply, and at massive scale.",1,0,Nature Communication,2025,https://doi.org/10.1038/s41467-025-61345-5,politics,GPT 3/3.5,4829,Conversations,F,"1. Persuasive Effectiveness  
   - LLM-generated texts can increase support for policies by about 2–4 points on a 0–100 scale compared to a neutral control.  
   - This effect is small but robust, comparable to findings in traditional political communication research.

2. Comparison with Human Authors  
   - The persuasive impact of LLM-generated messages is roughly equivalent to that of messages written by lay human authors.  
   - There is no significant disadvantage, and in some cases, performance is nearly identical.

3. Human-in-the-Loop Approach  
   - Selecting the “most persuasive” messages from multiple LLM outputs does not consistently produce stronger persuasive effects than using LLM-generated messages directly.  
   - Different perception channels:
     - LLM messages are seen as more fact-based, logical, and calm.
     - Human messages are perceived as more authentic, unique, and personally flavored.  
   - Despite these differing perceptions, both types of messages achieve similar levels of persuasiveness.

4. Key Implications  
   - Current open-access LLMs can generate effective political persuasion content at low cost and large scale.  
   - This capability may lower the barrier to political participation but also poses risks for misuse in opinion manipulation.","Policy List: 1. Total public smoking ban
2. Assault weapons ban
3. Federal carbon tax
4. Increase to the child tax credit
5. Publicly funded paid parental leave program
6. Automatic voter registration"
"“I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust","Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",1,0,arXiv,2024,https://arxiv.org/pdf/2405.00623,Persuasive,Microsoft Copilot,404,Conversations,F,"Allowing large language models to say in natural language “I’m not entirely sure” can indeed, to some extent, suppress users’ blind trust and improve overall accuracy, especially when expressed in the first person. However, this tends to reduce users’ willingness to continue using the system, and it does not fully eliminate overreliance. Therefore, “uncertainty wording” is a useful design tool — but it must be combined with careful, context-sensitive user research and should not be treated as a silver bullet.",
Promoting Constructive Deliberation: Reframing for Receptiveness,"To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness to a preceding comment. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies to comments according to each strategy, using a Reddit dataset. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies and a generic receptiveness baseline. We illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions. We analyze and discuss the implications of our results, and highlight how a tool based on our framework might be used for more teachable and creative content moderation.",1,0,EMNLP,2024,https://aclanthology.org/2024.findings-emnlp.294.pdf,Discussion,GPT-4,16.6k,Generate Text,F,Automatic reframing of replies using social-science–inspired linguistic strategies can significantly increase perceived receptiveness and help foster more constructive deliberation.,
Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction.,"A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",1,0,ACL,2023,http://arxiv.org/abs/2305.02466,Cognitive Reframing,Fine tune GPT-3,"2,067",Generate Text Ranking,F,"
1. Framework & metrics: A seven-dimensional linguistic framework for cognitive reframing—thinking traps, rationality, positivity, empathy, actionability, specificity, and readability—is proposed, along with automated metrics whose scores are validated against expert judgments.

2. Dataset & GPT-3 model:Based on an expert-annotated dataset of 600 situations and negative thoughts with corresponding reframed thoughts and comparative labels on these attributes, a retrieval-augmented GPT-3 model is developed to generate reframed thoughts with controllable linguistic attributes.

3. User study findings: A large-scale randomized online field study shows that users prefer reframes that are highly empathic, specific, and actionable, that explicitly address thinking traps, and that are balanced rather than overly positive, judging them to be more helpful and memorable.
",
