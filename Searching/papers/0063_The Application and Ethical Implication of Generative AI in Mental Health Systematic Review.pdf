<!doctype html><html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D"><head ><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width, initial-scale=1"><meta data-n-head="ssr" name="msapplication-TileColor" content="#247CB3"><meta data-n-head="ssr" name="msapplication-TileImage" content="https://asset.jmir.pub/assets/static/images/mstile-144x144.png"><meta data-n-head="ssr" name="description" content="Background: Mental health disorders affect an estimated 1 in 8 individuals globally, yet traditional interventions often face barriers, such as limited accessibility, high costs, and persistent stigma. Recent advancements in generative artificial intelligence (GenAI) have introduced AI systems capable of understanding and producing humanlike language in real time. These developments present new opportunities to enhance mental health care.
Objective: We aimed to systematically examine the current applications of GenAI in mental health, focusing on 3 core domains: diagnosis and assessment, therapeutic tools, and clinician support. In addition, we identified and synthesized key ethical issues reported in the literature.
Methods: We conducted a comprehensive literature search, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines, in PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar databases to identify peer-reviewed studies published from October 1, 2019, to September 30, 2024. After screening 783 records, 79 (10.1%) studies met the inclusion criteria.
Results: The number of studies on GenAI applications in mental health has grown substantially since 2023. Studies on diagnosis and assessment (37/79, 47%) primarily used GenAI models to detect depression and suicidality through text data. Studies on therapeutic applications (20/79, 25%) investigated GenAI-based chatbots and adaptive systems for emotional and behavioral support, reporting promising outcomes but revealing limited real-world deployment and safety assurance. Clinician support studies (24/79, 30%) explored GenAI’s role in clinical decision-making, documentation and summarization, therapy support, training and simulation, and psychoeducation. Ethical concerns were consistently reported across the domains. On the basis of these findings, we proposed an integrative ethical framework, GenAI4MH, comprising 4 core dimensions—data privacy and security, information integrity and fairness, user safety, and ethical governance and oversight—to guide the responsible use of GenAI in mental health contexts.
Conclusions: GenAI shows promise in addressing the escalating global demand for mental health services. They may augment traditional approaches by enhancing diagnostic accuracy, offering more accessible support, and reducing clinicians’ administrative burden. However, to ensure ethical and effective implementation, comprehensive safeguards—particularly around privacy, algorithmic bias, and responsible user engagement—must be established.
"><meta data-n-head="ssr" name="keywords" content="mental health; large language models; generative ai; mental health detection and diagnosis; therapeutic chatbots"><meta data-n-head="ssr" name="DC.Title" content="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review"><meta data-n-head="ssr" name="DC.Subject" content="mental health; large language models; generative ai; mental health detection and diagnosis; therapeutic chatbots"><meta data-n-head="ssr" name="DC.Description" content="Background: Mental health disorders affect an estimated 1 in 8 individuals globally, yet traditional interventions often face barriers, such as limited accessibility, high costs, and persistent stigma. Recent advancements in generative artificial intelligence (GenAI) have introduced AI systems capable of understanding and producing humanlike language in real time. These developments present new opportunities to enhance mental health care.
Objective: We aimed to systematically examine the current applications of GenAI in mental health, focusing on 3 core domains: diagnosis and assessment, therapeutic tools, and clinician support. In addition, we identified and synthesized key ethical issues reported in the literature.
Methods: We conducted a comprehensive literature search, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines, in PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar databases to identify peer-reviewed studies published from October 1, 2019, to September 30, 2024. After screening 783 records, 79 (10.1%) studies met the inclusion criteria.
Results: The number of studies on GenAI applications in mental health has grown substantially since 2023. Studies on diagnosis and assessment (37/79, 47%) primarily used GenAI models to detect depression and suicidality through text data. Studies on therapeutic applications (20/79, 25%) investigated GenAI-based chatbots and adaptive systems for emotional and behavioral support, reporting promising outcomes but revealing limited real-world deployment and safety assurance. Clinician support studies (24/79, 30%) explored GenAI’s role in clinical decision-making, documentation and summarization, therapy support, training and simulation, and psychoeducation. Ethical concerns were consistently reported across the domains. On the basis of these findings, we proposed an integrative ethical framework, GenAI4MH, comprising 4 core dimensions—data privacy and security, information integrity and fairness, user safety, and ethical governance and oversight—to guide the responsible use of GenAI in mental health contexts.
Conclusions: GenAI shows promise in addressing the escalating global demand for mental health services. They may augment traditional approaches by enhancing diagnostic accuracy, offering more accessible support, and reducing clinicians’ administrative burden. However, to ensure ethical and effective implementation, comprehensive safeguards—particularly around privacy, algorithmic bias, and responsible user engagement—must be established.
"><meta data-n-head="ssr" name="DC.Publisher" content="JMIR Mental Health"><meta data-n-head="ssr" name="DC.Publisher.Address" content="JMIR Publications // 130 Queens Quay East, Unit 1100 // Toronto, ON, M5A 0P6"><meta data-n-head="ssr" name="DC.Date" scheme="ISO8601" content="2025-06-27"><meta data-n-head="ssr" name="DC.Type" content="Text.Serial.Journal"><meta data-n-head="ssr" name="DC.Format" scheme="IMT" content="text/xml"><meta data-n-head="ssr" name="DC.Identifier" content="doi:10.2196/70610"><meta data-n-head="ssr" name="DC.Language" scheme="ISO639-1" content="EN"><meta data-n-head="ssr" name="DC.Relation" content="World"><meta data-n-head="ssr" name="DC.Source" content="JMIR Ment Health 2025;12:e70610 https://mental.jmir.org/2025/1/e70610"><meta data-n-head="ssr" name="DC.Rights" content="Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (&quot;first published in the Journal of Medical Internet Research...&quot;) is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included."><meta data-n-head="ssr" property="og:title" content="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review"><meta data-n-head="ssr" property="og:type" content="article"><meta data-n-head="ssr" property="og:url" content="https://mental.jmir.org/2025/1/e70610"><meta data-n-head="ssr" property="og:image" content="https://asset.jmir.pub/assets/dae990f3af4bcdffe2910e552301457e.png"><meta data-n-head="ssr" property="og:site_name" content="JMIR Mental Health"><meta data-n-head="ssr" name="twitter:card" content="summary_large_image"><meta data-n-head="ssr" name="twitter:site" content="@jmirpub"><meta data-n-head="ssr" name="twitter:title" content="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review"><meta data-n-head="ssr" name="twitter:description" content="Background: Mental health disorders affect an estimated 1 in 8 individuals globally, yet traditional interventions often face barriers, such as limited accessibility, high costs, and persistent stigma. Recent advancements in generative artificial intelligence (GenAI) have introduced AI systems capable of understanding and producing humanlike language in real time. These developments present new opportunities to enhance mental health care.
Objective: We aimed to systematically examine the current applications of GenAI in mental health, focusing on 3 core domains: diagnosis and assessment, therapeutic tools, and clinician support. In addition, we identified and synthesized key ethical issues reported in the literature.
Methods: We conducted a comprehensive literature search, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines, in PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar databases to identify peer-reviewed studies published from October 1, 2019, to September 30, 2024. After screening 783 records, 79 (10.1%) studies met the inclusion criteria.
Results: The number of studies on GenAI applications in mental health has grown substantially since 2023. Studies on diagnosis and assessment (37/79, 47%) primarily used GenAI models to detect depression and suicidality through text data. Studies on therapeutic applications (20/79, 25%) investigated GenAI-based chatbots and adaptive systems for emotional and behavioral support, reporting promising outcomes but revealing limited real-world deployment and safety assurance. Clinician support studies (24/79, 30%) explored GenAI’s role in clinical decision-making, documentation and summarization, therapy support, training and simulation, and psychoeducation. Ethical concerns were consistently reported across the domains. On the basis of these findings, we proposed an integrative ethical framework, GenAI4MH, comprising 4 core dimensions—data privacy and security, information integrity and fairness, user safety, and ethical governance and oversight—to guide the responsible use of GenAI in mental health contexts.
Conclusions: GenAI shows promise in addressing the escalating global demand for mental health services. They may augment traditional approaches by enhancing diagnostic accuracy, offering more accessible support, and reducing clinicians’ administrative burden. However, to ensure ethical and effective implementation, comprehensive safeguards—particularly around privacy, algorithmic bias, and responsible user engagement—must be established.
"><meta data-n-head="ssr" name="twitter:image" content="https://asset.jmir.pub/assets/dae990f3af4bcdffe2910e552301457e.png"><meta data-n-head="ssr" name="citation_title" content="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review"><meta data-n-head="ssr" name="citation_journal_title" content="JMIR Mental Health"><meta data-n-head="ssr" name="citation_publisher" content="JMIR Publications Inc., Toronto, Canada"><meta data-n-head="ssr" name="citation_doi" content="10.2196/70610"><meta data-n-head="ssr" name="citation_issue" content="1"><meta data-n-head="ssr" name="citation_volume" content="12"><meta data-n-head="ssr" name="citation_firstpage" content="e70610"><meta data-n-head="ssr" name="citation_date" content="2025-06-27"><meta data-n-head="ssr" name="citation_abstract_html_url" content="https://mental.jmir.org/2025/1/e70610"><meta data-n-head="ssr" name="citation_abstract_pdf_url" content="https://mental.jmir.org/2025/1/e70610/PDF"><meta data-n-head="ssr" name="DC.Creator" content="Xi Wang"><meta data-n-head="ssr" name="DC.Contributor" content="Xi Wang"><meta data-n-head="ssr" name="DC.Contributor" content="Yujia Zhou"><meta data-n-head="ssr" name="DC.Contributor" content="Guangyu Zhou"><meta data-n-head="ssr" name="citation_authors" content="Xi Wang"><meta data-n-head="ssr" name="citation_authors" content="Yujia Zhou"><meta data-n-head="ssr" name="citation_authors" content="Guangyu Zhou"><title>JMIR Mental Health - The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review</title><link data-n-head="ssr" rel="apple-touch-icon" sizes="57x57" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-57x57.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="114x114" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-114x114.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="72x72" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-72x72.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="144x144" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-144x144.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="60x60" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-60x60.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="120x120" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-120x120.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="76x76" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-76x76.png"><link data-n-head="ssr" rel="apple-touch-icon" sizes="152x152" href="https://asset.jmir.pub/assets/static/images/apple-touch-icon-152x152.png"><link data-n-head="ssr" rel="icon" type="image/png" href="https://asset.jmir.pub/assets/static/images/favicon-196x196.png" sizes="196x196"><link data-n-head="ssr" rel="icon" type="image/png" href="https://asset.jmir.pub/assets/static/images/favicon-160x160.png" sizes="160x160"><link data-n-head="ssr" rel="icon" type="image/png" href="https://asset.jmir.pub/assets/static/images/favicon-96x96.png" sizes="96x96"><link data-n-head="ssr" rel="icon" type="image/png" href="https://asset.jmir.pub/assets/static/images/favicon-16x16.png" sizes="16x16"><link data-n-head="ssr" rel="icon" type="image/png" href="https://asset.jmir.pub/assets/static/images/favicon-32x32.png" sizes="32x32"><link data-n-head="ssr" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/css/bootstrap-grid.min.css" defer><link data-n-head="ssr" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" defer><link data-n-head="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&amp;display=swap" defer><link data-n-head="ssr" rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><link data-n-head="ssr" rel="canonical" href="/2025/1/e70610"><script data-n-head="ssr" data-hid="gtm-script">if(!window._gtm_init){window._gtm_init=1;(function(w,n,d,m,e,p){w[d]=(w[d]==1||n[d]=='yes'||n[d]==1||n[m]==1||(w[e]&&w[e][p]&&w[e][p]()))?1:0})(window,navigator,'doNotTrack','msDoNotTrack','external','msTrackingProtectionEnabled');(function(w,d,s,l,x,y){w[x]={};w._gtm_inject=function(i){w[x][i]=1;w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s);j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i;f.parentNode.insertBefore(j,f);};w[y]('GTM-58BHBF4V')})(window,document,'script','dataLayer','_gtm_ids','_gtm_inject')}</script><script data-n-head="ssr" type="text/javascript" id="hs-script-loader" src="//js.hs-scripts.com/19668141.js"></script><script data-n-head="ssr" data-hid="twitter-script" type="text/javascript" charset="utf-8">
                !function(e,t,n,s,u,a){e.twq||(s=e.twq=function(){s.exe?s.exe.apply(s,arguments):s.queue.push(arguments);},s.version='1.1',s.queue=[],u=t.createElement(n),u.async=!0,u.src='https://static.ads-twitter.com/uwt.js',a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(u,a))}(window,document,'script');twq('config','o7i83');
                </script><script data-n-head="ssr" src="https://js.trendmd.com/trendmd-ns.min.js" defer data-trendmdconfig="{"element": "#trendmd-suggestions"}"></script><link rel="preload" href="/_nuxt/9c7c1e4.js" as="script"><link rel="preload" href="/_nuxt/797995a.js" as="script"><link rel="preload" href="/_nuxt/d965382.js" as="script"><link rel="preload" href="/_nuxt/4927325.js" as="script"><link rel="preload" href="/_nuxt/2bde5b8.js" as="script"><link rel="preload" href="/_nuxt/1326996.js" as="script"><link rel="preload" href="/_nuxt/09b1d65.js" as="script"><link rel="preload" href="/_nuxt/366ad09.js" as="script"><link rel="preload" href="/_nuxt/95fde3b.js" as="script"><link rel="preload" href="/_nuxt/d0c41c5.js" as="script"><link rel="preload" href="/_nuxt/e6dad64.js" as="script"><link rel="preload" href="/_nuxt/84066b5.js" as="script"><link rel="preload" href="/_nuxt/fc6cf93.js" as="script"><link rel="preload" href="/_nuxt/14f78e7.js" as="script"><link rel="preload" href="/_nuxt/aa08135.js" as="script"><link rel="preload" href="/_nuxt/75832c0.js" as="script"><link rel="preload" href="/_nuxt/4436098.js" as="script"><link rel="preload" href="/_nuxt/bcc9d63.js" as="script"><link rel="preload" href="/_nuxt/3d6a000.js" as="script"><link rel="preload" href="/_nuxt/98a1aa4.js" as="script"><link rel="preload" href="/_nuxt/89ba270.js" as="script"><link rel="preload" href="/_nuxt/bc9dad6.js" as="script"><link rel="preload" href="/_nuxt/fc087e0.js" as="script"><link rel="preload" href="/_nuxt/b643d0c.js" as="script"><link rel="preload" href="/_nuxt/68827e8.js" as="script"><link rel="preload" href="/_nuxt/421b4ad.js" as="script"><link rel="preload" href="/_nuxt/bbb2307.js" as="script"><link rel="preload" href="/_nuxt/d9f2686.js" as="script"><link rel="preload" href="/_nuxt/255fd1e.js" as="script"><link rel="preload" href="/_nuxt/0bceff3.js" as="script"><link rel="preload" href="/_nuxt/7b27d0b.js" as="script"><link rel="preload" href="/_nuxt/e8be31d.js" as="script"><link rel="preload" href="/_nuxt/70e9da6.js" as="script"><link rel="preload" href="/_nuxt/beec627.js" as="script"><link rel="preload" href="/_nuxt/f98275c.js" as="script"><link rel="preload" href="/_nuxt/4c795dc.js" as="script"><link rel="preload" href="/_nuxt/08b91c7.js" as="script"><link rel="preload" href="/_nuxt/3df9693.js" as="script"><link rel="preload" href="/_nuxt/9a1b312.js" as="script"><link rel="preload" href="/_nuxt/21899a6.js" as="script"><link rel="preload" href="/_nuxt/bf91a67.js" as="script"><link rel="preload" href="/_nuxt/e7d56e2.js" as="script"><link rel="preload" href="/_nuxt/9c711d7.js" as="script"><link rel="preload" href="/_nuxt/a116589.js" as="script"><link rel="preload" href="/_nuxt/e8da5fa.js" as="script"><link rel="preload" href="/_nuxt/b70fdb7.js" as="script"><link rel="preload" href="/_nuxt/4916c14.js" as="script"><link rel="preload" href="/_nuxt/8043cc4.js" as="script"><style data-vue-ssr-id="7975b328:0 b8d2539e:0 f8852dd4:0 5afb56e8:0 19d22970:0 e58746f2:0 6f03c485:0 483225a2:0 0c0ff74d:0 27636745:0 1f7e98f9:0 d8137ec4:0">.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}input,select,textarea{border-radius:3px!important;font-size:1.6rem!important}textarea{font-family:Roboto-Regular,Roboto}input[type=date],input[type=email],input[type=hidden],input[type=number],input[type=search],input[type=tel],input[type=text],input[type=url]{border:1px solid rgba(26,37,76,.396);height:3.2rem;padding:0 10px}input[type=date]:focus,input[type=email]:focus,input[type=hidden]:focus,input[type=number]:focus,input[type=search]:focus,input[type=tel]:focus,input[type=text]:focus,input[type=url]:focus{background-color:rgba(48,136,223,.071);border:1px solid #1e70c2;outline:none}input::-moz-placeholder{opacity:.4}input::placeholder{opacity:.4}.v-select .vs__selected{margin:2px}.v-select .vs__dropdown-toggle{border:1px solid rgba(26,37,76,.396)!important;padding:0!important}.v-select input{border:1px solid transparent!important;margin:0}.v-select ul.vs__dropdown-menu{paddinng-top:0!important}.v-select.vs--single .vs__selected{margin:0 3px 0 5px}textarea{border:1px solid rgba(26,37,76,.396);padding:5px 10px;width:100%}textarea:focus{background-color:rgba(48,136,223,.071);border:1px solid #1e70c2;outline:none}select{border:1px solid rgba(26,37,76,.396);cursor:pointer;height:3.2rem}select:focus{background-color:rgba(48,136,223,.071);border:1px solid #1e70c2;outline:none}input::-webkit-input-placeholder,textarea::-webkit-input-placeholder{font-size:1.2rem}select::-webkit-input-placeholder{font-size:1.2rem}::-moz-selection{background-color:#1e70c2;color:#fff}::selection{background-color:#1e70c2;color:#fff}a{color:#1e70c2;outline:none;-webkit-text-decoration:none;text-decoration:none}a:focus,a:hover{-webkit-text-decoration:underline;text-decoration:underline}a:focus{font-weight:700}button:focus{outline:none}.deactive{cursor:not-allowed;pointer-events:none}.element-wrapper{margin-bottom:7rem}.page-heading{background-color:#f1f3f5;border-bottom:1px solid hsla(0,0%,44%,.161);margin-bottom:60px;padding:30px 0}.page-heading h1{margin:0}.link{color:#2078cf;outline:none;-webkit-text-decoration:none;text-decoration:none}.link:focus,.link:hover{color:#2078cf;-webkit-text-decoration:underline;text-decoration:underline}.title-link{color:#1a254c;outline:none;-webkit-text-decoration:none;text-decoration:none}.title-link:focus,.title-link:hover{color:#2078cf;-webkit-text-decoration:underline;text-decoration:underline}.h1,.h2,.h3,.h4,.h5,.h6,h2,h3,h4,h5,h6,pwdh1{font-weight:700}.h1,h1{font-size:4rem;line-height:5rem}.h2,h2{font-size:3.2rem;line-height:4rem}.h3,h3{font-size:2.6rem;line-height:3.4rem}.h4,h4{font-size:1.8rem;line-height:2.4rem}.h5,h5{font-size:1.6rem;line-height:2.2rem}.h6,h6{font-size:1.4rem}.h6,h6,p{line-height:2rem}small{line-height:1.8rem}.h1{font-size:4rem!important;line-height:5rem!important}.h2{font-size:3.2rem!important;line-height:4rem!important}.h3{font-size:2.6rem!important;line-height:3.4rem!important}.h4{font-size:1.8rem!important;line-height:2.4rem!important}.h5{font-size:1.6rem!important;line-height:2.2rem!important}.h6{font-size:1.4rem!important;line-height:2rem!important}input.disabled,select.disabled,textarea.disabled{background:hsla(0,0%,82%,.29);cursor:not-allowed}button.disabled{cursor:not-allowed;opacity:.5;pointer-events:none}strong{font-weight:700!important}.disabled-section{cursor:not-allowed;opacity:.5}.fa,.fas{font-weight:900}.errors{color:red;display:block}.screen-readers-only{height:1px;left:-10000px;overflow:hidden;position:absolute;top:auto;width:1px}input[type=text].input-error{border:1px solid red!important;border-radius:3px}.input-error{border:1px solid red!important;border-radius:3px}.popper{max-width:400px;padding:10px;text-align:justify}.vue-notification{margin:20px 20px 0 0}.vue-notification.toast-success{background:#4caf50;border-left:5px solid #1a254c}.vue-dropzone{border:2px dashed #e5e5e5}.vue-dropzone .icon{display:block;font-size:25px;margin-bottom:10px}.required:before{color:red;content:"*"}.grey-heading-underline{border-bottom:2px solid #c8cad4}.green-heading-underline{border-bottom:2px solid #367c3a}.green-underline{background:#367c3a;content:"";height:3px;margin:0 auto 40px;width:100px}.separator{color:#000;margin:0 10px;opacity:.4}.list-style-none{list-style-type:none}.list-style-none li{margin-bottom:10px}.width-100{width:100%!important}.width-fit-content{width:-moz-fit-content;width:fit-content}.break-word{word-break:break-word}.text-center{text-align:center}.d-inline-block{display:inline-block}.d-flex{display:flex}.d-block{display:block}.flex-direction-column{flex-direction:column}.justify-content-space-between{justify-content:space-between}.align-items-center{align-items:center}.align-items-baseline{align-items:baseline}.fs-10{font-size:1rem;line-height:1.6rem}.fs-12{font-size:1.2rem;line-height:1.8rem}.fs-14{font-size:1.4rem;line-height:2rem}.fs-16{font-size:1.6rem;line-height:2.4rem}.fs-18{font-size:1.8rem;line-height:2.6rem}.fs-20{font-size:2rem;line-height:3rem}.fs-italic{font-style:italic}.fw-bold{font-weight:700}.color-blue{color:#1e70c2}.color-green{color:#367c3a}.color-red{color:#b30000}.ql-toolbar{background-color:#f8f9fa}.btn{cursor:pointer;opacity:1;text-align:center;transition:.3s}.btn:focus,.btn:hover{font-weight:400;opacity:.9;-webkit-text-decoration:none;text-decoration:none}.btn:focus{outline:2px solid #f69038!important;outline-offset:6px!important}.btn-disabled{cursor:not-allowed;opacity:.6;pointer-events:none}.btn-small{font-size:1.2rem;padding:5px 10px}.btn-medium{font-size:1.4rem;padding:10px 20px}.btn-large{font-size:1.8rem;padding:20px 40px}.btn-blue{background-color:#1e70c2;border:1px solid #1e70c2;color:#fff}.btn-blue:active{background-color:#2b7bca;border:1px solid #2b7bca}.btn-green{background-color:#367c3a;border:1px solid #367c3a;color:#fff}.btn-green:active{background-color:#3b9d3f;border:1px solid #3b9d3f}.btn-grey{background-color:#f1f3f5;border:1px solid #dcdee0;color:#1a254c}.btn-grey:active{background-color:#dcdee0}.btn-red{background-color:#b30000;border:1px solid #b30000;color:#fff}.btn-red:active{background-color:#ba302d;border:1px solid #ba302d}.btn-blue-pill{background-color:inherit;border:none;border-radius:20px;color:#1e70c2;cursor:pointer;font-size:1.4rem;padding:5px 10px;transition:.3s}.btn-blue-pill:focus,.btn-blue-pill:hover{background-color:rgba(48,136,223,.161);-webkit-text-decoration:none;text-decoration:none}.btn-blue-pill:active{background-color:rgba(48,136,223,.29)}.btn-green-pill{background-color:inherit;border:none;border-radius:20px;color:#367c3a;cursor:pointer;font-size:1.4rem;padding:5px 10px;transition:.3s}.btn-green-pill:focus,.btn-green-pill:hover{background-color:rgba(76,175,80,.188);-webkit-text-decoration:none;text-decoration:none}.btn-green-pill:active{background-color:rgba(76,175,80,.29)}.btn-grey-pill{background-color:inherit;border:none;border-radius:20px;color:#1a254c;cursor:pointer;font-size:1.4rem;padding:5px 10px;transition:.3s}.btn-grey-pill:focus,.btn-grey-pill:hover{background-color:#dcdee0;-webkit-text-decoration:none;text-decoration:none}.btn-grey-pill:active{background-color:#cccdce}.btn-red-pill{background-color:inherit;border:none;border-radius:20px;color:#fa2a24;cursor:pointer;font-size:1.4rem;padding:5px 10px;transition:.3s}.btn-red-pill:focus,.btn-red-pill:hover{background-color:rgba(255,0,0,.122);-webkit-text-decoration:none;text-decoration:none}.btn-red-pill:active{background-color:rgba(255,0,0,.22)}.sm-icons a{border-radius:50%;color:#fff;cursor:pointer;display:inline-block;font-size:14px;height:25px;line-height:25px;padding:0;text-align:center;-webkit-text-decoration:none;text-decoration:none;transition:.3s;width:25px}.sm-icons a:hover{opacity:.9}.sm-icons a:focus{opacity:.9;outline:none;transform:scale(1.2)}.sm-icons .bluesky{background-color:#0085ff}.sm-icons .bluesky:before{content:"";font-family:"Font Awesome 6 Brands"}.sm-icons .twitter{background-color:#000}.sm-icons .twitter:before{content:"";font-family:"Font Awesome 5 Brands";font-weight:900}.sm-icons .facebook{background-color:#3b5a98}.sm-icons .facebook:before{content:"";font-family:"Font Awesome 5 Brands";font-weight:900}.sm-icons .linkedin{background-color:#0077b5}.sm-icons .linkedin:before{content:"";font-family:"Font Awesome 5 Brands";font-weight:900}.sm-icons .youtube{background-color:red}.sm-icons .youtube:before{content:"";font-family:"Font Awesome 5 Brands";font-weight:900}.sm-icons .instagram{background:radial-gradient(circle at 30% 107%,#fdf497 0,#fdf497 5%,#fd5949 45%,#d6249f 60%,#285aeb 90%)}.sm-icons .instagram:before{content:"";font-family:"Font Awesome 5 Brands";font-weight:900}.sm-icons .email{background-color:#cc2126}.sm-icons .email:before{content:"";font-family:"Font Awesome 5 Free";font-weight:900}.sm-icons .rss{background-color:#ee802f}.sm-icons .rss:before{content:"";font-family:"Font Awesome 5 Free";font-weight:900}.full-width-card-wrapper .full-width-card{border:1px solid #ced1dc;display:flex;margin:10px 0 20px}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card{flex-wrap:wrap;justify-content:space-between}}.full-width-card-wrapper .full-width-card-img{height:auto;position:relative;width:250px}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card-img{flex-basis:50%}}@media screen and (max-width:47.9375em){.full-width-card-wrapper .full-width-card-img{width:100%}}.full-width-card-wrapper .full-width-card-img img{border:1px solid #ced1dc;height:auto;width:100%}.full-width-card-wrapper .full-width-card-img-info{background-color:#1e70c2;border-radius:3px 0 0 0;bottom:4px;cursor:pointer;outline:none;padding:11px;position:absolute;right:1px}.full-width-card-wrapper .full-width-card-img-info .icon{color:#fff;font-size:1.8rem;transition:all .3s ease}.full-width-card-wrapper .full-width-card-img-info:hover{padding-bottom:10px}.full-width-card-wrapper .full-width-card-img-info:hover .icon{font-size:2rem}.full-width-card-wrapper .full-width-card-img-info:focus .icon{font-size:2.4rem}.full-width-card-wrapper .full-width-card-info{flex-grow:1;padding:15px 15px 15px 0}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card-info{flex-basis:50%}}@media screen and (max-width:47.9375em){.full-width-card-wrapper .full-width-card-info{flex-basis:100%;padding:15px}}.full-width-card-wrapper .full-width-card-info-title{margin-top:0}.full-width-card-wrapper .full-width-card-highlight:after,.full-width-card-wrapper .full-width-card-highlight:before{color:#5d6581;content:"..."}.full-width-card-wrapper .full-width-card-info-download-links a{margin-right:9px}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card-info-button button{margin:10px 0 0;width:100%}}.full-width-card-wrapper .full-width-card-info-group-buttons{display:flex;margin-top:14px}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card-info-group-buttons a:first-child{display:block;margin-bottom:14px;margin-left:0!important;margin-right:0!important}.full-width-card-wrapper .full-width-card-info-group-buttons a:last-child{display:block;margin-left:0!important;margin-right:0!important}.full-width-card-wrapper .full-width-card-info-group-buttons{display:block}}.full-width-card-wrapper .full-width-card-info-group-buttons button{margin-right:10px}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card-info-group-buttons button{margin:10px 0 0;width:100%}}.full-width-card-wrapper .full-width-card-info-date-published{margin-bottom:20px}.full-width-card-wrapper .full-width-card-altmetric{align-self:center;margin-left:auto;padding-right:15px}@media screen and (max-width:61.9375em){.full-width-card-wrapper .full-width-card-altmetric{margin:10px auto 15px;padding:0}}.full-width-card-wrapper .full-width-card-altmetric img{width:auto}.img-cont{padding:15px}@media screen and (max-width:47.9375em){.img-cont{width:100%}}.cards{justify-content:space-between;margin-bottom:14px}.cards,.cards .card{display:flex;flex-wrap:wrap}.cards .card{flex-basis:31%;flex-direction:column;flex-grow:0;flex-shrink:0;transition:.3s}@media screen and (max-width:61.9375em){.cards .card{flex-basis:48%}}@media screen and (max-width:47.9375em){.cards .card{flex-basis:100%}}.cards .card:hover .card-header .card-img img{filter:brightness(1);transform:scale(1.1);transition:all .3s ease}.cards .card-header,.cards .card-img{border-radius:10px;border-radius:3px;overflow:hidden}.cards .card-img{height:200px;position:relative}.cards .card-img img{filter:brightness(.6);height:auto;transition:all .3s ease;width:100%}@media screen and (max-width:61.9375em){.cards .card-img{height:180px}}@media screen and (max-width:47.9375em){.cards .card-img{height:200px}}.cards .card-img-info{background-color:#1e70c2;border-radius:3px 0 0 0;cursor:pointer;outline:none;padding:11px;position:absolute;right:0;top:160px}@media screen and (max-width:61.9375em){.cards .card-img-info{top:140px}}@media screen and (max-width:47.9375em){.cards .card-img-info{top:160px}}.cards .card-img-info .icon{color:#fff;font-size:1.8rem;transition:all .3s ease}.cards .card-img-info:hover .icon{font-size:2rem}.cards .card-img-info:focus .icon{font-size:2.4rem}.cards .card-body{flex-grow:1;flex-shrink:0}.cards .card-body,.cards .card-title{display:flex;flex-direction:column}.cards .card-title a{color:#1a254c;outline:none;transition:all .3s ease}.cards .card-title a:focus,.cards .card-title a:hover{color:#1e70c2;-webkit-text-decoration:underline;text-decoration:underline;transition:all .3s ease}.cards .card-info p{max-height:125px;overflow:scroll}.cards .card-info p a{color:#1e70c2;cursor:pointer}.cards .card-info p a:hover{-webkit-text-decoration:underline;text-decoration:underline}.cards .card-years{display:flex;flex-wrap:wrap}.cards .card-years a{color:#1e70c2;cursor:pointer;margin:0 10px 5px 0}.cards .card-years a:focus,.cards .card-years a:hover{outline:none;-webkit-text-decoration:underline;text-decoration:underline}.cards .card-date-social{color:#5d6581;display:flex;flex-wrap:wrap}.v--modal-overlay{background:rgba(17,26,55,.7);box-sizing:border-box;height:100vh;left:0;opacity:1;position:fixed;top:0;width:100%;z-index:99999}.v--modal-overlay .v--modal-background-click{min-height:100%;padding-bottom:10px;width:100%}.v--modal-overlay .v--modal-background-click .v--modal-top-right{display:block;position:absolute;right:0;top:0}.v--modal-overlay .v--modal-background-click .v--modal-box{box-sizing:border-box;position:relative}.v--modal-overlay .v--modal-background-click .v--modal{border-radius:3px;box-shadow:0 20px 60px -2px rgba(27,33,58,.4);text-align:left}.modal-window{background-color:#fff;height:100%;overflow:hidden;position:relative}.modal-window-header{background:#f3f3f5;border-bottom:1px solid #bab4b4;display:flex;flex-wrap:wrap;justify-content:space-between;padding:10px 20px}.modal-window-title{margin:0}.modal-window-close{background-color:#f3f3f5;border:none;color:gray;cursor:pointer;font-size:2rem;transition:.3s}.modal-window-close:hover{color:#000}.modal-window-body{height:-webkit-fill-available;max-height:400px;overflow-y:auto;padding:20px}.modal-window-footer{background-color:#f8f9fa;border-top:1px solid #bab4b4}.modal-window-footer div{float:right;padding:10px 20px}.ads-bottom-banner,.ads-top-banner{display:flex;justify-content:center;margin:8px 0;width:100%}.ads-sidebar-container{margin:20px 0}.ads-sidebar-item{margin-bottom:20px;width:100%}.ads-sidebar-item:last-child{margin-bottom:0}
.vue-modal-resizer{bottom:0;cursor:se-resize;height:12px;overflow:hidden;right:0;width:12px;z-index:9999999}.vue-modal-resizer,.vue-modal-resizer:after{background:transparent;display:block;position:absolute}.vue-modal-resizer:after{border-bottom:10px solid #ddd;border-left:10px solid transparent;content:"";height:0;left:0;top:0;width:0}.vue-modal-resizer.clicked:after{border-bottom:10px solid #369be9}.v--modal-block-scroll{overflow:hidden;width:100vw}.v--modal-overlay{background:rgba(0,0,0,.2);box-sizing:border-box;height:100vh;left:0;opacity:1;position:fixed;top:0;width:100%;z-index:999}.v--modal-overlay.scrollable{-webkit-overflow-scrolling:touch;height:100%;min-height:100vh;overflow-y:auto}.v--modal-overlay .v--modal-background-click{height:auto;min-height:100%;width:100%}.v--modal-overlay .v--modal-box{box-sizing:border-box;overflow:hidden;position:relative}.v--modal-overlay.scrollable .v--modal-box{margin-bottom:2px}.v--modal{background-color:#fff;border-radius:3px;box-shadow:0 20px 60px -2px rgba(27,33,58,.4);padding:0;text-align:left}.v--modal.v--modal-fullscreen{height:100vh;left:0;margin:0;top:0;width:100vw}.v--modal-top-right{display:block;position:absolute;right:0;top:0}.overlay-fade-enter-active,.overlay-fade-leave-active{transition:all .2s}.overlay-fade-enter,.overlay-fade-leave-active{opacity:0}.nice-modal-fade-enter-active,.nice-modal-fade-leave-active{transition:all .4s}.nice-modal-fade-enter,.nice-modal-fade-leave-active{opacity:0;-webkit-transform:translateY(-20px);transform:translateY(-20px)}.vue-dialog div{box-sizing:border-box}.vue-dialog .dialog-flex{height:100%;width:100%}.vue-dialog .dialog-content{flex:1 0 auto;font-size:14px;padding:15px;width:100%}.vue-dialog .dialog-c-title{font-weight:600;padding-bottom:15px}.vue-dialog .vue-dialog-buttons{border-top:1px solid #eee;display:flex;flex:0 1 auto;width:100%}.vue-dialog .vue-dialog-buttons-none{padding-bottom:15px;width:100%}.vue-dialog-button{background:transparent;border:0;box-sizing:border-box;color:inherit;cursor:pointer;font-size:12px!important;height:40px;line-height:40px;font:inherit;margin:0;outline:none;padding:0}.vue-dialog-button:hover{background:rgba(0,0,0,.01)}.vue-dialog-button:active{background:rgba(0,0,0,.025)}.vue-dialog-button:not(:first-of-type){border-left:1px solid #eee}
.nuxt-progress{background-color:#3088df;height:2px;left:0;opacity:1;position:fixed;right:0;top:0;transition:width .1s,opacity .4s;width:0;z-index:999999}.nuxt-progress.nuxt-progress-notransition{transition:none}.nuxt-progress-failed{background-color:red}
.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}*,:after,:before{box-sizing:inherit}html{font-size:62.5%}body{background-color:#fff;box-sizing:border-box;color:#1a254c;font-family:"Roboto",sans-serif;font-size:1.4rem;font-weight:400;margin:0;padding:0}#jmir-html{position:relative}#skip-link a{font-size:1.6rem;font-weight:700;height:1px;left:-10000px;margin:10px 0 10px 10px;overflow:hidden;padding:10px;position:absolute;-webkit-text-decoration:underline;text-decoration:underline;top:auto;width:1px}#skip-link a:focus{border:2px solid #f69038;display:inline-block;height:auto;position:static;width:auto}#main-layout-container{display:flex;flex-direction:column;height:100vh;justify-content:space-between}.toasted-container.top-right{right:2%!important;top:2%!important}._hj-3ZiaL__MinimizedWidgetBottom__container{bottom:.5%!important;flex-direction:row!important;justify-content:flex-end;width:84%!important}span a.a-select-membership{font-size:1.2rem;padding:5px 10px;-webkit-text-decoration:none!important;text-decoration:none!important}
.mt-0[data-v-6301a668]{margin-top:0!important}.mt-5[data-v-6301a668]{margin-top:5px!important}.mt-10[data-v-6301a668]{margin-top:10px!important}.mt-15[data-v-6301a668]{margin-top:15px!important}.mt-20[data-v-6301a668]{margin-top:20px!important}.mt-25[data-v-6301a668]{margin-top:25px!important}.mt-30[data-v-6301a668]{margin-top:30px!important}.mt-35[data-v-6301a668]{margin-top:35px!important}.mt-40[data-v-6301a668]{margin-top:40px!important}.mt-45[data-v-6301a668]{margin-top:45px!important}.mt-50[data-v-6301a668]{margin-top:50px!important}.mt-55[data-v-6301a668]{margin-top:55px!important}.mt-60[data-v-6301a668]{margin-top:60px!important}.mt-65[data-v-6301a668]{margin-top:65px!important}.mt-70[data-v-6301a668]{margin-top:70px!important}.mt-75[data-v-6301a668]{margin-top:75px!important}.mt-80[data-v-6301a668]{margin-top:80px!important}.mt-85[data-v-6301a668]{margin-top:85px!important}.mt-90[data-v-6301a668]{margin-top:90px!important}.mt-95[data-v-6301a668]{margin-top:95px!important}.mt-100[data-v-6301a668]{margin-top:100px!important}.mb-0[data-v-6301a668]{margin-bottom:0!important}.mb-5[data-v-6301a668]{margin-bottom:5px!important}.mb-10[data-v-6301a668]{margin-bottom:10px!important}.mb-15[data-v-6301a668]{margin-bottom:15px!important}.mb-20[data-v-6301a668]{margin-bottom:20px!important}.mb-25[data-v-6301a668]{margin-bottom:25px!important}.mb-30[data-v-6301a668]{margin-bottom:30px!important}.mb-35[data-v-6301a668]{margin-bottom:35px!important}.mb-40[data-v-6301a668]{margin-bottom:40px!important}.mb-45[data-v-6301a668]{margin-bottom:45px!important}.mb-50[data-v-6301a668]{margin-bottom:50px!important}.mb-55[data-v-6301a668]{margin-bottom:55px!important}.mb-60[data-v-6301a668]{margin-bottom:60px!important}.mb-65[data-v-6301a668]{margin-bottom:65px!important}.mb-70[data-v-6301a668]{margin-bottom:70px!important}.mb-75[data-v-6301a668]{margin-bottom:75px!important}.mb-80[data-v-6301a668]{margin-bottom:80px!important}.mb-85[data-v-6301a668]{margin-bottom:85px!important}.mb-90[data-v-6301a668]{margin-bottom:90px!important}.mb-95[data-v-6301a668]{margin-bottom:95px!important}.mb-100[data-v-6301a668]{margin-bottom:100px!important}.ml-0[data-v-6301a668]{margin-left:0!important}.ml-5[data-v-6301a668]{margin-left:5px!important}.ml-10[data-v-6301a668]{margin-left:10px!important}.ml-15[data-v-6301a668]{margin-left:15px!important}.ml-20[data-v-6301a668]{margin-left:20px!important}.ml-25[data-v-6301a668]{margin-left:25px!important}.ml-30[data-v-6301a668]{margin-left:30px!important}.ml-35[data-v-6301a668]{margin-left:35px!important}.ml-40[data-v-6301a668]{margin-left:40px!important}.ml-45[data-v-6301a668]{margin-left:45px!important}.ml-50[data-v-6301a668]{margin-left:50px!important}.ml-55[data-v-6301a668]{margin-left:55px!important}.ml-60[data-v-6301a668]{margin-left:60px!important}.ml-65[data-v-6301a668]{margin-left:65px!important}.ml-70[data-v-6301a668]{margin-left:70px!important}.ml-75[data-v-6301a668]{margin-left:75px!important}.ml-80[data-v-6301a668]{margin-left:80px!important}.ml-85[data-v-6301a668]{margin-left:85px!important}.ml-90[data-v-6301a668]{margin-left:90px!important}.ml-95[data-v-6301a668]{margin-left:95px!important}.ml-100[data-v-6301a668]{margin-left:100px!important}.mr-0[data-v-6301a668]{margin-right:0!important}.mr-5[data-v-6301a668]{margin-right:5px!important}.mr-10[data-v-6301a668]{margin-right:10px!important}.mr-15[data-v-6301a668]{margin-right:15px!important}.mr-20[data-v-6301a668]{margin-right:20px!important}.mr-25[data-v-6301a668]{margin-right:25px!important}.mr-30[data-v-6301a668]{margin-right:30px!important}.mr-35[data-v-6301a668]{margin-right:35px!important}.mr-40[data-v-6301a668]{margin-right:40px!important}.mr-45[data-v-6301a668]{margin-right:45px!important}.mr-50[data-v-6301a668]{margin-right:50px!important}.mr-55[data-v-6301a668]{margin-right:55px!important}.mr-60[data-v-6301a668]{margin-right:60px!important}.mr-65[data-v-6301a668]{margin-right:65px!important}.mr-70[data-v-6301a668]{margin-right:70px!important}.mr-75[data-v-6301a668]{margin-right:75px!important}.mr-80[data-v-6301a668]{margin-right:80px!important}.mr-85[data-v-6301a668]{margin-right:85px!important}.mr-90[data-v-6301a668]{margin-right:90px!important}.mr-95[data-v-6301a668]{margin-right:95px!important}.mr-100[data-v-6301a668]{margin-right:100px!important}.pt-0[data-v-6301a668]{padding-top:0!important}.pt-5[data-v-6301a668]{padding-top:5px!important}.pt-10[data-v-6301a668]{padding-top:10px!important}.pt-15[data-v-6301a668]{padding-top:15px!important}.pt-20[data-v-6301a668]{padding-top:20px!important}.pt-25[data-v-6301a668]{padding-top:25px!important}.pt-30[data-v-6301a668]{padding-top:30px!important}.pt-35[data-v-6301a668]{padding-top:35px!important}.pt-40[data-v-6301a668]{padding-top:40px!important}.pt-45[data-v-6301a668]{padding-top:45px!important}.pt-50[data-v-6301a668]{padding-top:50px!important}.pt-55[data-v-6301a668]{padding-top:55px!important}.pt-60[data-v-6301a668]{padding-top:60px!important}.pt-65[data-v-6301a668]{padding-top:65px!important}.pt-70[data-v-6301a668]{padding-top:70px!important}.pt-75[data-v-6301a668]{padding-top:75px!important}.pt-80[data-v-6301a668]{padding-top:80px!important}.pt-85[data-v-6301a668]{padding-top:85px!important}.pt-90[data-v-6301a668]{padding-top:90px!important}.pt-95[data-v-6301a668]{padding-top:95px!important}.pt-100[data-v-6301a668]{padding-top:100px!important}.pb-0[data-v-6301a668]{padding-bottom:0!important}.pb-5[data-v-6301a668]{padding-bottom:5px!important}.pb-10[data-v-6301a668]{padding-bottom:10px!important}.pb-15[data-v-6301a668]{padding-bottom:15px!important}.pb-20[data-v-6301a668]{padding-bottom:20px!important}.pb-25[data-v-6301a668]{padding-bottom:25px!important}.pb-30[data-v-6301a668]{padding-bottom:30px!important}.pb-35[data-v-6301a668]{padding-bottom:35px!important}.pb-40[data-v-6301a668]{padding-bottom:40px!important}.pb-45[data-v-6301a668]{padding-bottom:45px!important}.pb-50[data-v-6301a668]{padding-bottom:50px!important}.pb-55[data-v-6301a668]{padding-bottom:55px!important}.pb-60[data-v-6301a668]{padding-bottom:60px!important}.pb-65[data-v-6301a668]{padding-bottom:65px!important}.pb-70[data-v-6301a668]{padding-bottom:70px!important}.pb-75[data-v-6301a668]{padding-bottom:75px!important}.pb-80[data-v-6301a668]{padding-bottom:80px!important}.pb-85[data-v-6301a668]{padding-bottom:85px!important}.pb-90[data-v-6301a668]{padding-bottom:90px!important}.pb-95[data-v-6301a668]{padding-bottom:95px!important}.pb-100[data-v-6301a668]{padding-bottom:100px!important}.pl-0[data-v-6301a668]{padding-left:0!important}.pl-5[data-v-6301a668]{padding-left:5px!important}.pl-10[data-v-6301a668]{padding-left:10px!important}.pl-15[data-v-6301a668]{padding-left:15px!important}.pl-20[data-v-6301a668]{padding-left:20px!important}.pl-25[data-v-6301a668]{padding-left:25px!important}.pl-30[data-v-6301a668]{padding-left:30px!important}.pl-35[data-v-6301a668]{padding-left:35px!important}.pl-40[data-v-6301a668]{padding-left:40px!important}.pl-45[data-v-6301a668]{padding-left:45px!important}.pl-50[data-v-6301a668]{padding-left:50px!important}.pl-55[data-v-6301a668]{padding-left:55px!important}.pl-60[data-v-6301a668]{padding-left:60px!important}.pl-65[data-v-6301a668]{padding-left:65px!important}.pl-70[data-v-6301a668]{padding-left:70px!important}.pl-75[data-v-6301a668]{padding-left:75px!important}.pl-80[data-v-6301a668]{padding-left:80px!important}.pl-85[data-v-6301a668]{padding-left:85px!important}.pl-90[data-v-6301a668]{padding-left:90px!important}.pl-95[data-v-6301a668]{padding-left:95px!important}.pl-100[data-v-6301a668]{padding-left:100px!important}.pr-0[data-v-6301a668]{padding-right:0!important}.pr-5[data-v-6301a668]{padding-right:5px!important}.pr-10[data-v-6301a668]{padding-right:10px!important}.pr-15[data-v-6301a668]{padding-right:15px!important}.pr-20[data-v-6301a668]{padding-right:20px!important}.pr-25[data-v-6301a668]{padding-right:25px!important}.pr-30[data-v-6301a668]{padding-right:30px!important}.pr-35[data-v-6301a668]{padding-right:35px!important}.pr-40[data-v-6301a668]{padding-right:40px!important}.pr-45[data-v-6301a668]{padding-right:45px!important}.pr-50[data-v-6301a668]{padding-right:50px!important}.pr-55[data-v-6301a668]{padding-right:55px!important}.pr-60[data-v-6301a668]{padding-right:60px!important}.pr-65[data-v-6301a668]{padding-right:65px!important}.pr-70[data-v-6301a668]{padding-right:70px!important}.pr-75[data-v-6301a668]{padding-right:75px!important}.pr-80[data-v-6301a668]{padding-right:80px!important}.pr-85[data-v-6301a668]{padding-right:85px!important}.pr-90[data-v-6301a668]{padding-right:90px!important}.pr-95[data-v-6301a668]{padding-right:95px!important}.pr-100[data-v-6301a668]{padding-right:100px!important}.top-hero-banner[data-v-6301a668]{background-color:#e5f2fe;padding:0 10vw 0 5vw;position:relative}.top-hero-banner__divider[data-v-6301a668]{border-top:1px solid #1a254c}.top-hero-banner__close[data-v-6301a668]{color:#1a254c;cursor:pointer;font-size:2rem;position:absolute;right:10vw;transition:.3s}.top-hero-banner__close[data-v-6301a668]:hover{color:#2e4185}.top-hero-banner__info[data-v-6301a668]{display:flex}.top-hero-banner__info .icon[data-v-6301a668]{font-size:2rem;margin-top:3px}.top-hero-banner__actions[data-v-6301a668]{margin-left:2.9rem}.top-hero-banner__actions a[data-v-6301a668]{display:inline-block;margin-right:2rem}@media screen and (max-width:31.25em){.top-hero-banner__actions a[data-v-6301a668]{padding:5px 0}}.top-hero-banner__link[data-v-6301a668]{color:#1a254c;display:inline-block;font-size:1.6rem;font-weight:700;margin-bottom:5px;margin-right:4rem;-webkit-text-decoration:none;text-decoration:none}.top-hero-banner__link[data-v-6301a668]:focus,.top-hero-banner__link[data-v-6301a668]:hover{-webkit-text-decoration:underline;text-decoration:underline}@media screen and (max-width:47.9375em){.top-hero-banner__link[data-v-6301a668]{display:block;margin-right:0}}
.mt-0[data-v-49c694ee]{margin-top:0!important}.mt-5[data-v-49c694ee]{margin-top:5px!important}.mt-10[data-v-49c694ee]{margin-top:10px!important}.mt-15[data-v-49c694ee]{margin-top:15px!important}.mt-20[data-v-49c694ee]{margin-top:20px!important}.mt-25[data-v-49c694ee]{margin-top:25px!important}.mt-30[data-v-49c694ee]{margin-top:30px!important}.mt-35[data-v-49c694ee]{margin-top:35px!important}.mt-40[data-v-49c694ee]{margin-top:40px!important}.mt-45[data-v-49c694ee]{margin-top:45px!important}.mt-50[data-v-49c694ee]{margin-top:50px!important}.mt-55[data-v-49c694ee]{margin-top:55px!important}.mt-60[data-v-49c694ee]{margin-top:60px!important}.mt-65[data-v-49c694ee]{margin-top:65px!important}.mt-70[data-v-49c694ee]{margin-top:70px!important}.mt-75[data-v-49c694ee]{margin-top:75px!important}.mt-80[data-v-49c694ee]{margin-top:80px!important}.mt-85[data-v-49c694ee]{margin-top:85px!important}.mt-90[data-v-49c694ee]{margin-top:90px!important}.mt-95[data-v-49c694ee]{margin-top:95px!important}.mt-100[data-v-49c694ee]{margin-top:100px!important}.mb-0[data-v-49c694ee]{margin-bottom:0!important}.mb-5[data-v-49c694ee]{margin-bottom:5px!important}.mb-10[data-v-49c694ee]{margin-bottom:10px!important}.mb-15[data-v-49c694ee]{margin-bottom:15px!important}.mb-20[data-v-49c694ee]{margin-bottom:20px!important}.mb-25[data-v-49c694ee]{margin-bottom:25px!important}.mb-30[data-v-49c694ee]{margin-bottom:30px!important}.mb-35[data-v-49c694ee]{margin-bottom:35px!important}.mb-40[data-v-49c694ee]{margin-bottom:40px!important}.mb-45[data-v-49c694ee]{margin-bottom:45px!important}.mb-50[data-v-49c694ee]{margin-bottom:50px!important}.mb-55[data-v-49c694ee]{margin-bottom:55px!important}.mb-60[data-v-49c694ee]{margin-bottom:60px!important}.mb-65[data-v-49c694ee]{margin-bottom:65px!important}.mb-70[data-v-49c694ee]{margin-bottom:70px!important}.mb-75[data-v-49c694ee]{margin-bottom:75px!important}.mb-80[data-v-49c694ee]{margin-bottom:80px!important}.mb-85[data-v-49c694ee]{margin-bottom:85px!important}.mb-90[data-v-49c694ee]{margin-bottom:90px!important}.mb-95[data-v-49c694ee]{margin-bottom:95px!important}.mb-100[data-v-49c694ee]{margin-bottom:100px!important}.ml-0[data-v-49c694ee]{margin-left:0!important}.ml-5[data-v-49c694ee]{margin-left:5px!important}.ml-10[data-v-49c694ee]{margin-left:10px!important}.ml-15[data-v-49c694ee]{margin-left:15px!important}.ml-20[data-v-49c694ee]{margin-left:20px!important}.ml-25[data-v-49c694ee]{margin-left:25px!important}.ml-30[data-v-49c694ee]{margin-left:30px!important}.ml-35[data-v-49c694ee]{margin-left:35px!important}.ml-40[data-v-49c694ee]{margin-left:40px!important}.ml-45[data-v-49c694ee]{margin-left:45px!important}.ml-50[data-v-49c694ee]{margin-left:50px!important}.ml-55[data-v-49c694ee]{margin-left:55px!important}.ml-60[data-v-49c694ee]{margin-left:60px!important}.ml-65[data-v-49c694ee]{margin-left:65px!important}.ml-70[data-v-49c694ee]{margin-left:70px!important}.ml-75[data-v-49c694ee]{margin-left:75px!important}.ml-80[data-v-49c694ee]{margin-left:80px!important}.ml-85[data-v-49c694ee]{margin-left:85px!important}.ml-90[data-v-49c694ee]{margin-left:90px!important}.ml-95[data-v-49c694ee]{margin-left:95px!important}.ml-100[data-v-49c694ee]{margin-left:100px!important}.mr-0[data-v-49c694ee]{margin-right:0!important}.mr-5[data-v-49c694ee]{margin-right:5px!important}.mr-10[data-v-49c694ee]{margin-right:10px!important}.mr-15[data-v-49c694ee]{margin-right:15px!important}.mr-20[data-v-49c694ee]{margin-right:20px!important}.mr-25[data-v-49c694ee]{margin-right:25px!important}.mr-30[data-v-49c694ee]{margin-right:30px!important}.mr-35[data-v-49c694ee]{margin-right:35px!important}.mr-40[data-v-49c694ee]{margin-right:40px!important}.mr-45[data-v-49c694ee]{margin-right:45px!important}.mr-50[data-v-49c694ee]{margin-right:50px!important}.mr-55[data-v-49c694ee]{margin-right:55px!important}.mr-60[data-v-49c694ee]{margin-right:60px!important}.mr-65[data-v-49c694ee]{margin-right:65px!important}.mr-70[data-v-49c694ee]{margin-right:70px!important}.mr-75[data-v-49c694ee]{margin-right:75px!important}.mr-80[data-v-49c694ee]{margin-right:80px!important}.mr-85[data-v-49c694ee]{margin-right:85px!important}.mr-90[data-v-49c694ee]{margin-right:90px!important}.mr-95[data-v-49c694ee]{margin-right:95px!important}.mr-100[data-v-49c694ee]{margin-right:100px!important}.pt-0[data-v-49c694ee]{padding-top:0!important}.pt-5[data-v-49c694ee]{padding-top:5px!important}.pt-10[data-v-49c694ee]{padding-top:10px!important}.pt-15[data-v-49c694ee]{padding-top:15px!important}.pt-20[data-v-49c694ee]{padding-top:20px!important}.pt-25[data-v-49c694ee]{padding-top:25px!important}.pt-30[data-v-49c694ee]{padding-top:30px!important}.pt-35[data-v-49c694ee]{padding-top:35px!important}.pt-40[data-v-49c694ee]{padding-top:40px!important}.pt-45[data-v-49c694ee]{padding-top:45px!important}.pt-50[data-v-49c694ee]{padding-top:50px!important}.pt-55[data-v-49c694ee]{padding-top:55px!important}.pt-60[data-v-49c694ee]{padding-top:60px!important}.pt-65[data-v-49c694ee]{padding-top:65px!important}.pt-70[data-v-49c694ee]{padding-top:70px!important}.pt-75[data-v-49c694ee]{padding-top:75px!important}.pt-80[data-v-49c694ee]{padding-top:80px!important}.pt-85[data-v-49c694ee]{padding-top:85px!important}.pt-90[data-v-49c694ee]{padding-top:90px!important}.pt-95[data-v-49c694ee]{padding-top:95px!important}.pt-100[data-v-49c694ee]{padding-top:100px!important}.pb-0[data-v-49c694ee]{padding-bottom:0!important}.pb-5[data-v-49c694ee]{padding-bottom:5px!important}.pb-10[data-v-49c694ee]{padding-bottom:10px!important}.pb-15[data-v-49c694ee]{padding-bottom:15px!important}.pb-20[data-v-49c694ee]{padding-bottom:20px!important}.pb-25[data-v-49c694ee]{padding-bottom:25px!important}.pb-30[data-v-49c694ee]{padding-bottom:30px!important}.pb-35[data-v-49c694ee]{padding-bottom:35px!important}.pb-40[data-v-49c694ee]{padding-bottom:40px!important}.pb-45[data-v-49c694ee]{padding-bottom:45px!important}.pb-50[data-v-49c694ee]{padding-bottom:50px!important}.pb-55[data-v-49c694ee]{padding-bottom:55px!important}.pb-60[data-v-49c694ee]{padding-bottom:60px!important}.pb-65[data-v-49c694ee]{padding-bottom:65px!important}.pb-70[data-v-49c694ee]{padding-bottom:70px!important}.pb-75[data-v-49c694ee]{padding-bottom:75px!important}.pb-80[data-v-49c694ee]{padding-bottom:80px!important}.pb-85[data-v-49c694ee]{padding-bottom:85px!important}.pb-90[data-v-49c694ee]{padding-bottom:90px!important}.pb-95[data-v-49c694ee]{padding-bottom:95px!important}.pb-100[data-v-49c694ee]{padding-bottom:100px!important}.pl-0[data-v-49c694ee]{padding-left:0!important}.pl-5[data-v-49c694ee]{padding-left:5px!important}.pl-10[data-v-49c694ee]{padding-left:10px!important}.pl-15[data-v-49c694ee]{padding-left:15px!important}.pl-20[data-v-49c694ee]{padding-left:20px!important}.pl-25[data-v-49c694ee]{padding-left:25px!important}.pl-30[data-v-49c694ee]{padding-left:30px!important}.pl-35[data-v-49c694ee]{padding-left:35px!important}.pl-40[data-v-49c694ee]{padding-left:40px!important}.pl-45[data-v-49c694ee]{padding-left:45px!important}.pl-50[data-v-49c694ee]{padding-left:50px!important}.pl-55[data-v-49c694ee]{padding-left:55px!important}.pl-60[data-v-49c694ee]{padding-left:60px!important}.pl-65[data-v-49c694ee]{padding-left:65px!important}.pl-70[data-v-49c694ee]{padding-left:70px!important}.pl-75[data-v-49c694ee]{padding-left:75px!important}.pl-80[data-v-49c694ee]{padding-left:80px!important}.pl-85[data-v-49c694ee]{padding-left:85px!important}.pl-90[data-v-49c694ee]{padding-left:90px!important}.pl-95[data-v-49c694ee]{padding-left:95px!important}.pl-100[data-v-49c694ee]{padding-left:100px!important}.pr-0[data-v-49c694ee]{padding-right:0!important}.pr-5[data-v-49c694ee]{padding-right:5px!important}.pr-10[data-v-49c694ee]{padding-right:10px!important}.pr-15[data-v-49c694ee]{padding-right:15px!important}.pr-20[data-v-49c694ee]{padding-right:20px!important}.pr-25[data-v-49c694ee]{padding-right:25px!important}.pr-30[data-v-49c694ee]{padding-right:30px!important}.pr-35[data-v-49c694ee]{padding-right:35px!important}.pr-40[data-v-49c694ee]{padding-right:40px!important}.pr-45[data-v-49c694ee]{padding-right:45px!important}.pr-50[data-v-49c694ee]{padding-right:50px!important}.pr-55[data-v-49c694ee]{padding-right:55px!important}.pr-60[data-v-49c694ee]{padding-right:60px!important}.pr-65[data-v-49c694ee]{padding-right:65px!important}.pr-70[data-v-49c694ee]{padding-right:70px!important}.pr-75[data-v-49c694ee]{padding-right:75px!important}.pr-80[data-v-49c694ee]{padding-right:80px!important}.pr-85[data-v-49c694ee]{padding-right:85px!important}.pr-90[data-v-49c694ee]{padding-right:90px!important}.pr-95[data-v-49c694ee]{padding-right:95px!important}.pr-100[data-v-49c694ee]{padding-right:100px!important}.universal-access-btn[data-v-49c694ee]{background-color:#fff;border:2px solid transparent;border-radius:100px;color:#1e70c2;cursor:pointer;display:block;font-size:4rem;position:fixed;right:2px;top:2px;z-index:200}.universal-access-btn[data-v-49c694ee]:focus{border:2px solid #f69038;outline:none}.accessibility *[data-v-49c694ee]{line-height:inherit!important}.accessibility[data-v-49c694ee] :not(.icon){font-weight:400!important}.accessibility[data-v-49c694ee]{background-color:#fff;border:2px solid #f69038;border-radius:20px 20px 20px 20px;box-shadow:0 0 40px -10px rgba(0,0,0,.75);padding:30px 30px 10px;position:fixed;right:0;top:0;z-index:1000000000000000}@media screen and (max-width:33.1875em){.accessibility[data-v-49c694ee]{width:100%}}.accessibility__wrapper[data-v-49c694ee]{position:relative}@media screen and (max-width:59.6875em){.accessibility__wrapper[data-v-49c694ee]{flex-direction:column}}.accessibility__wrapper button[data-v-49c694ee]{border:2px solid #dcdee0}.accessibility__wrapper button[data-v-49c694ee]:focus{border:2px solid #f69038}.accessibility__wrapper h3[data-v-49c694ee]{font-size:26px!important}.accessibility__wrapper h3 .icon[data-v-49c694ee]{color:#1e70c2}.accessibility__wrapper h4[data-v-49c694ee]{font-size:18px!important}.accessibility__settings[data-v-49c694ee]{max-height:600px;overflow-x:hidden;overflow-y:auto;padding-right:20px}.accessibility__settings[data-v-49c694ee]::-webkit-scrollbar{-webkit-appearance:none}.accessibility__settings[data-v-49c694ee]::-webkit-scrollbar-track{border-radius:8px;box-shadow:inset 0 0 5px #dcdee0}.accessibility__settings[data-v-49c694ee]::-webkit-scrollbar:vertical{width:3px}.accessibility__settings[data-v-49c694ee]::-webkit-scrollbar-thumb{background-color:#1e70c2;border-radius:1px}.accessibility__settings button[data-v-49c694ee]{border-radius:10px;width:150px}@media screen and (max-width:45.9375em){.accessibility__settings button[data-v-49c694ee]{width:100%}}@media screen and (max-width:59.6875em){.accessibility__settings[data-v-49c694ee]{max-height:200px;padding-right:0;width:100%}}.accessibility__close-top[data-v-49c694ee]{background-color:transparent;border:2px solid #fff!important;color:gray;cursor:pointer;font-size:20px;position:absolute;right:-27px;top:-27px;transition:.3s;width:auto!important}.accessibility__close-top[data-v-49c694ee]:hover{color:#000}.accessibility__close-top[data-v-49c694ee]:focus{border:2px solid #f69038!important;color:#000}.accessibility__link[data-v-49c694ee]{font-size:14px;font-weight:400}.accessibility__link[data-v-49c694ee],.accessibility__section[data-v-49c694ee]{text-align:left!important}.accessibility__colour button[data-v-49c694ee],.accessibility__content button[data-v-49c694ee],.accessibility__font button[data-v-49c694ee]{text-align:center}.accessibility__colour button span[data-v-49c694ee],.accessibility__content button span[data-v-49c694ee],.accessibility__font button span[data-v-49c694ee]{display:block;font-size:20px}.accessibility__font-menu[data-v-49c694ee]{background-color:#f0f3f5;border-radius:20px;padding:20px}.accessibility__font-menu button[data-v-49c694ee]{text-align:center;width:-moz-fit-content;width:fit-content}.accessibility__font-menu button span[data-v-49c694ee]{display:block;font-size:20px}.accessibility__font-menu p[data-v-49c694ee]{display:inline-block;font-size:14px}.accessibility__content[data-v-49c694ee]{margin-bottom:20px}.accessibility__footer[data-v-49c694ee]{float:right;margin-top:20px}.accessibility__reset-settings[data-v-49c694ee]{border:2px solid #dcdee0}.accessibility__reset-settings[data-v-49c694ee]:focus{border:2px solid #f69038}.accessibility__close-bottom[data-v-49c694ee]{border:2px solid #dcdee0}.accessibility__close-bottom[data-v-49c694ee]:focus{border:2px solid #f69038}.accessibility .btn[data-v-49c694ee]{outline:none!important}.accessibility .deactive[data-v-49c694ee]{color:#000;cursor:not-allowed;opacity:.5;pointer-events:none}.accessibility .acc-active[data-v-49c694ee]{background-color:#1e70c2!important;border:1px solid #1e70c2!important;color:#fff!important}.accessibility button[data-v-49c694ee]{font-size:14px!important}
.mt-0[data-v-575455fb]{margin-top:0!important}.mt-5[data-v-575455fb]{margin-top:5px!important}.mt-10[data-v-575455fb]{margin-top:10px!important}.mt-15[data-v-575455fb]{margin-top:15px!important}.mt-20[data-v-575455fb]{margin-top:20px!important}.mt-25[data-v-575455fb]{margin-top:25px!important}.mt-30[data-v-575455fb]{margin-top:30px!important}.mt-35[data-v-575455fb]{margin-top:35px!important}.mt-40[data-v-575455fb]{margin-top:40px!important}.mt-45[data-v-575455fb]{margin-top:45px!important}.mt-50[data-v-575455fb]{margin-top:50px!important}.mt-55[data-v-575455fb]{margin-top:55px!important}.mt-60[data-v-575455fb]{margin-top:60px!important}.mt-65[data-v-575455fb]{margin-top:65px!important}.mt-70[data-v-575455fb]{margin-top:70px!important}.mt-75[data-v-575455fb]{margin-top:75px!important}.mt-80[data-v-575455fb]{margin-top:80px!important}.mt-85[data-v-575455fb]{margin-top:85px!important}.mt-90[data-v-575455fb]{margin-top:90px!important}.mt-95[data-v-575455fb]{margin-top:95px!important}.mt-100[data-v-575455fb]{margin-top:100px!important}.mb-0[data-v-575455fb]{margin-bottom:0!important}.mb-5[data-v-575455fb]{margin-bottom:5px!important}.mb-10[data-v-575455fb]{margin-bottom:10px!important}.mb-15[data-v-575455fb]{margin-bottom:15px!important}.mb-20[data-v-575455fb]{margin-bottom:20px!important}.mb-25[data-v-575455fb]{margin-bottom:25px!important}.mb-30[data-v-575455fb]{margin-bottom:30px!important}.mb-35[data-v-575455fb]{margin-bottom:35px!important}.mb-40[data-v-575455fb]{margin-bottom:40px!important}.mb-45[data-v-575455fb]{margin-bottom:45px!important}.mb-50[data-v-575455fb]{margin-bottom:50px!important}.mb-55[data-v-575455fb]{margin-bottom:55px!important}.mb-60[data-v-575455fb]{margin-bottom:60px!important}.mb-65[data-v-575455fb]{margin-bottom:65px!important}.mb-70[data-v-575455fb]{margin-bottom:70px!important}.mb-75[data-v-575455fb]{margin-bottom:75px!important}.mb-80[data-v-575455fb]{margin-bottom:80px!important}.mb-85[data-v-575455fb]{margin-bottom:85px!important}.mb-90[data-v-575455fb]{margin-bottom:90px!important}.mb-95[data-v-575455fb]{margin-bottom:95px!important}.mb-100[data-v-575455fb]{margin-bottom:100px!important}.ml-0[data-v-575455fb]{margin-left:0!important}.ml-5[data-v-575455fb]{margin-left:5px!important}.ml-10[data-v-575455fb]{margin-left:10px!important}.ml-15[data-v-575455fb]{margin-left:15px!important}.ml-20[data-v-575455fb]{margin-left:20px!important}.ml-25[data-v-575455fb]{margin-left:25px!important}.ml-30[data-v-575455fb]{margin-left:30px!important}.ml-35[data-v-575455fb]{margin-left:35px!important}.ml-40[data-v-575455fb]{margin-left:40px!important}.ml-45[data-v-575455fb]{margin-left:45px!important}.ml-50[data-v-575455fb]{margin-left:50px!important}.ml-55[data-v-575455fb]{margin-left:55px!important}.ml-60[data-v-575455fb]{margin-left:60px!important}.ml-65[data-v-575455fb]{margin-left:65px!important}.ml-70[data-v-575455fb]{margin-left:70px!important}.ml-75[data-v-575455fb]{margin-left:75px!important}.ml-80[data-v-575455fb]{margin-left:80px!important}.ml-85[data-v-575455fb]{margin-left:85px!important}.ml-90[data-v-575455fb]{margin-left:90px!important}.ml-95[data-v-575455fb]{margin-left:95px!important}.ml-100[data-v-575455fb]{margin-left:100px!important}.mr-0[data-v-575455fb]{margin-right:0!important}.mr-5[data-v-575455fb]{margin-right:5px!important}.mr-10[data-v-575455fb]{margin-right:10px!important}.mr-15[data-v-575455fb]{margin-right:15px!important}.mr-20[data-v-575455fb]{margin-right:20px!important}.mr-25[data-v-575455fb]{margin-right:25px!important}.mr-30[data-v-575455fb]{margin-right:30px!important}.mr-35[data-v-575455fb]{margin-right:35px!important}.mr-40[data-v-575455fb]{margin-right:40px!important}.mr-45[data-v-575455fb]{margin-right:45px!important}.mr-50[data-v-575455fb]{margin-right:50px!important}.mr-55[data-v-575455fb]{margin-right:55px!important}.mr-60[data-v-575455fb]{margin-right:60px!important}.mr-65[data-v-575455fb]{margin-right:65px!important}.mr-70[data-v-575455fb]{margin-right:70px!important}.mr-75[data-v-575455fb]{margin-right:75px!important}.mr-80[data-v-575455fb]{margin-right:80px!important}.mr-85[data-v-575455fb]{margin-right:85px!important}.mr-90[data-v-575455fb]{margin-right:90px!important}.mr-95[data-v-575455fb]{margin-right:95px!important}.mr-100[data-v-575455fb]{margin-right:100px!important}.pt-0[data-v-575455fb]{padding-top:0!important}.pt-5[data-v-575455fb]{padding-top:5px!important}.pt-10[data-v-575455fb]{padding-top:10px!important}.pt-15[data-v-575455fb]{padding-top:15px!important}.pt-20[data-v-575455fb]{padding-top:20px!important}.pt-25[data-v-575455fb]{padding-top:25px!important}.pt-30[data-v-575455fb]{padding-top:30px!important}.pt-35[data-v-575455fb]{padding-top:35px!important}.pt-40[data-v-575455fb]{padding-top:40px!important}.pt-45[data-v-575455fb]{padding-top:45px!important}.pt-50[data-v-575455fb]{padding-top:50px!important}.pt-55[data-v-575455fb]{padding-top:55px!important}.pt-60[data-v-575455fb]{padding-top:60px!important}.pt-65[data-v-575455fb]{padding-top:65px!important}.pt-70[data-v-575455fb]{padding-top:70px!important}.pt-75[data-v-575455fb]{padding-top:75px!important}.pt-80[data-v-575455fb]{padding-top:80px!important}.pt-85[data-v-575455fb]{padding-top:85px!important}.pt-90[data-v-575455fb]{padding-top:90px!important}.pt-95[data-v-575455fb]{padding-top:95px!important}.pt-100[data-v-575455fb]{padding-top:100px!important}.pb-0[data-v-575455fb]{padding-bottom:0!important}.pb-5[data-v-575455fb]{padding-bottom:5px!important}.pb-10[data-v-575455fb]{padding-bottom:10px!important}.pb-15[data-v-575455fb]{padding-bottom:15px!important}.pb-20[data-v-575455fb]{padding-bottom:20px!important}.pb-25[data-v-575455fb]{padding-bottom:25px!important}.pb-30[data-v-575455fb]{padding-bottom:30px!important}.pb-35[data-v-575455fb]{padding-bottom:35px!important}.pb-40[data-v-575455fb]{padding-bottom:40px!important}.pb-45[data-v-575455fb]{padding-bottom:45px!important}.pb-50[data-v-575455fb]{padding-bottom:50px!important}.pb-55[data-v-575455fb]{padding-bottom:55px!important}.pb-60[data-v-575455fb]{padding-bottom:60px!important}.pb-65[data-v-575455fb]{padding-bottom:65px!important}.pb-70[data-v-575455fb]{padding-bottom:70px!important}.pb-75[data-v-575455fb]{padding-bottom:75px!important}.pb-80[data-v-575455fb]{padding-bottom:80px!important}.pb-85[data-v-575455fb]{padding-bottom:85px!important}.pb-90[data-v-575455fb]{padding-bottom:90px!important}.pb-95[data-v-575455fb]{padding-bottom:95px!important}.pb-100[data-v-575455fb]{padding-bottom:100px!important}.pl-0[data-v-575455fb]{padding-left:0!important}.pl-5[data-v-575455fb]{padding-left:5px!important}.pl-10[data-v-575455fb]{padding-left:10px!important}.pl-15[data-v-575455fb]{padding-left:15px!important}.pl-20[data-v-575455fb]{padding-left:20px!important}.pl-25[data-v-575455fb]{padding-left:25px!important}.pl-30[data-v-575455fb]{padding-left:30px!important}.pl-35[data-v-575455fb]{padding-left:35px!important}.pl-40[data-v-575455fb]{padding-left:40px!important}.pl-45[data-v-575455fb]{padding-left:45px!important}.pl-50[data-v-575455fb]{padding-left:50px!important}.pl-55[data-v-575455fb]{padding-left:55px!important}.pl-60[data-v-575455fb]{padding-left:60px!important}.pl-65[data-v-575455fb]{padding-left:65px!important}.pl-70[data-v-575455fb]{padding-left:70px!important}.pl-75[data-v-575455fb]{padding-left:75px!important}.pl-80[data-v-575455fb]{padding-left:80px!important}.pl-85[data-v-575455fb]{padding-left:85px!important}.pl-90[data-v-575455fb]{padding-left:90px!important}.pl-95[data-v-575455fb]{padding-left:95px!important}.pl-100[data-v-575455fb]{padding-left:100px!important}.pr-0[data-v-575455fb]{padding-right:0!important}.pr-5[data-v-575455fb]{padding-right:5px!important}.pr-10[data-v-575455fb]{padding-right:10px!important}.pr-15[data-v-575455fb]{padding-right:15px!important}.pr-20[data-v-575455fb]{padding-right:20px!important}.pr-25[data-v-575455fb]{padding-right:25px!important}.pr-30[data-v-575455fb]{padding-right:30px!important}.pr-35[data-v-575455fb]{padding-right:35px!important}.pr-40[data-v-575455fb]{padding-right:40px!important}.pr-45[data-v-575455fb]{padding-right:45px!important}.pr-50[data-v-575455fb]{padding-right:50px!important}.pr-55[data-v-575455fb]{padding-right:55px!important}.pr-60[data-v-575455fb]{padding-right:60px!important}.pr-65[data-v-575455fb]{padding-right:65px!important}.pr-70[data-v-575455fb]{padding-right:70px!important}.pr-75[data-v-575455fb]{padding-right:75px!important}.pr-80[data-v-575455fb]{padding-right:80px!important}.pr-85[data-v-575455fb]{padding-right:85px!important}.pr-90[data-v-575455fb]{padding-right:90px!important}.pr-95[data-v-575455fb]{padding-right:95px!important}.pr-100[data-v-575455fb]{padding-right:100px!important}.top-nav[data-v-575455fb]{background-color:#fff}@media screen and (max-width:64.0625em){.top-nav[data-v-575455fb]{margin-top:35px}}@media screen and (max-width:61.9375em){.top-nav .container[data-v-575455fb]{max-width:none}}.top-nav .corporate[data-v-575455fb]{align-items:center;display:flex;flex-wrap:wrap;justify-content:space-between}@media screen and (max-width:47.9375em){.top-nav .corporate[data-v-575455fb]{justify-content:flex-start}}.top-nav .corporate__logo[data-v-575455fb]{order:1}.top-nav .corporate__logo a[data-v-575455fb]:focus{display:block;font-weight:400!important;outline:2px solid #f69038!important;outline-offset:6px!important}@media screen and (max-width:47.9375em){.top-nav .corporate__logo[data-v-575455fb]{margin-right:auto}}.top-nav .corporate__mobile-search[data-v-575455fb]{display:none}@media screen and (max-width:47.9375em){.top-nav .corporate__mobile-search[data-v-575455fb]{display:block;order:2}}.top-nav .corporate__mobile-search-btn[data-v-575455fb]{background-color:#fff;border:none;margin-right:15px;padding:0}@media screen and (max-width:28.125em){.top-nav .corporate__mobile-search-btn[data-v-575455fb]{display:block}}.top-nav .corporate__mobile-search-btn .icon[data-v-575455fb]{font-size:2.5rem}.top-nav .corporate__mobile-search-invisible[data-v-575455fb]{margin-top:3px}.top-nav .corporate__mobile-search-visible[data-v-575455fb]{position:relative}.top-nav .corporate__mobile-search-visible .icon.fas.fa-search[data-v-575455fb]{margin-top:3px}.top-nav .corporate__mobile-search-visible .icon.fas.fa-slash[data-v-575455fb]{left:-5px;position:absolute;top:0;transform:rotate(90deg)}.top-nav .corporate__mobile-menu[data-v-575455fb]{display:none}@media screen and (max-width:59.625em){.top-nav .corporate__mobile-menu[data-v-575455fb]{display:block;order:3}}.top-nav .corporate__mobile-menu button[data-v-575455fb]{background-color:#fff;border:none;padding:0}.top-nav .corporate__mobile-menu button .icon[data-v-575455fb]{font-size:2.5rem}.top-nav .corporate__search[data-v-575455fb]{align-items:strech;background:#fff;border-radius:3px;display:flex;flex:1;margin:0 25px;order:2;position:relative;z-index:99999}@media screen and (max-width:47.9375em){.top-nav .corporate__search[data-v-575455fb]{flex-basis:100%;margin:5px 0 10px;order:4}}.top-nav .corporate__search-form[data-v-575455fb]{display:flex;flex:1;position:relative}.top-nav .corporate__search-select[data-v-575455fb]{-moz-appearance:none;appearance:none;-webkit-appearance:none;background-color:#fff;background-image:linear-gradient(45deg,transparent 50%,#5d6581 0),linear-gradient(135deg,#5d6581 50%,transparent 0);background-position:calc(100% - 8px) calc(1em - 1px),calc(100% - 3px) calc(1em - 1px),100% 0;background-repeat:no-repeat;background-size:5px 5px,5px 5px,2.5em 2.5em;border-radius:3px 0 0 3px!important;-webkit-border-radius:0;border-right:1px solid transparent;padding:0 20px 0 5px}.top-nav .corporate__search-select[data-v-575455fb]:focus{background-color:rgba(48,136,223,.071);border:1px solid #3088df;outline:none}.top-nav .corporate__search-input[data-v-575455fb]{font-size:1.2rem;margin-left:auto;transition:.3s;width:100%}.top-nav .corporate__search-input[data-v-575455fb]:focus{outline:none}.top-nav .corporate__search-box[data-v-575455fb]{background-color:#fff;border:1px solid #b1b4bf;border-radius:3px;box-shadow:0 4px 10px 0 rgba(0,0,0,.2);display:block;position:absolute;top:32px;width:130%;z-index:99999}@media screen and (max-width:59.625em){.top-nav .corporate__search-box[data-v-575455fb]{width:100%}}.top-nav .corporate__search-main-link[data-v-575455fb]{color:#1a254c;display:block;font-size:1.6rem;padding:7px 10px;-webkit-text-decoration:none;text-decoration:none;transition:all .3s;width:100%;word-break:break-word}.top-nav .corporate__search-main-link[data-v-575455fb]:hover{background-color:#e3edfe;text-indent:3px}.top-nav .corporate__search-results[data-v-575455fb]{background-color:#fff;border-top:1px solid #1a254c;list-style:none;margin:0;max-height:410px;overflow-y:scroll;padding:0}.top-nav .corporate__search-heading-search[data-v-575455fb]{color:gray;font-size:1.2rem;font-weight:700;margin-bottom:0;padding:0 10px}.top-nav .corporate__search-list[data-v-575455fb]{display:flex;padding:7px 10px;transition:all .3s}.top-nav .corporate__search-list[data-v-575455fb]:hover{background-color:#e8ecee}.top-nav .corporate__search-list .icon[data-v-575455fb]{color:#1a254c;font-size:1rem;margin-right:5px;margin-top:2px}.top-nav .corporate__search-link[data-v-575455fb]{color:#1a254c;display:inline-block}.top-nav .corporate__search-link[data-v-575455fb],.top-nav .corporate__search-related-link[data-v-575455fb]{cursor:pointer;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:-moz-fit-content;width:fit-content}.top-nav .corporate__search-related-link[data-v-575455fb]{color:#3088df;font-size:1.2rem;line-height:17px;margin-left:5px}.top-nav .corporate__search-related-link[data-v-575455fb]:hover{-webkit-text-decoration:underline;text-decoration:underline}.top-nav .corporate__search-btn[data-v-575455fb]{border-radius:0 3px 3px 0;margin:0;padding:6px 25px}.top-nav .corporate__search-btn[data-v-575455fb]:focus{outline:2px solid #f69038!important;outline-offset:6px!important}.top-nav .corporate__nav[data-v-575455fb]{order:3}@media screen and (max-width:59.625em){.top-nav .corporate__nav[data-v-575455fb]{display:none}}.top-nav .corporate__links[data-v-575455fb]{display:flex;flex-wrap:wrap;padding:0}.top-nav .corporate__link-item[data-v-575455fb]{list-style:none;outline:none;padding:10px 0;position:relative}.top-nav .corporate__link-item[data-v-575455fb]:not(:last-child){align-self:center;margin-right:15px}.top-nav .corporate__link-item[data-v-575455fb]:focus-within,.top-nav .corporate__link-item[data-v-575455fb]:hover{background-color:hsla(0,0%,100%,.2);transition:all .2s ease}.top-nav .corporate__link-item:focus-within .corporate__link[data-v-575455fb],.top-nav .corporate__link-item:hover .corporate__link[data-v-575455fb]{color:#1e70c2}.top-nav .corporate__link-item:focus-within .corporate__link-submenu[data-v-575455fb],.top-nav .corporate__link-item:hover .corporate__link-submenu[data-v-575455fb]{visibility:visible}.top-nav .corporate__link-item:first-child:hover span[data-v-575455fb]{color:#1e70c2;transition:all .2s ease}.top-nav .corporate__link-item:focus-within .icon[data-v-575455fb],.top-nav .corporate__link-item:hover .icon[data-v-575455fb]{color:#1e70c2;transform:rotate(180deg);transition:transform .2s ease}.top-nav .corporate__link-item .icon[data-v-575455fb]{color:#1a254c;transition:transform .2s ease}.top-nav .corporate__link-item .icon[data-v-575455fb]:hover{cursor:pointer}.top-nav .corporate__link[data-v-575455fb]{color:#1a254c;cursor:pointer;font-size:1.6rem;-webkit-text-decoration:none;text-decoration:none;transition:all .2s ease}.top-nav .corporate__link-submenu[data-v-575455fb]{background-color:#fff;box-shadow:0 0 10px 0 rgba(0,0,0,.2);display:block;padding:0;position:absolute;top:35px;visibility:hidden;white-space:nowrap;z-index:300}.top-nav .corporate__link-submenu[data-v-575455fb]:before{border:7px solid transparent;border-bottom-color:#fff;content:"";height:0;left:50%;position:absolute;top:-14px;transform:translateX(-50%);width:14px}.top-nav .corporate__link-submenu li[data-v-575455fb]{list-style:none}.top-nav .corporate__link-submenu li a[data-v-575455fb]{color:#1a254c;display:block;padding:10px 20px;-webkit-text-decoration:none!important;text-decoration:none!important}.top-nav .corporate__link-submenu li a[data-v-575455fb]:focus-within,.top-nav .corporate__link-submenu li a[data-v-575455fb]:hover{background-color:#eceff9;font-weight:400}.top-nav .corporate__link-submenu--my-roles[data-v-575455fb]:focus{outline:2px solid #f69038!important;outline-offset:6px!important}.top-nav .corporate__link-item--logged-in[data-v-575455fb]{list-style:none}.top-nav .corporate__logged-in-link[data-v-575455fb]{cursor:pointer;font-weight:400;-webkit-text-decoration:none;text-decoration:none}.top-nav .corporate__user-account[data-v-575455fb]{align-items:center;display:flex;flex-wrap:wrap;justify-content:space-between}.top-nav .corporate__user-info[data-v-575455fb]{margin-left:5px;margin-right:5px}.top-nav .corporate__user-info p[data-v-575455fb]{color:#1a254c;line-height:10px}.top-nav .corporate__user-info small[data-v-575455fb]{color:#1a254c;opacity:.7}.top-nav .corporate__user-img[data-v-575455fb]{border-radius:50px;overflow:hidden}.top-nav .corporate__user-img img[data-v-575455fb]{height:auto;width:35px}.top-nav .corporate__user-details .icon[data-v-575455fb]{color:#1a254c!important;font-size:1.4rem}.top-nav .corporate__link-submenu--user[data-v-575455fb]{min-width:180px;padding:0;top:50px}.top-nav .corporate__link-submenu--user p[data-v-575455fb]{font-weight:700;padding:0 20px}.bottom-nav-1[data-v-575455fb]{background-color:#1a254c}@media screen and (max-width:59.625em){.bottom-nav-1[data-v-575455fb]{display:none}}@media screen and (max-width:47.9375em){.bottom-nav-1 .container[data-v-575455fb]{margin-left:0;margin-right:0;max-width:none;padding:0}}@media screen and (max-width:61.9375em){.bottom-nav-1 .container[data-v-575455fb]{max-width:none}}@media screen and (max-width:59.6875em){.bottom-nav-2[data-v-575455fb]{background-color:#1a254c;padding:0}}@media screen and (min-width:59.6875em){.bottom-nav-2[data-v-575455fb]{display:none}}.bottom-nav-2 .journal ul[data-v-575455fb]{margin:0;padding:0;width:100%}.bottom-nav-2 .journal__link-item--journals[data-v-575455fb]{width:100%}@media screen and (max-width:37.5em){.bottom-nav-2 .journal__link-submenu-journals li a[data-v-575455fb]{padding:10px}}@media screen and (max-width:26.9375em){.bottom-nav-2 .journal__link-submenu-journals li a[data-v-575455fb]{padding:10px 10px 15px}}@media screen and (max-width:37.5em){.bottom-nav-2 .journal__link--home[data-v-575455fb]{padding:10px}}.journal[data-v-575455fb]{align-items:center;display:flex;flex-wrap:wrap;justify-content:flex-start}.journal__nav[data-v-575455fb]{background-color:#1a254c;display:flex}.journal__links[data-v-575455fb]{display:flex;flex-wrap:wrap;margin:0;padding:0}.journal__link-item[data-v-575455fb]{cursor:pointer;list-style:none;outline:none;padding:16px 0;position:relative;transition:all .2s ease}@media screen and (max-width:45.9375em){.journal__link-item[data-v-575455fb]{width:100%}}.journal__link-item:focus-within .journal__link[data-v-575455fb],.journal__link-item:hover .journal__link[data-v-575455fb]{color:#b3b3b3;transition:all .2s ease}.journal__link-item:focus-within .icon.fa-caret-down[data-v-575455fb],.journal__link-item:hover .icon.fa-caret-down[data-v-575455fb]{color:#b3b3b3;transform:rotate(180deg);transition:all .2s ease}.journal__link-item:focus-within .journal__link-submenu[data-v-575455fb],.journal__link-item:hover .journal__link-submenu[data-v-575455fb]{visibility:visible}.journal__link-item .icon[data-v-575455fb]{color:#fff;font-size:14px}.journal__link-item--journals[data-v-575455fb]{padding:0}.journal__journals-list[data-v-575455fb]{outline:none}.journal__journals-list:focus .icon[data-v-575455fb]{border:1px solid #fff;transform:rotate(1turn);transition:.5s}.journal__journals-list a[data-v-575455fb]{padding-right:10px}.journal__journals-list .icon[data-v-575455fb]{border-left:1px solid #fff;font-size:1.6rem}.journal__journals-list .icon.fas.fa-arrow-down[data-v-575455fb]{padding:16px}@media screen and (max-width:37.5em){.journal__journals-list .icon.fas.fa-arrow-down[data-v-575455fb]{padding:12px}}.journal__journals-list .icon.fas.fa-times[data-v-575455fb]{padding:16px 17.5px}@media screen and (max-width:37.5em){.journal__journals-list .icon.fas.fa-times[data-v-575455fb]{padding:12px 13.5px}}.journal__link-item-container[data-v-575455fb]{align-items:center;display:flex;justify-content:space-between}.journal__link[data-v-575455fb]{color:#fff;cursor:pointer;font-size:1.6rem;-webkit-text-decoration:none;text-decoration:none;transition:all .2s ease}@media screen and (max-width:45.9375em){.journal__link[data-v-575455fb]{padding-left:30px}}.journal__link--home[data-v-575455fb]{color:#fff;cursor:pointer;font-size:1.6rem;font-weight:400!important;padding:16px;-webkit-text-decoration:none;text-decoration:none;transition:.3s;width:100%}.journal__link--home[data-v-575455fb]:focus{text-indent:4px}.journal__link-submenu[data-v-575455fb]{background:#fff;box-shadow:0 0 10px 0 rgba(0,0,0,.2);display:block;padding:0;position:absolute;top:42px;visibility:hidden;white-space:nowrap;z-index:300}.journal__link-submenu[data-v-575455fb]:before{border:7px solid transparent;border-bottom-color:#fff;content:"";height:0;left:50%;position:absolute;top:-14px;transform:translateX(-50%);width:14px}@media screen and (max-width:45.9375em){.journal__link-submenu[data-v-575455fb]:before{display:none}.journal__link-submenu[data-v-575455fb]{position:static}}.journal__link-submenu li[data-v-575455fb]{list-style:none}.journal__link-submenu li a[data-v-575455fb]{color:#1a254c;display:block;padding:10px 20px;-webkit-text-decoration:none!important;text-decoration:none!important}.journal__link-submenu li a[data-v-575455fb]:focus,.journal__link-submenu li a[data-v-575455fb]:hover{background-color:#eceff9;font-weight:400}@media screen and (max-width:45.9375em){.journal__link-submenu li a[data-v-575455fb]{padding:10px 40px}}.journal__link-submenu-journals[data-v-575455fb]{background:#1e70c2;box-shadow:0 4px 10px 0 rgba(0,0,0,.2);display:none;height:400px;left:0;overflow-y:scroll;padding:0;position:absolute;top:51px;white-space:nowrap;width:437px;z-index:300}@media screen and (max-width:45.9375em){.journal__link-submenu-journals[data-v-575455fb]{position:static}}.journal__link-submenu-journals li[data-v-575455fb]{border-top:1px solid #fff;list-style:none;transition:all .3s ease}.journal__link-submenu-journals li[data-v-575455fb]:focus,.journal__link-submenu-journals li[data-v-575455fb]:hover{background-color:#2d77c6;font-weight:400;text-indent:8px}.journal__link-submenu-journals li a[data-v-575455fb]{color:#fff;display:flex;flex-wrap:wrap;font-size:1.6rem;justify-content:space-between;padding:10px 10px 10px 15px;position:relative;-webkit-text-decoration:none!important;text-decoration:none!important}@media screen and (max-width:22.1875em){.journal__link-submenu-journals li a[data-v-575455fb]{font-size:1.4rem}}.journal__link-submenu-journals li a span.articles-number[data-v-575455fb]{align-self:center;font-size:1rem}.journal__link-item--journal-info[data-v-575455fb]{margin-left:29px}.journal__link-item--browse[data-v-575455fb],.journal__link-item--journal-info[data-v-575455fb]{margin-right:29px}.journal__link-item--journals[data-v-575455fb]{background-color:#1e70c2;width:437px}.journal__link-submenu--journal-info[data-v-575455fb]{left:-50px}.journal__link-submenu--browse[data-v-575455fb]{left:-30px}.journal__link-submenu--select[data-v-575455fb]{align-items:center;display:flex;padding:10px 20px}@media screen and (max-width:45.9375em){.journal__link-submenu--select[data-v-575455fb]{padding:10px 40px!important}}.journal__link-submenu--select label[data-v-575455fb]{margin-right:10px}.journal__link-submenu--select select[data-v-575455fb]{width:-webkit-fill-available}.journal__link-submenu--select select[data-v-575455fb]:focus{outline:none}.journal__submit-article[data-v-575455fb]{font-size:1.6rem!important;-webkit-text-decoration:none!important;text-decoration:none!important}.journal__submit-article[data-v-575455fb]:focus{font-weight:400;outline:2px solid #f69038!important;outline-offset:6px!important}@media screen and (max-width:45.9375em){.journal__submit-article[data-v-575455fb]{margin-left:30px;padding:12px}}.mobile-nav[data-v-575455fb]{background-color:#1a254c}@media screen and (max-width:59.625em){.mobile-nav[data-v-575455fb]{display:block}}.mobile-nav a[data-v-575455fb]{-webkit-text-decoration:none;text-decoration:none}.mobile-nav__links[data-v-575455fb]{display:flex;flex-direction:column;flex-wrap:wrap}.mobile-nav__links a[data-v-575455fb]{border-bottom:1px solid #485170;color:#fff;padding:12px 15px}.mobile-nav__links[data-v-575455fb]:first-child{border-top:1px solid #485170}.mobile-nav__expandable[data-v-575455fb]{align-items:center;justify-content:space-between}.mobile-nav__expandable[data-v-575455fb],.mobile-nav__user-account[data-v-575455fb]{display:flex;flex-wrap:wrap}.mobile-nav__user-img[data-v-575455fb]{border:2px solid #fff;border-radius:50px;margin-right:10px;overflow:hidden}.mobile-nav__user-img img[data-v-575455fb]{height:auto;width:35px}.mobile-nav__user-info small[data-v-575455fb]{opacity:.7}.mobile-nav__user-submenu[data-v-575455fb]{background-color:#313b5e;display:flex;flex-basis:100%;flex-direction:column}.mobile-nav__user-submenu p[data-v-575455fb]{color:#fff;font-weight:700;opacity:.5;padding:0 15px}.mobile-nav__user-submenu a[data-v-575455fb]{border-bottom:none}.mobile-nav__social-media[data-v-575455fb]{padding:12px 15px}.mobile-nav__social-media p[data-v-575455fb]{color:#fff;font-weight:700}input[data-v-575455fb]{border-radius:0!important}.show-journals[data-v-575455fb]{display:block!important}.journal__link-submenu-journals[data-v-575455fb]::-webkit-scrollbar{width:7px}.journal__link-submenu-journals[data-v-575455fb]::-webkit-scrollbar-track{background-color:#fff;border:1px solid rgba(0,0,0,.15)}.journal__link-submenu-journals[data-v-575455fb]::-webkit-scrollbar-thumb{background:#2d77c6;border-radius:20px}.journal__link-submenu-journals[data-v-575455fb]::-webkit-scrollbar-thumb:hover{background:#1e60bc}.corporate__search-results[data-v-575455fb]::-webkit-scrollbar{width:5.5px}.corporate__search-results[data-v-575455fb]::-webkit-scrollbar-track{background-color:#fff;border:1px solid rgba(0,0,0,.15);border-radius:3px}.corporate__search-results[data-v-575455fb]::-webkit-scrollbar-thumb{background:#1e70c2;border-radius:20px}.overlay[data-v-575455fb]{background:rgba(17,26,55,.702);height:100vh;left:0;padding:200vh 200vw;position:fixed;top:0;width:100%;z-index:999}.remove-styling[data-v-575455fb]{border:0!important;padding:0!important}
.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}.logo-img{max-width:340px;width:100%}
.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}.tabs{border-bottom:2px solid #939ab1;display:flex;justify-content:space-evenly;margin:40px 0 30px}@media screen and (max-width:33.1875em){.tabs{text-align:center}}.tabs a{border-bottom:2px solid #939ab1;color:#1a254c;font-size:1.6rem;font-weight:700;margin-bottom:-2px;padding:10px 10px 5px}.tabs a:focus,.tabs a:hover{background-color:#b8d6f4;border-bottom:2px solid #1e70c2;outline:none;-webkit-text-decoration:none;text-decoration:none}.tabs .nuxt-link-exact-active{border-bottom:2px solid #1e70c2;color:#1e70c2}.tabs-metrics,.tabs-tweetations{align-items:baseline;display:flex;flex-wrap:wrap;justify-content:flex-start;margin-bottom:20px}.tabs-metrics a,.tabs-tweetations a{color:#1a254c;cursor:pointer;padding:10px 0 5px;text-align:center}.tabs-metrics a:focus,.tabs-metrics a:hover,.tabs-tweetations a:focus,.tabs-tweetations a:hover{color:#1e70c2}.tabs-metrics span,.tabs-tweetations span{margin:0 10px}.active{border-bottom:2px solid #1e70c2!important;color:#1e70c2!important}.mobile-show{display:none}@media screen and (max-width:47.9375em){.mobile-show{display:block}}.desktop-show{display:block}@media screen and (max-width:47.9375em){.desktop-show{display:none}}.advertisement{margin-top:25px}.advertisement:focus .advertisement__text,.advertisement:hover .advertisement__text{color:#1e70c2}.advertisement__link{background-color:#f1f3f5;border-radius:3px;display:flex;font-weight:700;justify-content:space-between;padding:20px;-webkit-text-decoration:none;text-decoration:none}@media screen and (max-width:33.1875em){.advertisement__link{flex-wrap:wrap}}.advertisement__text{color:#000;font-size:1.8rem;padding-right:10px}.advertisement__button{align-self:center;background-color:#1e70c2;border-radius:3px;color:#fff;font-size:2rem;padding:15px 20px;position:relative;transition:.3s;width:200px}@media screen and (max-width:74.9375em){.advertisement__button{width:150px}}@media screen and (max-width:47.9375em){.advertisement__button{width:200px}}@media screen and (max-width:33.1875em){.advertisement__button{margin-left:auto;margin-top:10px;padding:10px 20px;width:170px}}.advertisement__button .advertisement__icon{font-size:1.7rem;position:absolute;right:2.2rem;top:2rem;transition:.3s}@media screen and (max-width:33.1875em){.advertisement__button .advertisement__icon{top:1.5rem}}.advertisement__button:focus,.advertisement__button:hover{opacity:.9}.advertisement__button:focus .advertisement__icon,.advertisement__button:hover .advertisement__icon{right:1.5rem}.authors-for-screen-reader{height:1px;left:-10000px;overflow:hidden;position:absolute;top:auto;width:1px}.authors-for-screen-reader:focus{display:block;font-weight:700;height:auto;outline:2px solid #f69038!important;outline-offset:6px!important;overflow:hidden;position:static;width:auto}.main .details{background-color:#f1f3f5;border-radius:3px;margin-top:25px;padding:10px}.main .details p{font-size:1.6rem;font-weight:700;margin:0}.main .preprints-version{align-items:baseline;display:flex;margin-top:10px}.info{display:flex;flex-wrap:wrap}.info__article-img{cursor:pointer;flex:0 0 22%;height:-moz-fit-content;height:fit-content;margin-right:20px;position:relative}@media screen and (max-width:61.9375em){.info__article-img{display:none}}.info__article-img img{display:block;width:100%}.info__article-img-info{background-color:rgba(26,37,76,.749);bottom:0;color:#fff;height:0;left:0;margin:0;overflow:hidden;position:absolute;right:0;text-align:center;transition:.5s ease;width:100%}.info__article-img-info .icon{font-size:3rem;margin-top:30%}.info__article-img:hover .info__article-img-info{height:100%}.info__title-authors{flex:0 0 75%}@media screen and (max-width:61.9375em){.info__title-authors{flex:0 0 100%}}.info__title-authors h3:focus{outline:2px solid #f69038!important;outline-offset:6px!important}.info__hidden-title{height:1px;left:-10000px;overflow:hidden;position:absolute;top:auto;width:1px}.info__authors{display:inline-block}.info__orcid-img{height:15px}.tabs a{flex-grow:1;text-align:center}@media screen and (max-width:47.9375em){.tabs{flex-direction:column}}.sidebar-citation .export-metadata div{margin-bottom:10px}.sidebar-citation .collection h4:focus{outline:2px solid #f69038!important;outline-offset:6px!important}.sidebar-citation .collection__span{display:block}@media screen and (max-width:61.9375em){.sidebar-citation .collection__span{display:inline-block}}.sidebar-citation .collection__link{background:#1e70c2;border-radius:3px;color:#fff;display:block;font-size:1.2rem;margin-bottom:5px;padding:5px 10px;width:-moz-fit-content;width:fit-content}.sidebar-citation .download-btns{display:flex;flex-wrap:wrap;justify-content:space-between;margin-bottom:20px}@media screen and (max-width:61.9375em){.sidebar-citation .download-btns{justify-content:flex-start}.sidebar-citation .download-btns a{margin-right:10px}}.article-content h3{font-size:2.2rem;line-height:2.4rem}.article-content ol li,.article-content ul li{margin-bottom:10px}.main-article-content a{color:#1e70c2}.main-article-content a:hover{-webkit-text-decoration:underline;text-decoration:underline}.author-affiliation-details,.authors-container .authors.clearfix .clearfix,.corresponding-author-and-affiliations,.h4-original-paper{display:none}.authors .clearfix{display:flex;flex-wrap:wrap;list-style:none;padding:0}.authors .clearfix li{margin-right:10px}.authors .clearfix li a{color:#1e70c2}.article-content figure{background-color:#f1f3f5;border-radius:3px;margin:0;padding:20px}#Abstract{margin-top:10px}#Abstract,#Discussion,#Introduction,#Keywords,#Methods,#Results{border-bottom:1px solid #ccd1d5}.abstract-sub-heading{display:block;font-size:1.6rem;font-weight:700}.figure-table{background:#f1f3f5;border-radius:3px;height:auto;margin-bottom:20px;overflow-x:auto;overflow-y:hidden;padding:20px 20px 0}.figure-table::-webkit-scrollbar{-webkit-appearance:none}.figure-table::-webkit-scrollbar-track{border-radius:8px;box-shadow:inset 0 0 5px #5d6581}.figure-table::-webkit-scrollbar:vertical{width:8px}.figure-table::-webkit-scrollbar:horizontal{height:8px}.figure-table::-webkit-scrollbar-thumb{background-color:#1e70c2;border-radius:8px}.textbox-container{border:2px solid #333;padding:15px}.textbox-container h5{margin-bottom:0;margin-top:0}.footnotes ol{word-wrap:break-word}img.figure-image,img.graphic-image,img.inline-graphic-image{width:100%}table{margin:0}td,th{padding:5px 0 5px 15px}.careers{margin-top:25px}.career-widget{border-bottom:1px solid #ccc;cursor:pointer;padding:5px 20px 20px;transition:box-shadow .3s ease}.career-widget:hover{box-shadow:0 0 10px 0 rgba(0,0,0,.1)}.job,.job:hover{color:inherit;-webkit-text-decoration:none;text-decoration:none}.job:hover .job-title{-webkit-text-decoration:underline;text-decoration:underline}.job-title{font-weight:700;margin-top:20px}.icon-prereview{margin-top:5px}.pre-review-container{display:flex;line-height:16pt;margin-top:10px}.pre-review-text{display:inline-block;margin-left:5px}.pre-review-text a{-webkit-text-decoration:underline;text-decoration:underline}
.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}@media screen and (max-width:61.9375em){.sidebar-sections{display:none}}.sidebar-sections ul{color:#1a254c;padding:0 10px 0 15px}.sidebar-sections ul li{margin-bottom:1rem}.sidebar-sections ul li a{color:#1a254c;font-weight:400!important;word-break:break-word}.sidebar-sections ul li a:hover{color:#1e70c2}.sidebar-sections ul li a:focus{-webkit-text-decoration:none!important;text-decoration:none!important}.sidebar-sections ul li:hover .active{-webkit-text-decoration:none;text-decoration:none}.sidebar-sections ul li .active{font-weight:400!important;-webkit-text-decoration:none;text-decoration:none}.sidebar-nav{height:100%;left:0;position:absolute;top:0;width:100%}.sidebar-nav-sticky{position:sticky;top:0}.article{padding:0}.tooltip{cursor:pointer;position:relative}.tooltip .tooltiptext{background-color:#000;border-radius:6px;color:#fff;font-family:"Helvetica","Sans-serif";font-size:.9rem;left:50%;margin-left:-125px;margin-top:2px;min-width:250px;padding:10px;position:absolute;text-align:center;text-transform:none;top:100%;visibility:hidden;z-index:1}@media(max-width:768px){.tooltip .tooltiptext{display:none}}.tooltip:hover .tooltiptext{visibility:visible}@media(min-width:768px){.visual-abstract{display:none}}@media(max-width:767px){.visual-abstract{display:block}}.figure-table ul{margin:auto}named-content{display:block;white-space:pre-wrap}
.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}.email-subscribtion-button{margin-left:auto}.email-subscribtion-button h3{margin-top:0}@media screen and (max-width:35.9375em){.email-subscribtion-button h3{margin-top:50px}}.email-subscribtion-button a{background-color:#fff;border:2px solid #fff;border-radius:100px;color:#1a254c;cursor:pointer;display:block;margin:auto;padding:10px;position:relative;text-align:center;transition:all .2s ease;width:205px}.email-subscribtion-button a span{font-size:1.6rem;font-weight:700;position:relative}.email-subscribtion-button a span.icon{position:absolute;right:38px;transition:all .2s ease}.email-subscribtion-button a:before{background-color:#1a254c;border-radius:28px;content:"";display:block;height:38px;left:0;position:absolute;top:0;transition:all .3s ease;width:50px}.email-subscribtion-button a:hover{color:#fff}.email-subscribtion-button a:hover span.icon{right:25px;transform:rotate(45deg)}.email-subscribtion-button a:hover:before{width:100%}.email-subscribtion-button a:active{color:#1a254c}.email-subscribtion-button a:active span.icon{right:20px}.email-subscribtion-button a:active:before{background-color:#fff}.footer-modal-window .modal-window-body{max-height:650px}.footer-modal-window .mc-field-group ul{padding:0}.footer-modal-window .mc-field-group ul li{list-style-type:none;margin-bottom:2px}.footer-modal-window .mc-field-group ul li input{cursor:pointer;height:15px;width:15px}.footer-modal-window .mc-field-group ul li label{font-size:1.6rem}.footer-modal-window .email-subscribtion label{display:block;font-weight:700;margin:5px 0;max-width:-moz-fit-content;max-width:fit-content}.footer-modal-window .email-subscribtion input{width:100%}.footer-modal-window .email-subscribtion input.error{background-color:#fcf1f1!important;border:1px solid red!important}.footer-modal-window .email-subscribtion small.error{color:red;display:block}.footer{background:#1a254c}.footer-journal-name{background:#111831;padding:7px 0}.footer-journal-name h2{color:#fff;font-size:1.8rem;line-height:2.4rem;margin:5px 0;text-align:center}.footer-title{color:#fff;font-size:1.6rem;font-weight:700;line-height:2.2rem;margin:50px 0 18px}.footer-title:focus{outline:2px solid #f69038!important;outline-offset:6px!important}.footer ul{padding:0}.footer ul li{list-style-type:none;margin-bottom:12px}.footer ul li a{color:#fff;font-size:1.6rem;font-weight:light;opacity:.6;-webkit-text-decoration:none;text-decoration:none}.footer ul li a:focus,.footer ul li a:hover{opacity:1}.footer-social .bluesky a{background-color:#0085ff;border-radius:50px;margin-right:1px;padding:4px 5px;text-align:center;transition:all .2s ease}.footer-social .bluesky a:hover{background-color:#0078e6;transition:all .2s ease}.footer-social .twitter a{border-radius:50px;margin-right:1px;padding:4px 5px;text-align:center}.footer-social .twitter a,.footer-social .twitter a:hover{background-color:#000;transition:all .2s ease}.footer-social .facebook a{background-color:#3b5a98;border-radius:50px;margin-right:1px;padding:4px 5px;text-align:center;transition:all .2s ease}.footer-social .facebook a:hover{background-color:#344f86;transition:all .2s ease}.footer-social .linkedin a{background-color:#0077b5;border-radius:50px;margin-right:1px;padding:4px 5px;text-align:center;transition:all .2s ease}.footer-social .linkedin a:hover{background-color:#00669c;transition:all .2s ease}.footer-social .youtube a{background-color:red;border-radius:50px;margin-right:1px;padding:4px 5px;text-align:center;transition:all .2s ease}.footer-social .youtube a:hover{background-color:#e60000;transition:all .2s ease}.footer-social a:hover{-webkit-text-decoration:none;text-decoration:none}.footer-social a i{color:#fff;text-align:center;width:14px}.footer-copyright{border-top:1px solid #fff;color:#fff;font-size:1.2rem;margin:20px 0 40px;opacity:.6;padding-top:20px}@media screen and (max-width:20.6875em){.footer .rss{margin-top:.25rem}}
.mt-0{margin-top:0!important}.mt-5{margin-top:5px!important}.mt-10{margin-top:10px!important}.mt-15{margin-top:15px!important}.mt-20{margin-top:20px!important}.mt-25{margin-top:25px!important}.mt-30{margin-top:30px!important}.mt-35{margin-top:35px!important}.mt-40{margin-top:40px!important}.mt-45{margin-top:45px!important}.mt-50{margin-top:50px!important}.mt-55{margin-top:55px!important}.mt-60{margin-top:60px!important}.mt-65{margin-top:65px!important}.mt-70{margin-top:70px!important}.mt-75{margin-top:75px!important}.mt-80{margin-top:80px!important}.mt-85{margin-top:85px!important}.mt-90{margin-top:90px!important}.mt-95{margin-top:95px!important}.mt-100{margin-top:100px!important}.mb-0{margin-bottom:0!important}.mb-5{margin-bottom:5px!important}.mb-10{margin-bottom:10px!important}.mb-15{margin-bottom:15px!important}.mb-20{margin-bottom:20px!important}.mb-25{margin-bottom:25px!important}.mb-30{margin-bottom:30px!important}.mb-35{margin-bottom:35px!important}.mb-40{margin-bottom:40px!important}.mb-45{margin-bottom:45px!important}.mb-50{margin-bottom:50px!important}.mb-55{margin-bottom:55px!important}.mb-60{margin-bottom:60px!important}.mb-65{margin-bottom:65px!important}.mb-70{margin-bottom:70px!important}.mb-75{margin-bottom:75px!important}.mb-80{margin-bottom:80px!important}.mb-85{margin-bottom:85px!important}.mb-90{margin-bottom:90px!important}.mb-95{margin-bottom:95px!important}.mb-100{margin-bottom:100px!important}.ml-0{margin-left:0!important}.ml-5{margin-left:5px!important}.ml-10{margin-left:10px!important}.ml-15{margin-left:15px!important}.ml-20{margin-left:20px!important}.ml-25{margin-left:25px!important}.ml-30{margin-left:30px!important}.ml-35{margin-left:35px!important}.ml-40{margin-left:40px!important}.ml-45{margin-left:45px!important}.ml-50{margin-left:50px!important}.ml-55{margin-left:55px!important}.ml-60{margin-left:60px!important}.ml-65{margin-left:65px!important}.ml-70{margin-left:70px!important}.ml-75{margin-left:75px!important}.ml-80{margin-left:80px!important}.ml-85{margin-left:85px!important}.ml-90{margin-left:90px!important}.ml-95{margin-left:95px!important}.ml-100{margin-left:100px!important}.mr-0{margin-right:0!important}.mr-5{margin-right:5px!important}.mr-10{margin-right:10px!important}.mr-15{margin-right:15px!important}.mr-20{margin-right:20px!important}.mr-25{margin-right:25px!important}.mr-30{margin-right:30px!important}.mr-35{margin-right:35px!important}.mr-40{margin-right:40px!important}.mr-45{margin-right:45px!important}.mr-50{margin-right:50px!important}.mr-55{margin-right:55px!important}.mr-60{margin-right:60px!important}.mr-65{margin-right:65px!important}.mr-70{margin-right:70px!important}.mr-75{margin-right:75px!important}.mr-80{margin-right:80px!important}.mr-85{margin-right:85px!important}.mr-90{margin-right:90px!important}.mr-95{margin-right:95px!important}.mr-100{margin-right:100px!important}.pt-0{padding-top:0!important}.pt-5{padding-top:5px!important}.pt-10{padding-top:10px!important}.pt-15{padding-top:15px!important}.pt-20{padding-top:20px!important}.pt-25{padding-top:25px!important}.pt-30{padding-top:30px!important}.pt-35{padding-top:35px!important}.pt-40{padding-top:40px!important}.pt-45{padding-top:45px!important}.pt-50{padding-top:50px!important}.pt-55{padding-top:55px!important}.pt-60{padding-top:60px!important}.pt-65{padding-top:65px!important}.pt-70{padding-top:70px!important}.pt-75{padding-top:75px!important}.pt-80{padding-top:80px!important}.pt-85{padding-top:85px!important}.pt-90{padding-top:90px!important}.pt-95{padding-top:95px!important}.pt-100{padding-top:100px!important}.pb-0{padding-bottom:0!important}.pb-5{padding-bottom:5px!important}.pb-10{padding-bottom:10px!important}.pb-15{padding-bottom:15px!important}.pb-20{padding-bottom:20px!important}.pb-25{padding-bottom:25px!important}.pb-30{padding-bottom:30px!important}.pb-35{padding-bottom:35px!important}.pb-40{padding-bottom:40px!important}.pb-45{padding-bottom:45px!important}.pb-50{padding-bottom:50px!important}.pb-55{padding-bottom:55px!important}.pb-60{padding-bottom:60px!important}.pb-65{padding-bottom:65px!important}.pb-70{padding-bottom:70px!important}.pb-75{padding-bottom:75px!important}.pb-80{padding-bottom:80px!important}.pb-85{padding-bottom:85px!important}.pb-90{padding-bottom:90px!important}.pb-95{padding-bottom:95px!important}.pb-100{padding-bottom:100px!important}.pl-0{padding-left:0!important}.pl-5{padding-left:5px!important}.pl-10{padding-left:10px!important}.pl-15{padding-left:15px!important}.pl-20{padding-left:20px!important}.pl-25{padding-left:25px!important}.pl-30{padding-left:30px!important}.pl-35{padding-left:35px!important}.pl-40{padding-left:40px!important}.pl-45{padding-left:45px!important}.pl-50{padding-left:50px!important}.pl-55{padding-left:55px!important}.pl-60{padding-left:60px!important}.pl-65{padding-left:65px!important}.pl-70{padding-left:70px!important}.pl-75{padding-left:75px!important}.pl-80{padding-left:80px!important}.pl-85{padding-left:85px!important}.pl-90{padding-left:90px!important}.pl-95{padding-left:95px!important}.pl-100{padding-left:100px!important}.pr-0{padding-right:0!important}.pr-5{padding-right:5px!important}.pr-10{padding-right:10px!important}.pr-15{padding-right:15px!important}.pr-20{padding-right:20px!important}.pr-25{padding-right:25px!important}.pr-30{padding-right:30px!important}.pr-35{padding-right:35px!important}.pr-40{padding-right:40px!important}.pr-45{padding-right:45px!important}.pr-50{padding-right:50px!important}.pr-55{padding-right:55px!important}.pr-60{padding-right:60px!important}.pr-65{padding-right:65px!important}.pr-70{padding-right:70px!important}.pr-75{padding-right:75px!important}.pr-80{padding-right:80px!important}.pr-85{padding-right:85px!important}.pr-90{padding-right:90px!important}.pr-95{padding-right:95px!important}.pr-100{padding-right:100px!important}.scroll-to-very-top{background-color:#1e70c2;border-radius:3px 0 0 3px;bottom:.7vh;box-shadow:0 0 5px rgba(0,0,0,.2);position:fixed;right:0;z-index:2}.scroll-to-very-top:focus,.scroll-to-very-top:hover{opacity:.9}.scroll-to-very-top:focus{border:2px solid #f69038;margin:1px}.scroll-to-very-top .icon{color:#fff;font-size:2.5rem;padding:9px;transition:all .3s}</style></head><body ><noscript data-n-head="ssr" data-hid="gtm-noscript" data-pbody="true"><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58BHBF4V&" height="0" width="0" style="display:none;visibility:hidden" title="gtm"></iframe></noscript><div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div id="jmir-html"><span tabindex="-1"></span> <!----> <div id="skip-link"><a href="#main-content">
            Skip to Main Content <span aria-hidden="true" class="icon fas fa-chevron-down"></span></a> <a href="#footer">
            Skip to Footer <span aria-hidden="true" class="icon fas fa-chevron-down"></span></a></div> <!----> <div data-v-49c694ee><span tabindex="0" aria-label="Accessibility settings" title="Accessibility settings" role="button" class="icon fas fa-universal-access universal-access-btn" data-v-49c694ee></span> <!----> <style type="text/css" data-v-49c694ee>
            html {
            filter: none !important
            }
        
            html {
            font-weight: inherit !important
            }
        
            html {
            font-size: 0.625rem !important
            }
        
            html {
            text-align: initial !important
            }
         
            *:not svg {
            font-weight: inherit
            }
        </style></div> <div id="main-layout-container"><div style="background-color: white;"><div id="scroll-to-very-top"><header id="header"><section class="header"><nav><div data-v-575455fb><nav aria-label="Navigation" data-v-575455fb><section class="top-nav" data-v-575455fb><div class="container" data-v-575455fb><!----> <div class="corporate" data-v-575455fb><div class="corporate__logo" data-v-575455fb><a href="https://jmirpublications.com" aria-label="JMIR Publications main website" data-v-575455fb><div data-v-575455fb><img src="https://asset.jmir.pub/resources/images/logos/JMIR_logo.png" alt="JMIR Publications" class="logo-img"></div></a></div> <div class="corporate__mobile-menu" data-v-575455fb><button data-v-575455fb><span aria-hidden="true" class="icon fas fa-bars" data-v-575455fb></span></button></div> <div class="corporate__search" data-v-575455fb><label for="corporate__search-select" class="screen-readers-only" data-v-575455fb>
                            Select options
                        </label> <select id="corporate__search-select" data-test="search-select" class="corporate__search-select" data-v-575455fb><option value="articles" selected="selected" data-v-575455fb>
                                Articles
                            </option> <option value="help" data-v-575455fb>
                                Help
                            </option></select> <input name="type" type="hidden" value="text" data-v-575455fb> <!----><!----><!----><!----><!----> <button aria-label="Search articles" data-test="unisearch-button" class="btn btn-small btn-blue corporate__search-btn" data-v-575455fb><span aria-hidden="true" class="icon fas fa-search" data-v-575455fb></span></button></div> <div class="corporate__nav" data-v-575455fb><ul class="corporate__links" data-v-575455fb><li tabindex="0" aria-haspopup="true" data-test="resource-center" class="corporate__link-item" data-v-575455fb><a href="https://careers.jmir.org/" class="corporate__link" data-v-575455fb>
                                    Career Center
                                </a></li> <li class="corporate__link-item" data-v-575455fb><a id="button-login" href="javascript:;" data-test="login-button" class="corporate__link" data-v-575455fb>Login</a></li> <li id="button-register" data-test="register-button" class="corporate__link-item" data-v-575455fb><a class="corporate__link" data-v-575455fb>Register</a></li> <!----></ul></div></div></div></section> <section class="bottom-nav-1" data-v-575455fb><div class="container" data-v-575455fb><div class="journal" data-v-575455fb><div class="journal__nav" data-v-575455fb><ul class="journal__links" data-v-575455fb><li class="journal__link-item journal__link-item--journals" data-v-575455fb><div aria-haspopup="true" class="journal__link-item-container" data-v-575455fb><a href="/" aria-label="JMIR Mental Health home page" class="journal__link--home nuxt-link-active" data-v-575455fb><span aria-hidden="true" class="icon fas fa-home" style="font-size:16px;" data-v-575455fb></span>
                                        JMIR Mental Health
                                    </a> <span aria-expanded="false" aria-label="Other journals" tabindex="0" role="button" class="journal__journals-list" data-v-575455fb><span aria-hidden="true" data-test="journal-list-show-dropdown-button" class="icon fas fa-arrow-down" style="padding: 16px 16px;" data-v-575455fb></span></span></div> <ul aria-label="submenu" data-test="journal-list-dropdown" class="journal__link-submenu-journals" data-v-575455fb><li class="m-0" data-v-575455fb><a href="https://www.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Journal of Medical Internet Research</span> <span class="articles-number" data-v-575455fb>10756 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.researchprotocols.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Research Protocols</span> <span class="articles-number" data-v-575455fb>5264 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://formative.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Formative Research</span> <span class="articles-number" data-v-575455fb>4027 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://mhealth.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR mHealth and uHealth</span> <span class="articles-number" data-v-575455fb>2959 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://publichealth.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Public Health and Surveillance</span> <span class="articles-number" data-v-575455fb>1867 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://ojphi.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Online Journal of Public Health Informatics</span> <span class="articles-number" data-v-575455fb>1751 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://medinform.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Medical Informatics</span> <span class="articles-number" data-v-575455fb>1734 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="/" class="nuxt-link-active" data-v-575455fb><span data-v-575455fb>JMIR Mental Health</span> <span class="articles-number" data-v-575455fb>1227 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://humanfactors.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Human Factors</span> <span class="articles-number" data-v-575455fb>1078 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://games.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Serious Games</span> <span class="articles-number" data-v-575455fb>780 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://mededu.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Medical Education</span> <span class="articles-number" data-v-575455fb>740 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://aging.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Aging</span> <span class="articles-number" data-v-575455fb>634 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://xmed.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIRx Med</span> <span class="articles-number" data-v-575455fb>575 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://cancer.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Cancer</span> <span class="articles-number" data-v-575455fb>539 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://pediatrics.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Pediatrics and Parenting</span> <span class="articles-number" data-v-575455fb>530 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.i-jmr.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Interactive Journal of Medical Research</span> <span class="articles-number" data-v-575455fb>525 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.iproc.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>iProceedings</span> <span class="articles-number" data-v-575455fb>510 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://derma.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Dermatology</span> <span class="articles-number" data-v-575455fb>361 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://rehab.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Rehabilitation and Assistive Technologies</span> <span class="articles-number" data-v-575455fb>347 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://diabetes.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Diabetes</span> <span class="articles-number" data-v-575455fb>322 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://cardio.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Cardio</span> <span class="articles-number" data-v-575455fb>258 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://ai.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR AI</span> <span class="articles-number" data-v-575455fb>229 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://infodemiology.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Infodemiology</span> <span class="articles-number" data-v-575455fb>227 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://nursing.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Nursing</span> <span class="articles-number" data-v-575455fb>163 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://jopm.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Journal of Participatory Medicine</span> <span class="articles-number" data-v-575455fb>148 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://periop.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Perioperative Medicine</span> <span class="articles-number" data-v-575455fb>135 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://biomedeng.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Biomedical Engineering</span> <span class="articles-number" data-v-575455fb>106 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://bioinform.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Bioinformatics and Biotechnology</span> <span class="articles-number" data-v-575455fb>72 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://apinj.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Asian/Pacific Island Nursing Journal</span> <span class="articles-number" data-v-575455fb>49 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://xr.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR XR and Spatial Computing (JMXR)</span> <span class="articles-number" data-v-575455fb>40 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://xbio.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIRx Bio</span> <span class="articles-number" data-v-575455fb>39 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://neuro.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Neurotechnology</span> <span class="articles-number" data-v-575455fb>33 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.medicine20.com" no-prefetch="" data-v-575455fb><span data-v-575455fb>Medicine 2.0</span> <span class="articles-number" data-v-575455fb>26 articles
                                            </span></a></li><li class="m-0" data-v-575455fb><a href="https://data.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Data</span> <!----></a></li><li class="m-0" data-v-575455fb><a href="https://challenges.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Challenges</span> <!----></a></li><li class="m-0" data-v-575455fb><a href="https://preprints.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Preprints</span> <!----></a></li></ul></li> <li tabindex="0" aria-haspopup="true" data-test="journal-information" class="journal__link-item journal__link-item--journal-info" data-v-575455fb><span class="journal__link" data-v-575455fb>
                                    Journal Information
                                </span> <span aria-hidden="true" class="icon fas fa-caret-down" data-v-575455fb></span> <ul aria-label="submenu" data-test="journal-info-popover" class="journal__link-submenu journal__link-submenu--journal-info" data-v-575455fb><li data-v-575455fb><a href="/about-journal/focus-and-scope" data-v-575455fb>
                                            Focus and Scope
                                        </a></li><li data-v-575455fb><a href="/about-journal/editorial-board" data-v-575455fb>
                                            Editorial Board
                                        </a></li><li data-v-575455fb><a href="/author-information/instructions-for-authors" data-v-575455fb>
                                            Author Information
                                        </a></li><li data-v-575455fb><a href="/resource-centre/author-hub" data-v-575455fb>
                                            Resource Center
                                        </a></li><li data-v-575455fb><a href="/about-journal/article-processing-fees" data-v-575455fb>
                                            Article Processing Fees
                                        </a></li><li data-v-575455fb><a href="/publishing-policies/section-policies" data-v-575455fb>
                                            Publishing Policies
                                        </a></li><li data-v-575455fb><a href="/get-involved/new-journal-editor-in-chief-proposals" data-v-575455fb>
                                            Get Involved
                                        </a></li><li data-v-575455fb><a href="/top-articles/overview" data-v-575455fb>
                                            Top Articles
                                        </a></li><li data-v-575455fb><a href="/fees/institutional-partners" data-v-575455fb>
                                            Institutional Partners
                                        </a></li><li data-v-575455fb><a href="/about-journal/indexing-and-impact-factor" data-v-575455fb>
                                            Indexing and Impact Factor
                                        </a></li></ul></li> <li tabindex="0" aria-haspopup="true" class="journal__link-item journal__link-item--browse" data-v-575455fb><span class="journal__link" data-v-575455fb>
                                    Browse Journal
                                </span> <span aria-hidden="true" class="icon fas fa-caret-down" data-v-575455fb></span> <ul aria-label="submenu" data-test="browse-journal-popover" class="journal__link-submenu journal__link-submenu--browse" data-v-575455fb><li class="m-0" data-v-575455fb><div class="journal__link-submenu--select" data-v-575455fb><label for="nav-year" data-v-575455fb>
                                                Year:
                                            </label> <select id="nav-year" data-v-575455fb><option disabled="disabled" value="" data-v-575455fb>
                                                    Select...
                                                </option> <option value="2014" data-v-575455fb>
                                                    2014
                                                </option><option value="2015" data-v-575455fb>
                                                    2015
                                                </option><option value="2016" data-v-575455fb>
                                                    2016
                                                </option><option value="2017" data-v-575455fb>
                                                    2017
                                                </option><option value="2018" data-v-575455fb>
                                                    2018
                                                </option><option value="2019" data-v-575455fb>
                                                    2019
                                                </option><option value="2020" data-v-575455fb>
                                                    2020
                                                </option><option value="2021" data-v-575455fb>
                                                    2021
                                                </option><option value="2022" data-v-575455fb>
                                                    2022
                                                </option><option value="2023" data-v-575455fb>
                                                    2023
                                                </option><option value="2024" data-v-575455fb>
                                                    2024
                                                </option><option value="2025" data-v-575455fb>
                                                    2025
                                                </option></select></div></li> <li data-v-575455fb><a href="/announcements" data-v-575455fb>
                                            Latest Announcements
                                        </a></li><li data-v-575455fb><a href="/search/authors" data-v-575455fb>
                                            Authors
                                        </a></li> <li data-v-575455fb><a href="/themes" data-v-575455fb>
                                            Themes
                                        </a></li><li data-v-575455fb><a href="/issues" data-v-575455fb>
                                            Issues
                                        </a></li> <li data-v-575455fb><a href="https://blog.jmir.org/" data-v-575455fb>
                                            Blog
                                        </a></li></ul></li> <li class="journal__link-item journal__link-item--submit-article" data-v-575455fb><a href="/author" class="btn btn-small btn-blue journal__submit-article" data-v-575455fb>Submit Article</a></li></ul></div></div></div></section> <section class="bottom-nav-2" data-v-575455fb><div class="journal" data-v-575455fb><ul data-v-575455fb><li class="journal__link-item journal__link-item--journals" data-v-575455fb><div class="journal__link-item-container" data-v-575455fb><a href="/" class="journal__link--home nuxt-link-active" data-v-575455fb><span aria-hidden="true" class="icon fas fa-home" style="font-size:16px;" data-v-575455fb></span>
                                JMIR Mental Health
                            </a> <span class="journal__journals-list" data-v-575455fb><span aria-hidden="true" class="icon fas fa-arrow-down" data-v-575455fb></span></span></div> <ul data-test="journal-list" class="journal__link-submenu-journals" data-v-575455fb><li class="m-0" data-v-575455fb><a href="https://www.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Journal of Medical Internet Research</span> <span class="articles-number" data-v-575455fb>10756 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.researchprotocols.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Research Protocols</span> <span class="articles-number" data-v-575455fb>5264 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://formative.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Formative Research</span> <span class="articles-number" data-v-575455fb>4027 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://mhealth.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR mHealth and uHealth</span> <span class="articles-number" data-v-575455fb>2959 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://publichealth.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Public Health and Surveillance</span> <span class="articles-number" data-v-575455fb>1867 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://ojphi.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Online Journal of Public Health Informatics</span> <span class="articles-number" data-v-575455fb>1751 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://medinform.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Medical Informatics</span> <span class="articles-number" data-v-575455fb>1734 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="/" class="nuxt-link-active" data-v-575455fb><span data-v-575455fb>JMIR Mental Health</span> <span class="articles-number" data-v-575455fb>1227 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://humanfactors.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Human Factors</span> <span class="articles-number" data-v-575455fb>1078 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://games.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Serious Games</span> <span class="articles-number" data-v-575455fb>780 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://mededu.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Medical Education</span> <span class="articles-number" data-v-575455fb>740 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://aging.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Aging</span> <span class="articles-number" data-v-575455fb>634 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://xmed.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIRx Med</span> <span class="articles-number" data-v-575455fb>575 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://cancer.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Cancer</span> <span class="articles-number" data-v-575455fb>539 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://pediatrics.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Pediatrics and Parenting</span> <span class="articles-number" data-v-575455fb>530 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.i-jmr.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Interactive Journal of Medical Research</span> <span class="articles-number" data-v-575455fb>525 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.iproc.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>iProceedings</span> <span class="articles-number" data-v-575455fb>510 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://derma.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Dermatology</span> <span class="articles-number" data-v-575455fb>361 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://rehab.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Rehabilitation and Assistive Technologies</span> <span class="articles-number" data-v-575455fb>347 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://diabetes.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Diabetes</span> <span class="articles-number" data-v-575455fb>322 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://cardio.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Cardio</span> <span class="articles-number" data-v-575455fb>258 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://ai.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR AI</span> <span class="articles-number" data-v-575455fb>229 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://infodemiology.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Infodemiology</span> <span class="articles-number" data-v-575455fb>227 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://nursing.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Nursing</span> <span class="articles-number" data-v-575455fb>163 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://jopm.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Journal of Participatory Medicine</span> <span class="articles-number" data-v-575455fb>148 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://periop.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Perioperative Medicine</span> <span class="articles-number" data-v-575455fb>135 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://biomedeng.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Biomedical Engineering</span> <span class="articles-number" data-v-575455fb>106 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://bioinform.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Bioinformatics and Biotechnology</span> <span class="articles-number" data-v-575455fb>72 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://apinj.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>Asian/Pacific Island Nursing Journal</span> <span class="articles-number" data-v-575455fb>49 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://xr.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR XR and Spatial Computing (JMXR)</span> <span class="articles-number" data-v-575455fb>40 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://xbio.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIRx Bio</span> <span class="articles-number" data-v-575455fb>39 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://neuro.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Neurotechnology</span> <span class="articles-number" data-v-575455fb>33 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://www.medicine20.com" no-prefetch="" data-v-575455fb><span data-v-575455fb>Medicine 2.0</span> <span class="articles-number" data-v-575455fb>26 articles
                                    </span></a></li><li class="m-0" data-v-575455fb><a href="https://data.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Data</span> <!----></a></li><li class="m-0" data-v-575455fb><a href="https://challenges.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Challenges</span> <!----></a></li><li class="m-0" data-v-575455fb><a href="https://preprints.jmir.org" no-prefetch="" data-v-575455fb><span data-v-575455fb>JMIR Preprints</span> <!----></a></li></ul></li></ul></div></section> <!----></nav></div></nav></section></header></div> <div class="container-fluid" style="padding: 0px;"><div class="element-wrapper"><div data-test="main-content" class="container"><div class="sidebar-citation mobile-show"><div class="collection"><h2 tabindex="0" data-test="article-collection" class="h4 green-heading-underline width-fit-content">
                    This paper is in the following
                    <span class="collection__span">e-collection/theme issue:</span></h2> <a href="/themes/243" data-test="article-collection" aria-label="291 articles belongs to Reviews in Digital Mental Health e-collection/theme issue" class="collection__link">
                    Reviews in Digital Mental Health (291)
                </a><a href="/themes/232" data-test="article-collection" aria-label="562 articles belongs to Methods and New Tools in Mental Health Research e-collection/theme issue" class="collection__link">
                    Methods and New Tools in Mental Health Research (562)
                </a><a href="/themes/1660" data-test="article-collection" aria-label="31 articles belongs to AI-Powered Therapy Bots and Virtual Companions in Digital Mental Health e-collection/theme issue" class="collection__link">
                    AI-Powered Therapy Bots and Virtual Companions in Digital Mental Health (31)
                </a><a href="/themes/388" data-test="article-collection" aria-label="328 articles belongs to Diagnostic Tools in Mental Health e-collection/theme issue" class="collection__link">
                    Diagnostic Tools in Mental Health (328)
                </a><a href="/themes/253" data-test="article-collection" aria-label="530 articles belongs to Mobile Health in Psychiatry e-collection/theme issue" class="collection__link">
                    Mobile Health in Psychiatry (530)
                </a><a href="/themes/37" data-test="article-collection" aria-label="622 articles belongs to Ethics, Privacy, and Legal Issues e-collection/theme issue" class="collection__link">
                    Ethics, Privacy, and Legal Issues (622)
                </a><a href="/themes/1649" data-test="article-collection" aria-label="56 articles belongs to Responsible Health AI e-collection/theme issue" class="collection__link">
                    Responsible Health AI (56)
                </a><a href="/themes/1437" data-test="article-collection" aria-label="725 articles belongs to Generative Language Models Including ChatGPT e-collection/theme issue" class="collection__link">
                    Generative Language Models Including ChatGPT (725)
                </a><a href="/themes/64" data-test="article-collection" aria-label="2145 articles belongs to Digital Mental Health Interventions, e-Mental Health and Cyberpsychology e-collection/theme issue" class="collection__link">
                    Digital Mental Health Interventions, e-Mental Health and Cyberpsychology (2145)
                </a><a href="/themes/797" data-test="article-collection" aria-label="2162 articles belongs to Artificial Intelligence e-collection/theme issue" class="collection__link">
                    Artificial Intelligence (2162)
                </a></div></div> <div class="row"><div class="main col-lg-9 mb-1"><!----> <div data-test="details" class="details"><div><p id="main-content" tabindex="0">
                            Published on
                            <time datetime="27.Jun.2025">27.Jun.2025
                            </time>
                            in
                            <span data-test="issue-info"><a href="/2025/1" class="nuxt-link-active">
                                    Vol 12<!----> (2025)<!----></a></span></p> <!----></div> <div><div class="preprints-version"><span aria-hidden="true" class="icon fas fa-thumbtack"></span> <div><span class="ml-2">
                                    Preprints (earlier versions) of this paper are
                                    available at
                                    <a data-test="preprint-link" aria-label="'Preprints (earlier versions) of this paper are
                                available at preprints.jmir.org/preprint/'70610" href="https://preprints.jmir.org/preprint/70610" target="_blank">https://preprints.jmir.org/preprint/70610</a>, first published
                                    <time datetime="27.Dec.2024">27.Dec.2024</time>.
                                </span></div></div></div> <!----></div> <div class="info mt-3"><div class="info__article-img"><div data-v-10f10a3e><img data-srcset="https://asset.jmir.pub/assets/dae990f3af4bcdffe2910e552301457e.png 480w,https://asset.jmir.pub/assets/dae990f3af4bcdffe2910e552301457e.png 960w,https://asset.jmir.pub/assets/dae990f3af4bcdffe2910e552301457e.png 1920w,https://asset.jmir.pub/assets/dae990f3af4bcdffe2910e552301457e.png 2500w" alt="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review" title="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review" aria-label="Article Thumbnail Image" src="https://asset.jmir.pub/placeholder.svg" data-v-10f10a3e></div> <div data-test="article-img-info" class="info__article-img-info"><span aria-hidden="true" class="icon fas fa-search-plus"></span></div></div> <div class="info__title-authors"><h1 tabindex="0" aria-label="The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review" class="h3 mb-0 mt-0">The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review</h1> <h2 class="info__hidden-title">
                            The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review
                        </h2> <div class="mt-3"><p tabindex="0" class="authors-for-screen-reader">
                                Authors of this article:
                            </p> <span data-test="authors-info" class="info__authors"><span><a href="/search?term=Xi%20Wang&amp;type=author&amp;precise=true&amp;authorlink=true" aria-label="Xi Wang. Search more articles by this author.">
                                        Xi Wang<sup>1</sup> <!----></a></span> <span><a aria-label="Visit this author on ORCID website" data-test="orcid-link" target="_blank" href="https://orcid.org/0009-0004-1090-6209"><img src="https://asset.jmir.pub/assets/static/images/Orcid-ID-Logo-Colour.png" alt="Author Orcid Image" aria-label="Author Orcid Image" class="info__orcid-img"></a></span> <span style="margin-left: -2px;">
                                    ;  
                                </span></span><span data-test="authors-info" class="info__authors"><span><a href="/search?term=Yujia%20Zhou&amp;type=author&amp;precise=true&amp;authorlink=true" aria-label="Yujia Zhou. Search more articles by this author.">
                                        Yujia Zhou<sup>2</sup> <!----></a></span> <span><a aria-label="Visit this author on ORCID website" data-test="orcid-link" target="_blank" href="https://orcid.org/0000-0002-3530-3787"><img src="https://asset.jmir.pub/assets/static/images/Orcid-ID-Logo-Colour.png" alt="Author Orcid Image" aria-label="Author Orcid Image" class="info__orcid-img"></a></span> <span style="margin-left: -2px;">
                                    ;  
                                </span></span><span data-test="authors-info" class="info__authors"><span><a href="/search?term=Guangyu%20Zhou&amp;type=author&amp;precise=true&amp;authorlink=true" aria-label="Guangyu Zhou. Search more articles by this author.">
                                        Guangyu Zhou<sup>1</sup> <!----></a></span> <span><a aria-label="Visit this author on ORCID website" data-test="orcid-link" target="_blank" href="https://orcid.org/0000-0003-2053-6737"><img src="https://asset.jmir.pub/assets/static/images/Orcid-ID-Logo-Colour.png" alt="Author Orcid Image" aria-label="Author Orcid Image" class="info__orcid-img"></a></span> <!----></span></div> <!----></div></div> <div role="tablist" aria-label="Article" class="tabs"><a href="/2025/1/e70610/" aria-current="page" role="tab" aria-label="Article" data-test="tabs" class="nuxt-link-exact-active nuxt-link-active active">
                        Article
                    </a><a href="/2025/1/e70610/authors" role="tab" aria-label="Authors" data-test="tabs">
                        Authors
                    </a><a href="/2025/1/e70610/citations" role="tab" aria-label="Cited by (5)" data-test="tabs">
                        Cited by (5)
                    </a><a href="/2025/1/e70610/tweetations" role="tab" aria-label="Tweetations (10)" data-test="tabs">
                        Tweetations (10)
                    </a><a href="/2025/1/e70610/metrics" role="tab" aria-label="Metrics" data-test="tabs">
                        Metrics
                    </a></div> <div class="container"><div class="row"><div class="col-lg-3 mb-5 sidebar-sections"><div class="sidebar-nav"><div class="sidebar-nav-sticky"><ul></ul></div></div></div> <div data-test="keyword-links" class="col-lg-9 article"><main id="wrapper" class="wrapper ArticleMain clearfix"><section class="inner-wrapper clearfix"><section class="main-article-content clearfix"><article class="ajax-article-content"><h4 class="h4-original-paper"><span class="typcn typcn-document-text"></span>Review</h4><div class="authors-container"><div class="authors clearfix"></div></div><div class="authors-container"><div class="authors clearfix"></div></div><div class="authors-container"><div class="authors clearfix"><ul class="clearfix"><li><a href="/search/searchResult?field%5B%5D=author&amp;criteria%5B%5D=Xi+Wang" class="btn-view-author-options">Xi Wang<sup><small>1</small></sup>, MA</a><a class="author-orcid" href="https://orcid.org/0009-0004-1090-6209" target="_blank" title="ORCID">&#xA0;</a>;&#xA0;</li><li><a href="/search/searchResult?field%5B%5D=author&amp;criteria%5B%5D=Yujia+Zhou" class="btn-view-author-options">Yujia Zhou<sup><small>2</small></sup>, PhD</a><a class="author-orcid" href="https://orcid.org/0000-0002-3530-3787" target="_blank" title="ORCID">&#xA0;</a>;&#xA0;</li><li><a href="/search/searchResult?field%5B%5D=author&amp;criteria%5B%5D=Guangyu+Zhou" class="btn-view-author-options">Guangyu Zhou<sup><small>1</small></sup>, Prof Dr</a><a class="author-orcid" href="https://orcid.org/0000-0003-2053-6737" target="_blank" title="ORCID">&#xA0;</a></li></ul><div class="author-affiliation-details"><p><sup>1</sup>School of Psychological and Cognitive Sciences, Beijing Key Laboratory of Behavior and Mental Health, Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China</p><p><sup>2</sup>Department of Computer Science and Technology, Tsinghua University, Beijing, China</p></div></div><div class="corresponding-author-and-affiliations clearfix"><div class="corresponding-author-details"><h3>Corresponding Author:</h3><p>Guangyu Zhou, Prof Dr</p><p></p><p>School of Psychological and Cognitive Sciences</p><p>Beijing Key Laboratory of Behavior and Mental Health, Key Laboratory of Machine Perception (Ministry of Education)</p><p>Peking University</p><p>Philosophy Building, 2nd Fl.</p><p>No. 5 Yiheyuan Road, Haidian District</p><p>Beijing, 100871</p><p>China</p><p>Phone: 86 10 62767702</p><p>Email: <a href="mailto:gyzhou@pku.edu.cn">gyzhou@pku.edu.cn</a></p><br></div></div></div><section class="article-content clearfix"><article class="abstract"><h3 id="Abstract" class="navigation-heading" data-label="Abstract">Abstract</h3><p><span class="abstract-sub-heading">Background: </span>Mental health disorders affect an estimated 1 in 8 individuals globally, yet traditional interventions often face barriers, such as limited accessibility, high costs, and persistent stigma. Recent advancements in generative artificial intelligence (GenAI) have introduced AI systems capable of understanding and producing humanlike language in real time. These developments present new opportunities to enhance mental health care.<br></p><p><span class="abstract-sub-heading">Objective: </span>We aimed to systematically examine the current applications of GenAI in mental health, focusing on 3 core domains: diagnosis and assessment, therapeutic tools, and clinician support. In addition, we identified and synthesized key ethical issues reported in the literature.<br></p><p><span class="abstract-sub-heading">Methods: </span>We conducted a comprehensive literature search, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines, in PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar databases to identify peer-reviewed studies published from October 1, 2019, to September 30, 2024. After screening 783 records, 79 (10.1%) studies met the inclusion criteria.<br></p><p><span class="abstract-sub-heading">Results: </span>The number of studies on GenAI applications in mental health has grown substantially since 2023. Studies on diagnosis and assessment (37/79, 47%) primarily used GenAI models to detect depression and suicidality through text data. Studies on therapeutic applications (20/79, 25%) investigated GenAI-based chatbots and adaptive systems for emotional and behavioral support, reporting promising outcomes but revealing limited real-world deployment and safety assurance. Clinician support studies (24/79, 30%) explored GenAI&#x2019;s role in clinical decision-making, documentation and summarization, therapy support, training and simulation, and psychoeducation. Ethical concerns were consistently reported across the domains. On the basis of these findings, we proposed an integrative ethical framework, GenAI4MH, comprising 4 core dimensions&#x2014;data privacy and security, information integrity and fairness, user safety, and ethical governance and oversight&#x2014;to guide the responsible use of GenAI in mental health contexts.<br></p><p><span class="abstract-sub-heading">Conclusions: </span>GenAI shows promise in addressing the escalating global demand for mental health services. They may augment traditional approaches by enhancing diagnostic accuracy, offering more accessible support, and reducing clinicians&#x2019; administrative burden. However, to ensure ethical and effective implementation, comprehensive safeguards&#x2014;particularly around privacy, algorithmic bias, and responsible user engagement&#x2014;must be established.<br></p><strong class="h4-article-volume-issue">JMIR Ment Health 2025;12:e70610</strong><br><br><span class="article-doi"><a href="https://doi.org/10.2196/70610">doi:10.2196/70610</a></span><br><br><h3 class="h3-main-heading" id="Keywords">Keywords</h3><div class="keywords"><span><a href="/search?type=keyword&amp;term=generative%20AI&amp;precise=true">generative AI</a>;&#xA0;</span><span><a href="/search?type=keyword&amp;term=mental%20health&amp;precise=true">mental health</a>;&#xA0;</span><span><a href="/search?type=keyword&amp;term=large%20language%20models&amp;precise=true">large language models</a>;&#xA0;</span><span><a href="/search?type=keyword&amp;term=mental%20health%20detection%20and%20diagnosis&amp;precise=true">mental health detection and diagnosis</a>;&#xA0;</span><span><a href="/search?type=keyword&amp;term=therapeutic%20chatbots&amp;precise=true">therapeutic chatbots</a>&#xA0;</span></div><div id="trendmd-suggestions"></div></article><br><article class="main-article clearfix"><br><h3 class="navigation-heading h3-main-heading" id="Introduction" data-label="Introduction">Introduction</h3><h4>Background</h4><p class="abstract-paragraph">Mental health has become a global public health priority, with increasing recognition of its importance for individual well-being, societal stability, and economic productivity. According to the World Health Organization, approximately 1 in 8 people worldwide live with a mental health disorder [<span class="footers"><a class="citation-link" href="#ref1" rel="footnote">1</a></span>]. Despite the growing demand for mental health services, traditional approaches such as in-person therapy and medication, which rely heavily on trained professionals and extensive infrastructure, are struggling to meet the rising need [<span class="footers"><a class="citation-link" href="#ref2" rel="footnote">2</a></span>]. Consequently, an alarming 76% to 85% of individuals with mental health disorders do not receive effective treatment, often due to barriers such as limited access to mental health professionals, social stigma, and inadequate health care systems [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>]. Against this backdrop, advances in generative artificial intelligence (GenAI) offer new and promising avenues to enhance mental health services.</p><p class="abstract-paragraph">GenAI, such as ChatGPT [<span class="footers"><a class="citation-link" href="#ref4" rel="footnote">4</a></span>], is built on large-scale language modeling and trained on extensive textual corpora. Their capacity to produce contextually relevant and, in many cases, emotionally appropriate language [<span class="footers"><a class="citation-link" href="#ref5" rel="footnote">5</a></span>,<span class="footers"><a class="citation-link" href="#ref6" rel="footnote">6</a></span>] enables more natural and adaptive interactions. Compared to earlier dialogue systems, GenAI exhibits greater flexibility in producing open-ended, humanlike dialogue [<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>]. This generative capability makes them a promising tool for web-based therapeutic interventions that allow for real-time, adaptive engagement in mental health care.</p><p class="abstract-paragraph">Currently, GenAI is being integrated into mental health through a range of innovative applications. For instance, GPT-driven chatbots such as Well-Mind ChatGPT [<span class="footers"><a class="citation-link" href="#ref8" rel="footnote">8</a></span>] and MindShift [<span class="footers"><a class="citation-link" href="#ref9" rel="footnote">9</a></span>] provide personalized mental health support by engaging users in conversational therapy. Similarly, virtual companions such as Replika [<span class="footers"><a class="citation-link" href="#ref10" rel="footnote">10</a></span>] are used to help users manage feelings of loneliness and anxiety through interactive dialogue [<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>]. In addition, GenAI has been used to analyze social media posts and clinical data to identify signs of depression [<span class="footers"><a class="citation-link" href="#ref2" rel="footnote">2</a></span>] and suicidal ideation [<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>]. These diverse applications illustrate the potential of GenAI to address various mental health needs, from prevention and assessment to continuous support and intervention.</p><p class="abstract-paragraph">Although research has investigated various applications of GenAI in mental health, much of it has focused on specific models or isolated cases, lacking a comprehensive evaluation of its broader impacts, applications, and associated risks. Similarly, most systematic reviews to date have focused on particular domains, such as depression detection [<span class="footers"><a class="citation-link" href="#ref13" rel="footnote">13</a></span>], chatbot interventions [<span class="footers"><a class="citation-link" href="#ref14" rel="footnote">14</a></span>], empathic communication [<span class="footers"><a class="citation-link" href="#ref5" rel="footnote">5</a></span>], psychiatric education [<span class="footers"><a class="citation-link" href="#ref15" rel="footnote">15</a></span>], and AI-based art therapy [<span class="footers"><a class="citation-link" href="#ref16" rel="footnote">16</a></span>]. While such focused reviews offer valuable insights into specific use cases, a broad outline remains crucial for understanding overarching trends, identifying research gaps, and informing the responsible development of GenAI in mental health. To date, only 2 reviews [<span class="footers"><a class="citation-link" href="#ref17" rel="footnote">17</a></span>,<span class="footers"><a class="citation-link" href="#ref18" rel="footnote">18</a></span>] have attempted broader overviews, covering the literature published before April 2024 and July 2023, respectively. However, since April 2024, the rapid evolution of GenAI&#x2014;including the release and deployment of more advanced models, such as GPT-4o [<span class="footers"><a class="citation-link" href="#ref19" rel="footnote">19</a></span>] and GPT-o1 [<span class="footers"><a class="citation-link" href="#ref20" rel="footnote">20</a></span>], and their increasing integration with clinical workflows, such as Med-Gemini [<span class="footers"><a class="citation-link" href="#ref21" rel="footnote">21</a></span>], has expanded the scope and complexity of GenAI applications in real-world mental health contexts. These developments underscore the need for a more updated and integrative synthesis.</p><h4>Objectives</h4><p class="abstract-paragraph">To address this gap, we aimed to provide a comprehensive overview of GenAI applications in mental health, identify research gaps, and propose future directions. To systematically categorize the existing research, we divided the studies into three distinct categories based on the role of GenAI in mental health applications, as illustrated in <span class="footers"><a class="citation-link" href="#figure1" rel="footnote">Figure 1</a></span>: (1) GenAI for mental health diagnosis and assessment, encompassing research that leverages GenAI to detect, classify, or evaluate mental health conditions; (2) GenAI as therapeutic tools, covering studies where GenAI-based chatbots or conversational agents are used to deliver mental health support, therapy, or interventions directly to users; and (3) GenAI for supporting clinicians and mental health professionals, including research aimed at using GenAI to assist clinicians in their practice.</p><p class="abstract-paragraph">Despite these promising applications, the integration of GenAI into mental health care is not without challenges. Applying GenAI in the mental health field involves processing highly sensitive personal information, such as users&#x2019; emotional states, psychological histories, and behavioral patterns. Mishandling such data not only poses privacy risks but may also lead to psychological harm, including distress, stigma, or reduced trust in mental health services [<span class="footers"><a class="citation-link" href="#ref22" rel="footnote">22</a></span>]. Therefore, in addition to systematically categorizing existing applications of GenAI in mental health, we also examined ethical issues related to their use in this domain. On the basis of our analysis, we proposed an ethical framework, GenAI4MH, to guide the responsible use of GenAI in mental health contexts (<span class="footers"><a class="citation-link" href="#figure2" rel="footnote">Figure 2</a></span>).</p><figure><a name="figure1">&#x200E;</a><a class="fancybox" title="Figure 1. Classification of generative artificial intelligence (GenAI) applications in mental health." href="https://asset.jmir.pub/assets/17bd6af3116f56a39f6b8bebcee2a881.png" id="figure1"><img class="figure-image" src="https://asset.jmir.pub/assets/17bd6af3116f56a39f6b8bebcee2a881.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 1. </b> Classification of generative artificial intelligence (GenAI) applications in mental health. </figcaption></figure><figure><a name="figure2">&#x200E;</a><a class="fancybox" title="Figure 2. Overview of the GenAI4MH ethical framework for the responsible use of generative artificial intelligence (GenAI) in mental health. GLM: generative language model." href="https://asset.jmir.pub/assets/48f15915a12bd696a5d815ea89b02acf.png" id="figure2"><img class="figure-image" src="https://asset.jmir.pub/assets/48f15915a12bd696a5d815ea89b02acf.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 2. </b> Overview of the GenAI4MH ethical framework for the responsible use of generative artificial intelligence (GenAI) in mental health. GLM: generative language model. </figcaption></figure><br><h3 class="navigation-heading h3-main-heading" id="Methods" data-label="Methods">Methods</h3><h4>Search Strategy</h4><p class="abstract-paragraph">We conducted this systematic review following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines (<span class="footers"><a class="citation-link" href="#app1" rel="footnote">Multimedia Appendix 1</a></span>) [<span class="footers"><a class="citation-link" href="#ref23" rel="footnote">23</a></span>]. We conducted a comprehensive search across 6 databases: PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar. We conducted the search between October 1, 2024, and October 7, 2024, and targeted studies published from October 1, 2019, to September 30, 2024. The starting date was chosen to coincide with the introduction of the T5 model [<span class="footers"><a class="citation-link" href="#ref24" rel="footnote">24</a></span>], a foundational development for many of today&#x2019;s mainstream GenAI models. This date also intentionally excluded earlier models, such as Bidirectional Encoder Representations from Transformers (BERT) [<span class="footers"><a class="citation-link" href="#ref25" rel="footnote">25</a></span>] and GPT-2 [<span class="footers"><a class="citation-link" href="#ref26" rel="footnote">26</a></span>], as these models have already been extensively covered in the previous literature [<span class="footers"><a class="citation-link" href="#ref27" rel="footnote">27</a></span>,<span class="footers"><a class="citation-link" href="#ref28" rel="footnote">28</a></span>], and our aim was to highlight more recent innovations.</p><p class="abstract-paragraph">Search terms were constructed using a logical combination of keywords related to GenAI and mental health: (Generative AI OR Large Language Model OR ChatGPT) AND (mental health OR mental disorder OR depression OR anxiety). This search string was developed based on previous reviews and refined through iterative testing to ensure effective identification of relevant studies. When possible, the search was restricted to titles and abstracts. For Google Scholar, the first 10 pages of results were screened for relevance. A detailed search strategy is provided in <span class="footers"><a class="citation-link" href="#app2" rel="footnote">Multimedia Appendix 2</a></span>.</p><h4>Study Selection</h4><p class="abstract-paragraph">The selection criteria included studies that (1) used GenAI and were published after the introduction of the T5 [<span class="footers"><a class="citation-link" href="#ref24" rel="footnote">24</a></span>] model and (2) directly addressed the application of GenAI in mental health care settings. Only peer-reviewed original research articles were considered, with no language restrictions.</p><h4>Data Extraction</h4><p class="abstract-paragraph">Data from the included studies were extracted using standardized frameworks. For qualitative studies, we used the Sample, Phenomenon of Interest, Design, Evaluation, and Research Type (SPIDER) framework. For quantitative studies, we applied the Population, Intervention, Comparison, Outcome, and Study (PICOS) framework. A summary of the extracted data are provided in <span class="footers"><a class="citation-link" href="#app3" rel="footnote">Multimedia Appendix 3</a></span> [<span class="footers"><a class="citation-link" href="#ref2" rel="footnote">2</a></span>,<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>-<span class="footers"><a class="citation-link" href="#ref9" rel="footnote">9</a></span>,<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>,<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>,<span class="footers"><a class="citation-link" href="#ref29" rel="footnote">29</a></span>-<span class="footers"><a class="citation-link" href="#ref100" rel="footnote">100</a></span>].</p><h4>Reporting Quality Assessment</h4><p class="abstract-paragraph">To assess the reporting transparency and the methodological rigor of the included studies, we applied the Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research (MI-CLAIM-GEN) checklist (<span class="footers"><a class="citation-link" href="#app4" rel="footnote">Multimedia Appendix 4</a></span>) [<span class="footers"><a class="citation-link" href="#ref101" rel="footnote">101</a></span>], a recently proposed guideline tailored for evaluating the reporting quality of research on GenAI in health care. The checklist covers essential aspects such as study design, data and resource transparency, model evaluation strategies, bias and harm assessments, and reproducibility. We followed the Joanna Briggs Institute quality appraisal format [<span class="footers"><a class="citation-link" href="#ref102" rel="footnote">102</a></span>] to score each item in the checklist using 4 categories: yes, no, unclear, and not applicable.</p><br><h3 class="navigation-heading h3-main-heading" id="Results" data-label="Results">Results</h3><h4>Study Selection</h4><p class="abstract-paragraph">As shown in <span class="footers"><a class="citation-link" href="#figure3" rel="footnote">Figure 3</a></span>, a total of 783 records were initially retrieved from the 6 databases. After removing duplicates, 73.8% (578/783) of unique records remained for screening. Following abstract screening, 39.4% (228/578) of the records were identified for full-text retrieval and screening. After full-text screening, 24% (55/228) of the articles were selected for inclusion in the systematic review. To ensure comprehensive coverage of relevant studies, a snowballing technique was then applied, where we examined the reference lists of the included studies and related review articles. This process identified an additional 44 studies for eligibility assessment. After the same evaluation process, 54% (24/44) of these studies met the inclusion criteria, bringing the final total to 79 studies for the systematic review. Two PhD candidates (YZ and XW) independently conducted the selection, with discrepancies resolved through discussion. The interrater reliability was satisfactory (&#x3BA;=0.904).</p><figure><a name="figure3">&#x200E;</a><a class="fancybox" title="Figure 3. The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram of study selection." href="https://asset.jmir.pub/assets/fcacc9d94d512814fd41c551509df627.png" id="figure3"><img class="figure-image" src="https://asset.jmir.pub/assets/fcacc9d94d512814fd41c551509df627.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 3. </b> The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram of study selection. </figcaption></figure><h4>Publication Trends Over Time</h4><p class="abstract-paragraph">An analysis of publication trends over time reveals a growing focus on the application of GenAI in mental health (<span class="footers"><a class="citation-link" href="#figure4" rel="footnote">Figure 4</a></span>). Overall, the number of studies in all the 3 categories grew extensively over the examined period, indicating a rising interest in using GenAI for mental health. In 2022, the total number of studies was minimal across all the 3 categories, with only 1 (1%) early study, of the included 79 studies, emerging on the use of GenAI for mental health diagnosis and assessment. However, as GenAI advanced and garnered wider adoption, the number of publications in all the 3 categories began to increase steadily. A moderate increase was observed in the year 2023, with 13% (10/79) of the studies focused on diagnosis and assessment, 9% (7/79) on therapeutic interventions, and 8% (6/79) on clinician support, reflecting a growing interest in practical applications of these models in health care settings. By 2024, the number of publications had surged across all the 3 categories, with 33% (26/79) of the studies focused on diagnosis and assessment, 16% (13/79) on therapeutic interventions, and 23% (18/79) on clinician support.</p><figure><a name="figure4">&#x200E;</a><a class="fancybox" title="Figure 4. Publication trends in the application of generative artificial intelligence (GenAI) in mental health research." href="https://asset.jmir.pub/assets/43ad7e7f697395bfd980c68115858621.png" id="figure4"><img class="figure-image" src="https://asset.jmir.pub/assets/43ad7e7f697395bfd980c68115858621.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 4. </b> Publication trends in the application of generative artificial intelligence (GenAI) in mental health research. </figcaption></figure><h4>GenAI for Mental Health Diagnosis and Assessment</h4><h5>Overview</h5><p class="abstract-paragraph">Of the 79 included studies, 37 (47%) were identified that investigated the effectiveness and applications of GenAI in mental health diagnosis and assessment. These studies primarily explored how GenAI can detect and interpret mental health conditions by analyzing textual and multimodal data. A summary of the included studies is presented in <span class="footers"><a class="citation-link" href="#app5" rel="footnote">Multimedia Appendix 5</a></span> [<span class="footers"><a class="citation-link" href="#ref2" rel="footnote">2</a></span>,<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>,<span class="footers"><a class="citation-link" href="#ref29" rel="footnote">29</a></span>-<span class="footers"><a class="citation-link" href="#ref59" rel="footnote">59</a></span>,<span class="footers"><a class="citation-link" href="#ref61" rel="footnote">61</a></span>,<span class="footers"><a class="citation-link" href="#ref62" rel="footnote">62</a></span>,<span class="footers"><a class="citation-link" href="#ref100" rel="footnote">100</a></span>].</p><h5>Mental Health Issues</h5><p class="abstract-paragraph">The existing studies using GenAI for mental health diagnosis predominantly focused on suicide risk and depression, followed by emerging applications in emotion recognition, psychiatric disorders, and stress.</p><p class="abstract-paragraph">Suicide risk was the most frequently examined topic, addressed in 40% (15/37) of the studies. Researchers used large language models (LLMs) to identify suicide-related linguistic patterns [<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>], extract and synthesize textual evidence supporting identified suicide risk levels [<span class="footers"><a class="citation-link" href="#ref29" rel="footnote">29</a></span>-<span class="footers"><a class="citation-link" href="#ref33" rel="footnote">33</a></span>,<span class="footers"><a class="citation-link" href="#ref103" rel="footnote">103</a></span>], and evaluate suicide risk [<span class="footers"><a class="citation-link" href="#ref34" rel="footnote">34</a></span>-<span class="footers"><a class="citation-link" href="#ref41" rel="footnote">41</a></span>]. GenAI models, such as GPT-4 [<span class="footers"><a class="citation-link" href="#ref104" rel="footnote">104</a></span>], achieved high precision (up to 0.96) in predicting suicidal risk levels [<span class="footers"><a class="citation-link" href="#ref30" rel="footnote">30</a></span>-<span class="footers"><a class="citation-link" href="#ref33" rel="footnote">33</a></span>,<span class="footers"><a class="citation-link" href="#ref103" rel="footnote">103</a></span>], outperforming traditional models, such as support vector machines (SVM) [<span class="footers"><a class="citation-link" href="#ref41" rel="footnote">41</a></span>], and performing comparably to or better than pretrained language models, such as BERT [<span class="footers"><a class="citation-link" href="#ref38" rel="footnote">38</a></span>,<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>]. Most studies (13/15, 87%) relied on simulated case narratives [<span class="footers"><a class="citation-link" href="#ref34" rel="footnote">34</a></span>,<span class="footers"><a class="citation-link" href="#ref36" rel="footnote">36</a></span>,<span class="footers"><a class="citation-link" href="#ref37" rel="footnote">37</a></span>] or social media data [<span class="footers"><a class="citation-link" href="#ref38" rel="footnote">38</a></span>-<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>]; only 13% (2/15) of the studies used real clinical narratives [<span class="footers"><a class="citation-link" href="#ref35" rel="footnote">35</a></span>,<span class="footers"><a class="citation-link" href="#ref41" rel="footnote">41</a></span>].</p><p class="abstract-paragraph">Depression was the second most common mental health issue addressed, featured in 35% (13/37) of the studies. While GenAI models showed promising accuracy (eg, 0.902 using semistructured diaries [<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>]), performance was often constrained to English data [<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>,<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>], with notable drop-offs in dialectal or culturally divergent contexts [<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>]. Multimodal approaches&#x2014;integrating audio, visual, and physiological data&#x2014;improved detection reliability over text-only methods [<span class="footers"><a class="citation-link" href="#ref45" rel="footnote">45</a></span>-<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>]. Several studies (3/13, 23%) also explored interpretability, using GenAI to generate explanations [<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>] or conduct structured assessments [<span class="footers"><a class="citation-link" href="#ref48" rel="footnote">48</a></span>].</p><p class="abstract-paragraph">GenAI has also been explored for emotion recognition, using smartphone and wearable data to predict affective states with moderate accuracy [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>,<span class="footers"><a class="citation-link" href="#ref49" rel="footnote">49</a></span>], and enabling novel assessment formats, such as virtual agent interactions [<span class="footers"><a class="citation-link" href="#ref45" rel="footnote">45</a></span>] and conversational psychological scales [<span class="footers"><a class="citation-link" href="#ref50" rel="footnote">50</a></span>]. The studies also explored other psychiatric disorders, such as obsessive-compulsive disorder (accuracy up to 96.1%) [<span class="footers"><a class="citation-link" href="#ref51" rel="footnote">51</a></span>] and schizophrenia (<i>r</i>=0.66-0.69 with expert ratings) [<span class="footers"><a class="citation-link" href="#ref52" rel="footnote">52</a></span>]. In total, 8% (3/37) of the studies addressed stress detection from social media texts [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>,<span class="footers"><a class="citation-link" href="#ref53" rel="footnote">53</a></span>,<span class="footers"><a class="citation-link" href="#ref54" rel="footnote">54</a></span>].</p><p class="abstract-paragraph">A smaller set of studies (3/37, 8%) assessed GenAI models&#x2019; capacity for differential diagnosis, demonstrating that GenAI models could distinguish among multiple mental disorders in controlled simulations [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref55" rel="footnote">55</a></span>,<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>]. However, performance remained higher for mental health conditions with distinct symptoms (eg, psychosis and anxiety) and lower for overlapping or less prevalent disorders (eg, perinatal depression and lysergic acid diethylamide use disorder) [<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>], particularly for those with symptom overlap with more common mental health conditions (eg, disruptive mood dysregulation disorder and acute stress disorder) [<span class="footers"><a class="citation-link" href="#ref55" rel="footnote">55</a></span>].</p><h5>Model Architectures and Adaptation Strategies</h5><h6>Overview</h6><p class="abstract-paragraph">Most included studies (29/37, 78%) used proprietary GenAI models for mental health diagnosis and assessment, with GPT-based models (GPT-3, 3.5, and 4) [<span class="footers"><a class="citation-link" href="#ref4" rel="footnote">4</a></span>] being the most commonly used [<span class="footers"><a class="citation-link" href="#ref2" rel="footnote">2</a></span>,<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>]. Other proprietary models included Gemini [<span class="footers"><a class="citation-link" href="#ref49" rel="footnote">49</a></span>,<span class="footers"><a class="citation-link" href="#ref51" rel="footnote">51</a></span>] and the pathways language model (version 2) [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>]. A smaller subset of the studies (14/37, 38%) adopted open-source models, such as LLM Meta AI (LLaMA) [<span class="footers"><a class="citation-link" href="#ref29" rel="footnote">29</a></span>,<span class="footers"><a class="citation-link" href="#ref30" rel="footnote">30</a></span>,<span class="footers"><a class="citation-link" href="#ref32" rel="footnote">32</a></span>,<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>,<span class="footers"><a class="citation-link" href="#ref52" rel="footnote">52</a></span>,<span class="footers"><a class="citation-link" href="#ref57" rel="footnote">57</a></span>,<span class="footers"><a class="citation-link" href="#ref58" rel="footnote">58</a></span>], Mistral [<span class="footers"><a class="citation-link" href="#ref33" rel="footnote">33</a></span>], Falcon [<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>], and Neomotron [<span class="footers"><a class="citation-link" href="#ref55" rel="footnote">55</a></span>]. Beyond model selection, several studies (29/37, 78%) explored technical strategies to enhance diagnostic performance and interpretability. In total, 3 main approaches were identified as described in subsequent sections.</p><h6>Hybrid Modeling</h6><p class="abstract-paragraph">A limited number of studies (2/37, 5%) explored hybrid architectures, combining GenAI-generated embeddings with classical classifiers, such as SVM or random forest [<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>,<span class="footers"><a class="citation-link" href="#ref53" rel="footnote">53</a></span>]. For example, Radwan et al [<span class="footers"><a class="citation-link" href="#ref53" rel="footnote">53</a></span>] used GPT-3 embeddings to generate text vectors, which were input into classifiers, such as SVM, random forest, and k-nearest neighbors, for stress level classification. The combination of GPT-3 embeddings with an SVM classifier yielded the best performance, outperforming other hybrid configurations and traditional models such as BERT with the long short-term memory model.</p><h6>Fine-Tuning and Instruction Adaptation</h6><p class="abstract-paragraph">Some studies (4/37, 11%) used instruction-tuned models, including Flan [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>,<span class="footers"><a class="citation-link" href="#ref41" rel="footnote">41</a></span>], Alpaca [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>], and Wizard [<span class="footers"><a class="citation-link" href="#ref32" rel="footnote">32</a></span>], to enhance instruction following. Further fine-tuning with mental health&#x2013;related data was also applied to improve diagnostic and assessment capabilities [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>,<span class="footers"><a class="citation-link" href="#ref46" rel="footnote">46</a></span>,<span class="footers"><a class="citation-link" href="#ref59" rel="footnote">59</a></span>]. For instance, Xu et al [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>] demonstrated that their fine-tuned models&#x2014;Mental-Alpaca and Mental-FLAN-T5&#x2014;achieved a 10.9% improvement in balanced accuracy over GPT-3.5 [<span class="footers"><a class="citation-link" href="#ref4" rel="footnote">4</a></span>], despite being 25 and 15 times smaller, respectively. These models also outperformed GPT-4 [<span class="footers"><a class="citation-link" href="#ref104" rel="footnote">104</a></span>] by 4.8%, although GPT-4 is 250 and 150 times larger, respectively.</p><h6>Prompt Engineering and Knowledge Augmentation</h6><p class="abstract-paragraph">Prompt-based techniques&#x2014;including few-shot learning [<span class="footers"><a class="citation-link" href="#ref31" rel="footnote">31</a></span>,<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>,<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>,<span class="footers"><a class="citation-link" href="#ref46" rel="footnote">46</a></span>,<span class="footers"><a class="citation-link" href="#ref54" rel="footnote">54</a></span>,<span class="footers"><a class="citation-link" href="#ref58" rel="footnote">58</a></span>], chain-of-thought prompting [<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>,<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>,<span class="footers"><a class="citation-link" href="#ref49" rel="footnote">49</a></span>,<span class="footers"><a class="citation-link" href="#ref59" rel="footnote">59</a></span>,<span class="footers"><a class="citation-link" href="#ref60" rel="footnote">60</a></span>], and example contrast [<span class="footers"><a class="citation-link" href="#ref54" rel="footnote">54</a></span>]&#x2014;have been shown to substantially enhance diagnostic performance, especially for smaller models [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>]. Meanwhile, retrieval-augmented generation (RAG) approaches enriched LLMs with structured knowledge (eg, <i>Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition</i> criteria), improving factual grounding in some cases [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>], but occasionally introducing noise or reducing performance due to redundancy and semantic drift [<span class="footers"><a class="citation-link" href="#ref55" rel="footnote">55</a></span>].</p><h5>Data Source</h5><p class="abstract-paragraph"><span class="footers"><a class="citation-link" href="#table1" rel="footnote">Table 1</a></span> summarizes the datasets used for GenAI-based mental health diagnosis and assessment, categorized by data modality and mental health focus. The full dataset list, including metadata and sampling details, is provided in <span class="footers"><a class="citation-link" href="#app6" rel="footnote">Multimedia Appendix 6</a></span> [<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>,<span class="footers"><a class="citation-link" href="#ref30" rel="footnote">30</a></span>,<span class="footers"><a class="citation-link" href="#ref35" rel="footnote">35</a></span>,<span class="footers"><a class="citation-link" href="#ref41" rel="footnote">41</a></span>,<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>,<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>,<span class="footers"><a class="citation-link" href="#ref49" rel="footnote">49</a></span>,<span class="footers"><a class="citation-link" href="#ref53" rel="footnote">53</a></span>,<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>,<span class="footers"><a class="citation-link" href="#ref62" rel="footnote">62</a></span>,<span class="footers"><a class="citation-link" href="#ref103" rel="footnote">103</a></span>,<span class="footers"><a class="citation-link" href="#ref105" rel="footnote">105</a></span>-<span class="footers"><a class="citation-link" href="#ref133" rel="footnote">133</a></span>].</p><p class="abstract-paragraph">Social media posts, such as those on Reddit [<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>,<span class="footers"><a class="citation-link" href="#ref29" rel="footnote">29</a></span>,<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>], Twitter [<span class="footers"><a class="citation-link" href="#ref58" rel="footnote">58</a></span>], and Weibo [<span class="footers"><a class="citation-link" href="#ref45" rel="footnote">45</a></span>], emerged as prominent data sources. Beyond social media, 19% (7/37) of the studies used professionally curated clinical vignettes, providing controlled scenarios that simulate clinical cases and allow for standardized assessment across GenAI models [<span class="footers"><a class="citation-link" href="#ref34" rel="footnote">34</a></span>,<span class="footers"><a class="citation-link" href="#ref51" rel="footnote">51</a></span>]. Only a few studies (4/37, 11%) used clinical text data sources, including clinical interviews [<span class="footers"><a class="citation-link" href="#ref61" rel="footnote">61</a></span>], diary texts [<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>], and written responses of participants [<span class="footers"><a class="citation-link" href="#ref50" rel="footnote">50</a></span>,<span class="footers"><a class="citation-link" href="#ref62" rel="footnote">62</a></span>].</p><p class="abstract-paragraph">In total, 14% (5/37) of the studies used multimodal data sources&#x2014;such as speech [<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>,<span class="footers"><a class="citation-link" href="#ref45" rel="footnote">45</a></span>], sensor data [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>], and electroencephalogram (EEG) [<span class="footers"><a class="citation-link" href="#ref46" rel="footnote">46</a></span>]&#x2014;to enhance the accuracy and comprehensiveness of mental health assessments. For example, Englhardt et al [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>] developed prompting strategies for GenAI models to classify depression using passive sensing data (eg, activity, sleep, and social behavior) from mobile and wearable devices, achieving improved classification accuracy (up to 61.1%) over classical machine learning baselines. Similarly, Hu et al [<span class="footers"><a class="citation-link" href="#ref46" rel="footnote">46</a></span>] integrated EEG, audio, and facial expressions to boost predictive performance and proposed MultiEEG-GPT, a GPT-4o-based method for mental health assessment using multimodal inputs, including EEG, facial expressions, and audio. Their results across the 3 datasets showed that combining EEG with audio or facial expressions significantly improved prediction accuracy in both zero-shot and few-shot settings.</p><div class="figure-table"><figcaption><span class="typcn typcn-clipboard"></span><b>Table 1. </b>Summary of datasets used in the studies on generative artificial intelligence (GenAI) models for mental health diagnosis and assessment.</figcaption><table width="1000" cellpadding="5" cellspacing="0" border="1" rules="groups" frame="hsides"><col width="30" span="1"><col width="470" span="1"><col width="0" span="1"><col width="500" span="1"><thead><tr valign="top"><td colspan="3" rowspan="1">Categories</td><td rowspan="1" colspan="1">References</td></tr></thead><tbody><tr valign="top"><td colspan="4" rowspan="1"><b>By modality</b></td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Text (clinical vignettes)</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>,<span class="footers"><a class="citation-link" href="#ref105" rel="footnote">105</a></span>-<span class="footers"><a class="citation-link" href="#ref108" rel="footnote">108</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Text (social media posts)</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>,<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>,<span class="footers"><a class="citation-link" href="#ref53" rel="footnote">53</a></span>,<span class="footers"><a class="citation-link" href="#ref109" rel="footnote">109</a></span>-<span class="footers"><a class="citation-link" href="#ref123" rel="footnote">123</a></span>,<span class="footers"><a class="citation-link" href="#ref134" rel="footnote">134</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Text (transcripts)</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref35" rel="footnote">35</a></span>,<span class="footers"><a class="citation-link" href="#ref124" rel="footnote">124</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Text (daily self-reports)</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>,<span class="footers"><a class="citation-link" href="#ref62" rel="footnote">62</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Multimodal dataset</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>,<span class="footers"><a class="citation-link" href="#ref49" rel="footnote">49</a></span>,<span class="footers"><a class="citation-link" href="#ref125" rel="footnote">125</a></span>-<span class="footers"><a class="citation-link" href="#ref131" rel="footnote">131</a></span>]</td></tr><tr valign="top"><td colspan="4" rowspan="1"><b>By mental health issues</b></td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Depression</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>,<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>,<span class="footers"><a class="citation-link" href="#ref62" rel="footnote">62</a></span>,<span class="footers"><a class="citation-link" href="#ref109" rel="footnote">109</a></span>,<span class="footers"><a class="citation-link" href="#ref110" rel="footnote">110</a></span>,<span class="footers"><a class="citation-link" href="#ref113" rel="footnote">113</a></span>,<span class="footers"><a class="citation-link" href="#ref114" rel="footnote">114</a></span>,<span class="footers"><a class="citation-link" href="#ref117" rel="footnote">117</a></span>,<span class="footers"><a class="citation-link" href="#ref123" rel="footnote">123</a></span>,<span class="footers"><a class="citation-link" href="#ref126" rel="footnote">126</a></span>-<span class="footers"><a class="citation-link" href="#ref128" rel="footnote">128</a></span>,<span class="footers"><a class="citation-link" href="#ref130" rel="footnote">130</a></span>,<span class="footers"><a class="citation-link" href="#ref134" rel="footnote">134</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Suicide risk</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref12" rel="footnote">12</a></span>,<span class="footers"><a class="citation-link" href="#ref35" rel="footnote">35</a></span>,<span class="footers"><a class="citation-link" href="#ref40" rel="footnote">40</a></span>-<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>,<span class="footers"><a class="citation-link" href="#ref108" rel="footnote">108</a></span>,<span class="footers"><a class="citation-link" href="#ref109" rel="footnote">109</a></span>,<span class="footers"><a class="citation-link" href="#ref111" rel="footnote">111</a></span>,<span class="footers"><a class="citation-link" href="#ref112" rel="footnote">112</a></span>,<span class="footers"><a class="citation-link" href="#ref118" rel="footnote">118</a></span>,<span class="footers"><a class="citation-link" href="#ref119" rel="footnote">119</a></span>,<span class="footers"><a class="citation-link" href="#ref121" rel="footnote">121</a></span>,<span class="footers"><a class="citation-link" href="#ref122" rel="footnote">122</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Posttraumatic stress disorder</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref110" rel="footnote">110</a></span>,<span class="footers"><a class="citation-link" href="#ref125" rel="footnote">125</a></span>,<span class="footers"><a class="citation-link" href="#ref127" rel="footnote">127</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Anxiety</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref115" rel="footnote">115</a></span>,<span class="footers"><a class="citation-link" href="#ref125" rel="footnote">125</a></span>,<span class="footers"><a class="citation-link" href="#ref128" rel="footnote">128</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Bipolar disorder</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref120" rel="footnote">120</a></span>,<span class="footers"><a class="citation-link" href="#ref124" rel="footnote">124</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Stress</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref53" rel="footnote">53</a></span>,<span class="footers"><a class="citation-link" href="#ref115" rel="footnote">115</a></span>,<span class="footers"><a class="citation-link" href="#ref132" rel="footnote">132</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Emotion regulation</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref62" rel="footnote">62</a></span>,<span class="footers"><a class="citation-link" href="#ref129" rel="footnote">129</a></span>,<span class="footers"><a class="citation-link" href="#ref131" rel="footnote">131</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1"><br></td><td rowspan="1" colspan="1">Multiple psychiatric disorders</td><td colspan="2" rowspan="1">[<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>,<span class="footers"><a class="citation-link" href="#ref63" rel="footnote">63</a></span>,<span class="footers"><a class="citation-link" href="#ref106" rel="footnote">106</a></span>,<span class="footers"><a class="citation-link" href="#ref107" rel="footnote">107</a></span>,<span class="footers"><a class="citation-link" href="#ref133" rel="footnote">133</a></span>]</td></tr></tbody></table></div><h4>GenAI as Therapeutic Tools</h4><p class="abstract-paragraph">Of the 79 included studies, 20 (25%) investigated the use of GenAI-based chatbots and conversational agents to facilitate interventions ranging from emotional support to more structured therapies. To assess the feasibility and potential impact of these interventions, we analyzed studies across four key dimensions: (1) therapeutic targets, (2) implementation strategies, (3) evaluation outcomes, and (4) real-world deployment features.</p><h5>Intervention Targets and Theoretical Alignments</h5><p class="abstract-paragraph">As presented in <span class="footers"><a class="citation-link" href="#figure5" rel="footnote">Figure 5</a></span>, most studies (16/20, 80%) targeted the general population. A smaller subset (5/20, 25%) focused on vulnerable or underserved groups, including outpatients undergoing psychiatric treatment [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>], lesbian, gay, bisexual, transgender, and queer (LGBTQ) individuals [<span class="footers"><a class="citation-link" href="#ref65" rel="footnote">65</a></span>], sexual harassment survivors [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>], children with attention-deficit/hyperactivity disorder [<span class="footers"><a class="citation-link" href="#ref67" rel="footnote">67</a></span>], and older adults [<span class="footers"><a class="citation-link" href="#ref68" rel="footnote">68</a></span>]. In addition to population-specific adaptations, some studies (4/20, 20%) focused on chatbots targeting specific psychological and behavioral challenges, including attention-deficit/hyperactivity disorder [<span class="footers"><a class="citation-link" href="#ref67" rel="footnote">67</a></span>], problematic smartphone use [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>], preoperative anxiety [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>], and relationship issues [<span class="footers"><a class="citation-link" href="#ref71" rel="footnote">71</a></span>].</p><p class="abstract-paragraph">Despite the growing prevalence of these systems, most studies do not explicitly state the theoretical frameworks guiding their development. Among the reviewed studies, only 30% (6/20) of the studies explicitly adopted a psychological theory: person-centered therapy [<span class="footers"><a class="citation-link" href="#ref72" rel="footnote">72</a></span>]; cognitive behavioral therapy [<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>,<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>-<span class="footers"><a class="citation-link" href="#ref75" rel="footnote">75</a></span>]; and existence, relatedness, and growth theory [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>].</p><p class="abstract-paragraph">Beyond chatbot-based interventions, several studies (2/20, 10%) used passive monitoring, combining real-time physiological [<span class="footers"><a class="citation-link" href="#ref74" rel="footnote">74</a></span>] and behavioral data [<span class="footers"><a class="citation-link" href="#ref8" rel="footnote">8</a></span>] from wearables to assess mental states and trigger interventions. For example, empathic LLMs developed by Dongre [<span class="footers"><a class="citation-link" href="#ref74" rel="footnote">74</a></span>] adapted responses based on users&#x2019; stress levels, achieved 85.1% stress detection accuracy, and fostered strong therapeutic engagement in a pilot study involving 13 PhD students.</p><figure><a name="figure5">&#x200E;</a><a class="fancybox" title="Figure 5. Sankey diagram mapping target group, problem, and theoretical framework in generative artificial intelligence&#x2013;based mental health therapy research. ADHD: attention-deficit/hyperactivity disorder; CBT: cognitive behavioral therapy; ERG: existence, relatedness, and growth; LGBTQ+: lesbian, gay, bisexual, transgender, queer, and other minority groups; PCT: present&#x2010;centered therapy." href="https://asset.jmir.pub/assets/13aa64121356c3067c9b405caf1f6aff.png" id="figure5"><img class="figure-image" src="https://asset.jmir.pub/assets/13aa64121356c3067c9b405caf1f6aff.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 5. </b> Sankey diagram mapping target group, problem, and theoretical framework in generative artificial intelligence&#x2013;based mental health therapy research. ADHD: attention-deficit/hyperactivity disorder; CBT: cognitive behavioral therapy; ERG: existence, relatedness, and growth; LGBTQ+: lesbian, gay, bisexual, transgender, queer, and other minority groups; PCT: present&#x2010;centered therapy. </figcaption></figure><h5>Evaluation Strategies and Reported Outcomes</h5><p class="abstract-paragraph">Evaluation methods across the included studies varied considerably in terms of design, measurement, and reported outcomes. Approximately one-third of the included studies (7/20, 35%) used structured experimental designs, including randomized controlled trials [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>,<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>], field experiments [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>], and quasi-experimental studies [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>], with intervention spanning from one session to several weeks. These studies reported improvements in emotional intensity [<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>], anxiety [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>], or behavioral outcomes [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>]. For instance, a 5-week field study involving 25 participants demonstrated a 7% to 10% reduction in smartphone use and up to 22.5% improvement in intervention acceptance [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>]. Several studies (5/20, 25%) conducted simulated evaluations using test scenarios [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>], prompt-response validation [<span class="footers"><a class="citation-link" href="#ref76" rel="footnote">76</a></span>], and expert review [<span class="footers"><a class="citation-link" href="#ref67" rel="footnote">67</a></span>,<span class="footers"><a class="citation-link" href="#ref77" rel="footnote">77</a></span>]. A third group used user-centered approaches, such as semistructured interviews [<span class="footers"><a class="citation-link" href="#ref65" rel="footnote">65</a></span>], open-ended surveys [<span class="footers"><a class="citation-link" href="#ref72" rel="footnote">72</a></span>], or retrospective analyses of user-generated content [<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>].</p><p class="abstract-paragraph">Evaluation metrics were clustered into several domains. A substantial number of studies (14/20, 70%) assessed subjective user experiences, such as emotional relief, satisfaction, engagement, and self-efficacy [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>,<span class="footers"><a class="citation-link" href="#ref71" rel="footnote">71</a></span>,<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>]. These measures often relied on Likert-scale items or thematic coding of user interviews, particularly in studies involving direct patient interaction. Standardized psychometric instruments were applied in several studies to quantify clinical outcomes, such as the State-Trait Anxiety Inventory [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>] and the Self-Efficacy Scale [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>]. In contrast, studies focused on technical development predominantly adopted automated metrics, such as perplexity, bilingual evaluation understudy scores, and top-k accuracy [<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>,<span class="footers"><a class="citation-link" href="#ref78" rel="footnote">78</a></span>].</p><p class="abstract-paragraph">Across these varied approaches, most studies (17/20, 85%) reported positive outcomes. Emotional support functions were generally well received, with users describing increased affective relief [<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>], perceived empathy [<span class="footers"><a class="citation-link" href="#ref65" rel="footnote">65</a></span>], and greater openness to self-reflection [<span class="footers"><a class="citation-link" href="#ref75" rel="footnote">75</a></span>]. Structured interventions showed measurable improvements in behavior, including reduced problematic smartphone use and increased adherence to interventions [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>]. Nevertheless, several studies (5/20, 25%) highlighted users&#x2019; concerns regarding personalization, contextual fit, and trust [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>]. Moreover, while GenAI models often succeeded in simulating supportive interactions, they struggled to offer nuanced responses or adapt to complex individual needs [<span class="footers"><a class="citation-link" href="#ref65" rel="footnote">65</a></span>]. Users also raised concerns about repetitive phrasing, overly generic suggestions, and insufficient safety mechanisms, particularly in high-stakes scenarios such as crisis intervention or identity-sensitive disclosures [<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>,<span class="footers"><a class="citation-link" href="#ref71" rel="footnote">71</a></span>].</p><h5>Model Architectures and Adaptation Strategies</h5><p class="abstract-paragraph">The included studies used a variety of base models, with GPT-series being the most frequently adopted across interventions [<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>,<span class="footers"><a class="citation-link" href="#ref60" rel="footnote">60</a></span>,<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>,<span class="footers"><a class="citation-link" href="#ref67" rel="footnote">67</a></span>,<span class="footers"><a class="citation-link" href="#ref68" rel="footnote">68</a></span>,<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>,<span class="footers"><a class="citation-link" href="#ref71" rel="footnote">71</a></span>,<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>,<span class="footers"><a class="citation-link" href="#ref75" rel="footnote">75</a></span>,<span class="footers"><a class="citation-link" href="#ref76" rel="footnote">76</a></span>,<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. A small set of studies (6/20, 30%) used alternatives such as Falcon [<span class="footers"><a class="citation-link" href="#ref74" rel="footnote">74</a></span>], LLaMA [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>,<span class="footers"><a class="citation-link" href="#ref77" rel="footnote">77</a></span>], or custom transformer-based architectures [<span class="footers"><a class="citation-link" href="#ref72" rel="footnote">72</a></span>,<span class="footers"><a class="citation-link" href="#ref78" rel="footnote">78</a></span>].</p><p class="abstract-paragraph">To tailor GenAI models for mental health applications, researchers have adopted a range of adaptation techniques. Prompt engineering was the most frequently applied strategy. This approach included emotional state-sensitive prompting [<span class="footers"><a class="citation-link" href="#ref69" rel="footnote">69</a></span>] and modular prompt templates [<span class="footers"><a class="citation-link" href="#ref60" rel="footnote">60</a></span>]. A smaller number of studies (2/20, 10%) applied fine-tuning strategies using real-world therapy dialogues or support data [<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>,<span class="footers"><a class="citation-link" href="#ref77" rel="footnote">77</a></span>]. For instance, Yu and McGuinness [<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>] fine-tuned DialoGPT on 5000 therapy conversations and layered it with knowledge-injected prompts via ChatGPT-3.5, achieving improved conversational relevance and empathy as assessed by perplexity, bilingual evaluation understudy scores and user ratings. Herencia [<span class="footers"><a class="citation-link" href="#ref77" rel="footnote">77</a></span>] used Low-Rank Adaptation to fine-tune LLaMA-2 on mental health dialogue data, resulting in a fine-tuned model that outperformed the base LLaMA in BERT and Metric for Evaluation of Translation with Explicit Ordering scores, with reduced inference time and improved contextual sensitivity in simulated counseling interactions.</p><p class="abstract-paragraph">Beyond internal adaptations, RAG was used to enrich responses with external knowledge. For instance, Vakayil et al [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>] integrated RAG into a LLaMA-2&#x2013;based chatbot to support survivors of sexual harassment, combining empathetic dialogue with accurate legal and crisis information drawn from a curated database.</p><h5>Clinical Readiness</h5><p class="abstract-paragraph">To evaluate the translational potential of GenAI models into clinical practice, we synthesized four indicators of real-world readiness across the included studies: (1) expert evaluation, (2) user acceptability, (3) clinical deployment, and (4) safety mechanisms. Among the 20 studies reviewed, only 4 (20%) involved formal expert evaluation, such as ratings by licensed clinicians or psychiatric specialists [<span class="footers"><a class="citation-link" href="#ref67" rel="footnote">67</a></span>,<span class="footers"><a class="citation-link" href="#ref68" rel="footnote">68</a></span>]. In contrast, user acceptability was more frequently assessed, with 60% (12/20) of the studies reporting participant feedback on usability, supportiveness, or trust in GenAI. Clinical implementation was reported in only 15% (3/20) of the studies conducted in real-world or quasi-clinical settings. Regarding safety, only 30% (6/20) of the studies implemented explicit safety measures, such as toxicity filters [<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>], crisis response triggers [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>], or expert validation [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>].</p><h4>GenAI for Supporting Clinicians and Mental Health Professionals</h4><p class="abstract-paragraph">Of the 79 included studies, 24 (30%) focused on applying GenAI to support clinicians and mental health professionals, with 2 (2%) overlapping with the research on GenAI models for mental health diagnosis and assessment.</p><h5>Role of GenAI in Supporting Clinicians and Mental Health Professionals</h5><h6>Overview</h6><p class="abstract-paragraph">Recent research has demonstrated a growing interest in the use of GenAI to support mental health professionals across diverse clinical tasks. Drawing on a synthesis of empirical studies (<span class="footers"><a class="citation-link" href="#table2" rel="footnote">Table 2</a></span>), we identified five core functional roles through which GenAI contributes to mental health services: (1) clinical decision support, (2) documentation and summarization, (3) therapy support, (4) psychoeducation, and (5) training and simulation.</p><div class="figure-table"><figcaption><span class="typcn typcn-clipboard"></span><b>Table 2. </b>Categorization of generative artificial intelligence (GenAI) support roles and representative applications in mental health contexts.</figcaption><table width="1000" cellpadding="5" cellspacing="0" border="1" rules="groups" frame="hsides"><col width="270" span="1"><col width="530" span="1"><col width="200" span="1"><thead><tr valign="top"><td rowspan="1" colspan="1">Support roles</td><td rowspan="1" colspan="1">Representative tasks</td><td rowspan="1" colspan="1">References</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">Clinical decision support</td><td rowspan="1" colspan="1">Treatment planning, prognosis, and case formulation</td><td rowspan="1" colspan="1">[<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>,<span class="footers"><a class="citation-link" href="#ref80" rel="footnote">80</a></span>-<span class="footers"><a class="citation-link" href="#ref87" rel="footnote">87</a></span>,<span class="footers"><a class="citation-link" href="#ref89" rel="footnote">89</a></span>,<span class="footers"><a class="citation-link" href="#ref96" rel="footnote">96</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1">Documentation and summarization</td><td rowspan="1" colspan="1">Summarizing counseling sessions and summarization of multimodal sensor data</td><td rowspan="1" colspan="1">[<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>,<span class="footers"><a class="citation-link" href="#ref88" rel="footnote">88</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1">Therapy support</td><td rowspan="1" colspan="1">Reframing, emotion extraction, and reflection</td><td rowspan="1" colspan="1">[<span class="footers"><a class="citation-link" href="#ref90" rel="footnote">90</a></span>-<span class="footers"><a class="citation-link" href="#ref93" rel="footnote">93</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1">Psychoeducation</td><td rowspan="1" colspan="1">Questions and answers, recommendations, and interactive guidance</td><td rowspan="1" colspan="1">[<span class="footers"><a class="citation-link" href="#ref63" rel="footnote">63</a></span>,<span class="footers"><a class="citation-link" href="#ref80" rel="footnote">80</a></span>,<span class="footers"><a class="citation-link" href="#ref94" rel="footnote">94</a></span>-<span class="footers"><a class="citation-link" href="#ref98" rel="footnote">98</a></span>]</td></tr><tr valign="top"><td rowspan="1" colspan="1">Training and simulation</td><td rowspan="1" colspan="1">Case vignettes and synthetic data</td><td rowspan="1" colspan="1">[<span class="footers"><a class="citation-link" href="#ref9" rel="footnote">9</a></span>,<span class="footers"><a class="citation-link" href="#ref84" rel="footnote">84</a></span>,<span class="footers"><a class="citation-link" href="#ref99" rel="footnote">99</a></span>]</td></tr></tbody></table></div><h6>Clinical Decision Support</h6><p class="abstract-paragraph">One of the most frequently studied applications of GenAI is its use in supporting clinical decision-making. This includes tasks such as treatment planning [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref80" rel="footnote">80</a></span>-<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>], case formulation [<span class="footers"><a class="citation-link" href="#ref83" rel="footnote">83</a></span>-<span class="footers"><a class="citation-link" href="#ref85" rel="footnote">85</a></span>], and prognosis assessment [<span class="footers"><a class="citation-link" href="#ref86" rel="footnote">86</a></span>,<span class="footers"><a class="citation-link" href="#ref87" rel="footnote">87</a></span>]. Studies show that GenAI-generated treatment plans are often consistent with clinical guidelines and therapeutic theories [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref85" rel="footnote">85</a></span>], and sometimes outperform general practitioners in adherence [<span class="footers"><a class="citation-link" href="#ref81" rel="footnote">81</a></span>,<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>]. For case formulation, GenAI has been shown to produce coherent and theory-driven conceptualizations, including psychodynamic [<span class="footers"><a class="citation-link" href="#ref83" rel="footnote">83</a></span>] and multimodal [<span class="footers"><a class="citation-link" href="#ref84" rel="footnote">84</a></span>] therapy. Prognostic predictions for mental health conditions such as depression [<span class="footers"><a class="citation-link" href="#ref87" rel="footnote">87</a></span>] and schizophrenia [<span class="footers"><a class="citation-link" href="#ref86" rel="footnote">86</a></span>] have also shown expert-level agreement. However, when used for engaging directly with patients for clinical assessment, GenAI models still lack capabilities in structured interviewing and differential diagnosis [<span class="footers"><a class="citation-link" href="#ref80" rel="footnote">80</a></span>].</p><h6>Documentation and Summarization</h6><p class="abstract-paragraph">GenAI models have also demonstrated potential in reducing clinicians&#x2019; administrative burden through automated documentation. Adhikary et al [<span class="footers"><a class="citation-link" href="#ref88" rel="footnote">88</a></span>] benchmarked 11 LLMs on their ability to summarize mental health counseling sessions, identifying Mistral and MentalLLaMA as having the highest extractive quality. Beyond summarization, GenAI has also been applied to the integration of multisensor behavioral health data. Englhardt et al [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>] examined LLMs&#x2019; ability to analyze passive sensing data for assessing mental health conditions such as depression and anxiety. Their results showed that LLMs correctly referenced numerical data 75% of the time and achieved a classification accuracy of 61.1%, surpassing traditional machine learning models. However, both studies identified hallucination as a critical limitation, including errors such as incorrect documentation of suicide risk [<span class="footers"><a class="citation-link" href="#ref88" rel="footnote">88</a></span>].</p><h6>Therapy Support</h6><p class="abstract-paragraph">A growing body of research suggests that GenAI can enhance therapeutic processes by supporting treatment goal setting [<span class="footers"><a class="citation-link" href="#ref89" rel="footnote">89</a></span>], emotional reflection [<span class="footers"><a class="citation-link" href="#ref90" rel="footnote">90</a></span>], cognitive restructuring [<span class="footers"><a class="citation-link" href="#ref91" rel="footnote">91</a></span>,<span class="footers"><a class="citation-link" href="#ref92" rel="footnote">92</a></span>], and motivational interviewing [<span class="footers"><a class="citation-link" href="#ref93" rel="footnote">93</a></span>]. In the context of cognitive behavioral therapy, GenAI has been used to identify mismatched thought-feeling pairs, with a 73.5% cross-validated accuracy rate [<span class="footers"><a class="citation-link" href="#ref91" rel="footnote">91</a></span>], and to assist in reframing maladaptive cognitions with high rates of successful reconstruction [<span class="footers"><a class="citation-link" href="#ref92" rel="footnote">92</a></span>]. Other therapeutic applications include guided journaling for mood tracking, which has been shown to increase patient engagement and emotional awareness [<span class="footers"><a class="citation-link" href="#ref90" rel="footnote">90</a></span>].</p><h6>Psychoeducation</h6><p class="abstract-paragraph">GenAI has been used to provide accessible mental health information to the public, with studies showing that it can deliver accurate and actionable content while maintaining empathetic tone [<span class="footers"><a class="citation-link" href="#ref94" rel="footnote">94</a></span>]. GenAI has also been explored as a tool for creating interactive psychoeducational experiences, particularly for children and adolescents, through role-playing and other engagement strategies [<span class="footers"><a class="citation-link" href="#ref95" rel="footnote">95</a></span>]. For example, Hu et al [<span class="footers"><a class="citation-link" href="#ref96" rel="footnote">96</a></span>] developed a child-facing GenAI agent designed to foster psychological resilience, which demonstrated improvements in both engagement and mental health outcomes. Nevertheless, limitations in emotional nuance and consistency have been observed. For example, Giorgi et al [<span class="footers"><a class="citation-link" href="#ref97" rel="footnote">97</a></span>] documented harmful outputs in substance use queries, and comparative analyses have shown that GenAI often lacks the emotional attunement characteristic of human clinicians [<span class="footers"><a class="citation-link" href="#ref63" rel="footnote">63</a></span>,<span class="footers"><a class="citation-link" href="#ref98" rel="footnote">98</a></span>].</p><h6>Training and Simulation</h6><p class="abstract-paragraph">Beyond direct patient care, GenAI has been increasingly applied in clinical education as low-risk tools for skill development and reasoning practice. They have been used to generate case vignettes, simulate diagnostic interviews, support self-directed learning, prompt clinical reasoning, and create synthetic datasets for model development [<span class="footers"><a class="citation-link" href="#ref9" rel="footnote">9</a></span>,<span class="footers"><a class="citation-link" href="#ref84" rel="footnote">84</a></span>,<span class="footers"><a class="citation-link" href="#ref99" rel="footnote">99</a></span>], offering scalable solutions for training, especially in resource-limited settings.</p><h5>Modeling and Evaluation Strategies in GenAI for Mental Health Support</h5><p class="abstract-paragraph">GPT-3.5 [<span class="footers"><a class="citation-link" href="#ref4" rel="footnote">4</a></span>] and GPT-4 [<span class="footers"><a class="citation-link" href="#ref104" rel="footnote">104</a></span>] were the most frequently used models for clinician support tasks [<span class="footers"><a class="citation-link" href="#ref81" rel="footnote">81</a></span>,<span class="footers"><a class="citation-link" href="#ref83" rel="footnote">83</a></span>,<span class="footers"><a class="citation-link" href="#ref84" rel="footnote">84</a></span>,<span class="footers"><a class="citation-link" href="#ref89" rel="footnote">89</a></span>,<span class="footers"><a class="citation-link" href="#ref90" rel="footnote">90</a></span>], yet comparative findings reveal that no single model consistently outperforms others. For instance, Bard (rebranded as Gemini) [<span class="footers"><a class="citation-link" href="#ref135" rel="footnote">135</a></span>] has been shown to outperform GPT-4 [<span class="footers"><a class="citation-link" href="#ref104" rel="footnote">104</a></span>] in reconstructing negative thoughts [<span class="footers"><a class="citation-link" href="#ref92" rel="footnote">92</a></span>], and LLaMA-2 [<span class="footers"><a class="citation-link" href="#ref136" rel="footnote">136</a></span>] surpasses GPT-4 [<span class="footers"><a class="citation-link" href="#ref104" rel="footnote">104</a></span>] in adequacy, appropriateness, and overall quality when addressing substance use-related questions [<span class="footers"><a class="citation-link" href="#ref97" rel="footnote">97</a></span>]. These findings emphasize the importance of task-specific model selection. Consequently, recent studies have turned to customized or fine-tuned models that are better aligned with domain-specific linguistic and contextual demands. For example, Furukawa et al [<span class="footers"><a class="citation-link" href="#ref91" rel="footnote">91</a></span>] used a fine-tuned Japanese T5 model [<span class="footers"><a class="citation-link" href="#ref24" rel="footnote">24</a></span>] to assist clinicians in emotion prediction during cognitive restructuring. By analyzing more than 7000 thought-feeling records from 2 large-scale randomized controlled trials, the model helped to identify mismatched thought-feeling pairs with 73.5% accuracy. Empirical studies further support this approach, demonstrating that domain-specific models consistently outperform general-purpose models in mental health care tasks [<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>,<span class="footers"><a class="citation-link" href="#ref88" rel="footnote">88</a></span>].</p><p class="abstract-paragraph">A range of adaptation strategies and evaluation methods were identified across the included studies. As illustrated in <span class="footers"><a class="citation-link" href="#figure6" rel="footnote">Figure 6</a></span>, prompt engineering was the most common strategy, especially in clinical decision support [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>,<span class="footers"><a class="citation-link" href="#ref89" rel="footnote">89</a></span>], psychoeducation [<span class="footers"><a class="citation-link" href="#ref63" rel="footnote">63</a></span>,<span class="footers"><a class="citation-link" href="#ref97" rel="footnote">97</a></span>], and therapy support tasks [<span class="footers"><a class="citation-link" href="#ref90" rel="footnote">90</a></span>,<span class="footers"><a class="citation-link" href="#ref93" rel="footnote">93</a></span>]. Fine-tuning was used less frequently, limited to contexts with domain-specific corpora (eg, documentation [<span class="footers"><a class="citation-link" href="#ref88" rel="footnote">88</a></span>] and emotion classification [<span class="footers"><a class="citation-link" href="#ref96" rel="footnote">96</a></span>]). Modular orchestration strategies were identified in only a small number (2/24, 8%) of studies [<span class="footers"><a class="citation-link" href="#ref95" rel="footnote">95</a></span>,<span class="footers"><a class="citation-link" href="#ref96" rel="footnote">96</a></span>].</p><figure><a name="figure6">&#x200E;</a><a class="fancybox" title="Figure 6. Sankey diagram showing the methodological flow in generative artificial intelligence&#x2013;based mental health support research." href="https://asset.jmir.pub/assets/73a555d548f48e404a8b2ff9292091f5.png" id="figure6"><img class="figure-image" src="https://asset.jmir.pub/assets/73a555d548f48e404a8b2ff9292091f5.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 6. </b> Sankey diagram showing the methodological flow in generative artificial intelligence&#x2013;based mental health support research. </figcaption></figure><p class="abstract-paragraph">Evaluation methods also varied by task type. Clinical and diagnostic tasks favored expert review [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref80" rel="footnote">80</a></span>,<span class="footers"><a class="citation-link" href="#ref84" rel="footnote">84</a></span>] and automated metrics [<span class="footers"><a class="citation-link" href="#ref88" rel="footnote">88</a></span>,<span class="footers"><a class="citation-link" href="#ref92" rel="footnote">92</a></span>], whereas patient-facing tasks&#x2014;such as psychoeducation [<span class="footers"><a class="citation-link" href="#ref96" rel="footnote">96</a></span>] and emotional support [<span class="footers"><a class="citation-link" href="#ref93" rel="footnote">93</a></span>]&#x2014;relied more on user-centered feedback or psychometric assessments.</p><h5>Clinical Readiness</h5><p class="abstract-paragraph">Among the 24 studies reviewed, only 2 (8%) involved real-world clinical deployment [<span class="footers"><a class="citation-link" href="#ref51" rel="footnote">51</a></span>,<span class="footers"><a class="citation-link" href="#ref91" rel="footnote">91</a></span>]. Expert evaluation was reported in more than 80% (20/24) of the studies, while user acceptability appeared in only 25% (6/24) of the studies. Safety mechanisms&#x2014;such as hallucination control, bias mitigation, and clinician override&#x2014;were explicitly implemented in 17% (4/24) of the studies.</p><h4>Reporting Quality of Included Studies</h4><p class="abstract-paragraph">We assessed the reporting quality of the included studies using the MI-CLAIM-GEN checklist [<span class="footers"><a class="citation-link" href="#ref101" rel="footnote">101</a></span>]. Each item was scored on a 4-point scale (yes, no, unsure, and not applicable) following the Joanna Briggs Institute quality appraisal format [<span class="footers"><a class="citation-link" href="#ref102" rel="footnote">102</a></span>]. The results are presented in <span class="footers"><a class="citation-link" href="#figure7" rel="footnote">Figure 7</a></span>.</p><figure><a name="figure7">&#x200E;</a><a class="fancybox" title="Figure 7. Reporting quality of the included studies based on the Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research (MI-CLAIM-GEN) checklist." href="https://asset.jmir.pub/assets/9987a8143a8bc93d3dfe90b1d00fe6ca.png" id="figure7"><img class="figure-image" src="https://asset.jmir.pub/assets/9987a8143a8bc93d3dfe90b1d00fe6ca.png"></a><figcaption><span class="typcn typcn-image"></span><b>Figure 7. </b> Reporting quality of the included studies based on the Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research (MI-CLAIM-GEN) checklist. </figcaption></figure><p class="abstract-paragraph">On average, 45.39% (753/1659) of items were rated as <i>yes</i>, indicating a moderate level of reporting transparency across the corpus. Reporting completeness varied substantially across the items, and only 10 items achieved <i>yes</i> ratings in more than half (40/79, 51%) of the studies. As shown in <span class="footers"><a class="citation-link" href="#figure7" rel="footnote">Figure 7</a></span>, items related to study design (items 1.1&#x2013;1.5), model performance and evaluation (items 3.1&#x2013;3.4), and model examination (items 4.1&#x2013;4.5) were most consistently reported, with 73.9% (292/395), 56% (177/316), and 54.1% (171/316) of the studies achieving <i>yes</i> ratings, respectively. In contrast, items concerning resources and optimization (items 2.1&#x2013;2.4) and reproducibility (items 5.1&#x2013;5.3) were frequently underreported, with 25.3% (100/395) and 5.5% (13/237) of the studies providing sufficient information in these areas.</p><p class="abstract-paragraph">Item-level analysis further revealed critical disparities. Core design elements were consistently addressed&#x2014;for instance, 1.1 (study context) and 1.2 (research question) received <i>yes</i> ratings in 97% (77/79) and 100% (79/79) of the studies, respectively. However, items, such as 1.5 (representativeness of training data) were often overlooked, with only 11% (9/79) of studies providing sufficient reporting. Similarly, while 89% (70/79) of the studies described model outputs (item 3c), only 20% (16/79) of the studies included a comprehensive evaluation framework (item 3b). Postdeployment considerations, including harm assessment (item 4e) and evaluation under real-world settings (item 4d), were almost entirely absent. In the reproducibility domain, none of the studies provided a model card (item 5b), and only 14% (11/79) of the studies reached tier-1 reproducibility by reporting sufficient implementation details (item 5a).</p><h4>Ethical Issues and the Responsible Use of GenAI in Mental Health</h4><p class="abstract-paragraph">On the basis of the analysis of ethical concerns identified across the included studies, we synthesized 4 core domains&#x2014;data privacy, information integrity, user safety, and ethical governance and oversight. Drawing on these dimensions, we proposed the GenAI4MH ethical framework (<span class="footers"><a class="citation-link" href="#figure2" rel="footnote">Figure 2</a></span>) to comprehensively address the unique ethical challenges in this domain and guide the responsible design, deployment, and use of GenAI in mental health contexts.</p><h5>Data Privacy and Security</h5><p class="abstract-paragraph">The use of GenAI in mental health settings raises heightened concerns regarding data privacy due to the inherently sensitive nature of psychological data. In this context, data privacy and security involve 3 dimensions: confidentiality (who has access to the data), security (how the data are technically and administratively protected), and anonymity (whether the data can be traced back to individuals). Both users [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>] and clinicians [<span class="footers"><a class="citation-link" href="#ref47" rel="footnote">47</a></span>] reported concerns about sharing sensitive information with GenAI, citing a lack of clarity on data storage and regulatory oversight [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>]. These concerns are further amplified in vulnerable populations, including children [<span class="footers"><a class="citation-link" href="#ref96" rel="footnote">96</a></span>] and LGBTQ individuals [<span class="footers"><a class="citation-link" href="#ref65" rel="footnote">65</a></span>].</p><p class="abstract-paragraph">To mitigate these risks, previous studies proposed 2 main strategies. First, platforms should implement transparency notices that clearly inform users of potential data logging and caution against disclosing personally identifiable or highly sensitive information [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>]. Second, systems should incorporate real-time filtering and alert mechanisms to detect and block unauthorized disclosures, such as names and contact details, especially during emotionally charged interactions [<span class="footers"><a class="citation-link" href="#ref67" rel="footnote">67</a></span>].</p><h5>Information Integrity and Fairness</h5><p class="abstract-paragraph">Information integrit<i>y</i> and fairness refers to the factual correctness, fairness, reliability, and cultural appropriateness of GenAI-generated outputs. A central challenge lies in the presence of systematic biases. Heinz et al [<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>] found that LLMs reproduced real-world disparities: American Indian and Alaska Native individuals were more likely to be labeled with substance use disorders, and women with borderline personality disorder. Although not all patterns of bias were observed&#x2014;for instance, the overdiagnosis of psychosis in Black individuals&#x2014;other studies reported similar trends. Perlis et al [<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>] noted reduced recommendation accuracy for Black women, while Soun and Nair [<span class="footers"><a class="citation-link" href="#ref38" rel="footnote">38</a></span>] identified performance disparities across gender, favoring young women over older men.</p><p class="abstract-paragraph">GenAI models also show limited cross-cultural adaptability. Performance drops have been observed in dialectal and underrepresented language contexts [<span class="footers"><a class="citation-link" href="#ref44" rel="footnote">44</a></span>], and users have reported that GenAI models fail to interpret nuanced cultural norms or offer locally appropriate mental health resources [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>,<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>]. Another major concern involves consistency and factual reliability. GenAI models have been found to generate medically inaccurate or harmful content, including nonexistent drugs [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>], contradicted medications [<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>], incorrect hotline information [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>], and unsupported interventions [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. Some models hallucinated suicide behaviors [<span class="footers"><a class="citation-link" href="#ref35" rel="footnote">35</a></span>] or missed explicit crisis signals [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. In one study, nearly 80% of users reported encountering outdated, biased, or inaccurate outputs [<span class="footers"><a class="citation-link" href="#ref22" rel="footnote">22</a></span>]. Moreover, outputs often vary across minor prompt changes and repeated runs [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>,<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>], and the temporal lag between model training and deployment may result in misalignment with current psychiatric guidelines [<span class="footers"><a class="citation-link" href="#ref99" rel="footnote">99</a></span>].</p><p class="abstract-paragraph">To address these challenges, a range of mitigation strategies has been proposed across fairness, cultural adaptation, factual integrity, and response consistency. For bias and fairness, researchers have proposed several strategies targeting the underlying causes&#x2014;most notably, the skewed demographic representation in training data [<span class="footers"><a class="citation-link" href="#ref38" rel="footnote">38</a></span>]. These approaches include value-aligned data augmentation, training set debiasing, and increasing the diversity of demographic groups represented in both training and evaluation datasets [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. Instruction-tuned models developed specifically for mental health tasks have also demonstrated improved subgroup performance and fairness across gender and age groups [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>,<span class="footers"><a class="citation-link" href="#ref55" rel="footnote">55</a></span>]. To enhance cultural adaptability, studies have proposed multilingual fine-tuning, dialect-specific testing, and adaptive language modeling tailored to users&#x2019; linguistic and sociodemographic backgrounds [<span class="footers"><a class="citation-link" href="#ref42" rel="footnote">42</a></span>,<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>]. For improving factual reliability and reducing hallucinations, techniques include conservative prompting (eg, yes or no formats) [<span class="footers"><a class="citation-link" href="#ref41" rel="footnote">41</a></span>], factual verification pipelines [<span class="footers"><a class="citation-link" href="#ref97" rel="footnote">97</a></span>], and RAG from validated clinical sources [<span class="footers"><a class="citation-link" href="#ref55" rel="footnote">55</a></span>]. Domain-specific fine-tuning [<span class="footers"><a class="citation-link" href="#ref58" rel="footnote">58</a></span>], hallucination detection tools, manual output review, and ensemble modeling [<span class="footers"><a class="citation-link" href="#ref52" rel="footnote">52</a></span>] have also shown promise. In addition, some studies incorporate real-time web retrieval to reduce outdated information and increase clinical relevance [<span class="footers"><a class="citation-link" href="#ref66" rel="footnote">66</a></span>]. To promote response consistency, researchers have applied parameter-controlled generation and reduced model temperature, both of which have been shown to decrease output variability across repeated prompts [<span class="footers"><a class="citation-link" href="#ref2" rel="footnote">2</a></span>].</p><h5>User Safety</h5><p class="abstract-paragraph">In mental health contexts, user safety refers to protecting individuals from psychological harm, ethical breaches, or misguided decision-making caused by GenAI. Ensuring safety encompasses multiple facets, including (1) preserving user autonomy, (2) enabling reliable crisis intervention, and (3) preventing overtrust in GenAI-generated content.</p><p class="abstract-paragraph">A major concern is the erosion of user autonomy. Users may mistakenly ascribe clinical authority to GenAI, despite their lack of diagnostic competence, reasoning capability, or professional accountability [<span class="footers"><a class="citation-link" href="#ref80" rel="footnote">80</a></span>], leading to uncritical acceptance of their suggestions [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. In some cases, GenAI models have at times abruptly terminated high-risk conversations (eg, involving suicidal ideation) without explanation, leaving users feeling emotionally abandoned or disoriented [<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>].</p><p class="abstract-paragraph">GenAI also shows limited reliability in crisis recognition and management. For instance, GPT-3.5 [<span class="footers"><a class="citation-link" href="#ref4" rel="footnote">4</a></span>] has been found to consistently underestimate suicide risk [<span class="footers"><a class="citation-link" href="#ref35" rel="footnote">35</a></span>]. Even in explicit crisis prompts (eg, &#x201C;I want to die&#x201D;), only 56.6% of self-harm prompts were correctly flagged [<span class="footers"><a class="citation-link" href="#ref64" rel="footnote">64</a></span>,<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. Moreover, even when crises were detected, responses were often delayed [<span class="footers"><a class="citation-link" href="#ref76" rel="footnote">76</a></span>], and more than 38% of the generated replies were rated as unhelpful or misleading [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. Only a small proportion of GenAI models provided referral resources following risk detection [<span class="footers"><a class="citation-link" href="#ref60" rel="footnote">60</a></span>,<span class="footers"><a class="citation-link" href="#ref76" rel="footnote">76</a></span>].</p><p class="abstract-paragraph">To address these risks, several mitigation strategies have been proposed. Researchers recommend embedding disclaimers and transparency cues to clarify the system&#x2019;s nonclinical role [<span class="footers"><a class="citation-link" href="#ref72" rel="footnote">72</a></span>] and using empathic prompt templates to encourage user agency and referral to human professionals [<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>]. For high-risk scenarios, hybrid pipelines combining automated detection (eg, keyword scanning and risk scoring) with human oversight have been adopted to improve user safety [<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>].</p><h5>Ethical Governance</h5><p class="abstract-paragraph">Ethical governance refers to the establishment of regulatory, procedural, and normative frameworks that ensure these technologies are developed and deployed responsibly. Core governance dimensions include informed consent, transparency, ethics approval, ongoing oversight, and ethical dilemmas and responsibility.</p><p class="abstract-paragraph">A recurring concern is the lack of informed consent and operational transparency. Several studies have highlighted that users are often unaware of system limitations, data storage practices, or liability implications [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. Both clinicians and patients have also expressed concerns about the &#x201C;black box&#x201D; nature of GenAI, which offers limited interpretability and constrains clinical supervision and shared decision-making [<span class="footers"><a class="citation-link" href="#ref98" rel="footnote">98</a></span>]. Long-term governance remains underdeveloped. Ethics approval procedures are not consistently reported across studies, even when the research involves sensitive mental health content. Moreover, most systems lack clinical auditing mechanisms or feedback loops from licensed professionals. For example, a commercial chatbot was found to generate inappropriate content, such as drug use instructions and adult conversations with minors [<span class="footers"><a class="citation-link" href="#ref11" rel="footnote">11</a></span>]. Emerging ethical dilemmas further complicate implementation. For example, some platforms restrict outputs on sensitive topics to comply with platform policies, but such censorship may interfere with clinically relevant conversations [<span class="footers"><a class="citation-link" href="#ref51" rel="footnote">51</a></span>]. In other cases, systems blur the boundary between psychological support and formal treatment, raising unresolved questions about responsibility when harm occurs [<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>]. Current frameworks also provide little clarity on liability attribution&#x2014;whether it should rest with developers, platform operators, clinicians, or end users [<span class="footers"><a class="citation-link" href="#ref76" rel="footnote">76</a></span>].</p><p class="abstract-paragraph">In response, several governance strategies have been proposed. These include explicit informed consent procedures that inform users about system capabilities, data use, and the right to opt out at any time [<span class="footers"><a class="citation-link" href="#ref73" rel="footnote">73</a></span>], as well as prompt-based transparency cues to support clinician evaluation of GenAI outputs [<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>]. Technical methods&#x2014;such as knowledge-enhanced pretraining [<span class="footers"><a class="citation-link" href="#ref29" rel="footnote">29</a></span>] and symbolic reasoning graphs [<span class="footers"><a class="citation-link" href="#ref43" rel="footnote">43</a></span>]&#x2014;have been explored to improve model explainability. To strengthen ethical oversight, researchers have advocated for feedback-integrated learning pipelines involving clinician input, institutional ethics review protocols [<span class="footers"><a class="citation-link" href="#ref3" rel="footnote">3</a></span>], independent auditing bodies [<span class="footers"><a class="citation-link" href="#ref37" rel="footnote">37</a></span>], postdeployment safety evaluations [<span class="footers"><a class="citation-link" href="#ref97" rel="footnote">97</a></span>], and public registries for mental health&#x2013;related GenAI models [<span class="footers"><a class="citation-link" href="#ref7" rel="footnote">7</a></span>].</p><br><h3 class="navigation-heading h3-main-heading" id="Discussion" data-label="Discussion">Discussion</h3><h4>Principal Findings</h4><p class="abstract-paragraph">We systematically reviewed the applications of GenAI in mental health, focusing on 3 main areas: diagnosis and assessment, therapeutic tools, and clinician support. The findings reveal the potential of GenAI across these domains, while also highlighting technical, ethical, and implementation-related challenges.</p><p class="abstract-paragraph">First, in mental health diagnosis and assessment, GenAI has been widely used to detect and interpret mental health conditions. These models analyze textual and multimodal data to identify mental health issues, such as depression and stress, providing a novel pathway for early identification and intervention. Despite promising applications, the current body of research largely focuses on suicide risk and depression, with relatively few studies addressing other critical conditions. The lack of comprehensive coverage of these conditions limits our understanding of how GenAI might perform across a broader range of psychiatric conditions, each with unique clinical and social implications. Future research should prioritize expanding the scope to encompass less frequently addressed mental health conditions, enabling a more thorough evaluation of GenAI models&#x2019; utility and effectiveness across diverse mental health assessments. Moreover, a substantial portion of GenAI-based diagnostic research relies on social media datasets. While such data sources are abundant and often rich in user-expressed emotion, they frequently skew toward specific demographics&#x2014;such as younger, digitally active, and predominantly English-speaking users [<span class="footers"><a class="citation-link" href="#ref137" rel="footnote">137</a></span>]&#x2014;which may limit the cultural and linguistic diversity of the models&#x2019; training inputs. These limitations can affect model generalizability and raise concerns about bias when applied across different populations. As an alternative, integrating more diverse and ecologically valid data&#x2014;such as real-world data from electronic health records or community-based mental health services&#x2014;could better capture population-level heterogeneity. At the same time, although integrating multimodal signals&#x2014;such as vocal tone, facial expression, and behavioral patterns&#x2014;offers potential to improve the accuracy and richness of mental health assessments, such data are significantly more challenging to collect due to technical, ethical, and privacy-related constraints. Thus, there is an inherent tradeoff between the richness of data and the feasibility of acquisition. Future work should weigh these tradeoffs and may benefit from hybrid approaches that combine modest multimodal inputs with improved text-based modeling.</p><p class="abstract-paragraph">Second, as a therapeutic tool, GenAI has been applied to develop chatbots and conversational agents to provide emotional support, behavioral interventions, and crisis management. GPT-powered chatbots, for example, can engage users in managing anxiety, stress, and other emotional challenges, enhancing accessibility and personalization in mental health services [<span class="footers"><a class="citation-link" href="#ref70" rel="footnote">70</a></span>]. By offering accessible and anonymous mental health support, these GenAI models help bridge gaps in traditional mental health services, especially in areas with limited resources or high social stigma, thus supporting personalized mental health management and extending access to those who might otherwise avoid seeking help. However, the efficacy of these tools in managing complex emotions and crisis situations requires further validation, as many studies are constrained by small sample sizes or rely on simulated scenarios and engineering-focused approaches without real user testing. In particular, crisis detection capabilities present a complex tradeoff. On the one hand, prompt identification of suicidal ideation or emotional breakdowns is critical to prevent harm; on the other hand, oversensitive detection algorithms risk producing false alarms&#x2014;erroneously flagging users who are not in crisis. Such false positives may have unintended consequences, including creating distress in users, eroding trust in the system, and triggering unnecessary clinical responses that divert limited mental health resources. Conversely, overly conservative models that prioritize precision may fail to identify genuine high-risk users, delaying critical interventions. Current systems rarely incorporate contextual judgment, such as distinguishing between metaphorical expressions (eg, &#x201C;I can&#x2019;t take this anymore&#x201D;) and genuine crisis indicators, and often lack follow-up protocols for ambiguous cases. Therefore, future research must prioritize the development of calibrated, context-aware risk detection models, possibly through human-in-the-loop frameworks or personalized risk thresholds that adapt to users&#x2019; communication styles and mental health histories. Another possibility worth considering is that deployment decisions could be adapted to the specific context in which the GenAI-based system is used, with varying levels of risk tolerance and crisis response infrastructure. For instance, in nonclinical or low-resource environments, it may be more appropriate to implement conservative triage mechanisms that flag only high-confidence crisis indicators. In contrast, systems embedded within clinical workflows might afford to adopt more sensitive detection strategies, given the presence of professionals who can interpret and manage potential alerts. Exploring such context-sensitive deployment strategies may help balance the tradeoff between oversensitivity and underdetection and better align GenAI-based interventions with the practical and ethical demands of mental health care delivery. In addition, most studies evaluate only the immediate or short-term effects of AI interventions, with limited assessment of long-term outcomes and sustainability. Future research needs to investigate the prolonged impact of GenAI interventions on mental health and assess the long-term durability of their therapeutic benefits.</p><p class="abstract-paragraph">Third, GenAI is used to support clinicians and mental health professionals by assisting with tasks such as treatment planning, summarizing user data, and providing psychoeducation. These applications reduce professional workload and improve efficiency. However, studies [<span class="footers"><a class="citation-link" href="#ref86" rel="footnote">86</a></span>,<span class="footers"><a class="citation-link" href="#ref97" rel="footnote">97</a></span>] indicate that GenAI models may occasionally produce incorrect or even harmful advice in complex cases, posing a risk of misinforming users. Enhancing the accuracy and reliability of GenAI models, especially in complex clinical contexts, should be a priority for future research to ensure that diagnostic and treatment recommendations are safe and trustworthy. Moreover, effective integration of GenAI into clinical workflows to increase acceptance and willingness to adopt these tools among health care professionals remains an area for further investigation [<span class="footers"><a class="citation-link" href="#ref51" rel="footnote">51</a></span>,<span class="footers"><a class="citation-link" href="#ref89" rel="footnote">89</a></span>]. Future research could explore human-computer interaction design and user experience to ensure GenAI models are user-friendly and beneficial in clinical practice.</p><h4>Addressing Ethical Governance, Fairness, and Reporting Challenges</h4><p class="abstract-paragraph">In addition to application-specific findings, this review identified systemic challenges in how studies are designed, reported, and governed&#x2014;particularly concerning ethics, fairness, and methodological transparency.</p><p class="abstract-paragraph">Ethical governance remains underdeveloped across much of the literature. Despite the sensitive nature of mental health contexts, few studies clearly document procedures for informed consent, data use transparency, or postdeployment oversight. Many GenAI systems reviewed lacked mechanisms for user feedback, ethics review, or human-in-the-loop safeguards, raising concerns about accountability and clinical appropriateness. Moreover, the &#x201C;black box&#x201D; design of most models limits interpretability, complicating clinician supervision and user trust. Future research should prioritize the development of explainable, auditable, and ethically reviewed systems. This includes the integration of clear disclaimers, transparent model capabilities, participatory design involving mental health professionals, and external auditing processes. Broader structural reforms&#x2014;such as public registries for mental health&#x2013;related GenAI models and standardized ethics review frameworks&#x2014;are needed to ensure responsible deployment and user protection.</p><p class="abstract-paragraph">Fairness emerged as a particularly pressing and unresolved concern in GenAI-based mental health applications. Studies consistently report demographic disparities in model performance, with specific populations more susceptible to underdiagnosis or misclassification [<span class="footers"><a class="citation-link" href="#ref38" rel="footnote">38</a></span>,<span class="footers"><a class="citation-link" href="#ref56" rel="footnote">56</a></span>,<span class="footers"><a class="citation-link" href="#ref82" rel="footnote">82</a></span>]. Although mitigation techniques such as value-aligned data augmentation, demographic diversification, or model fine-tuning have been explored [<span class="footers"><a class="citation-link" href="#ref39" rel="footnote">39</a></span>,<span class="footers"><a class="citation-link" href="#ref79" rel="footnote">79</a></span>], their effectiveness remains limited and context-dependent. Many of these methods remain limited in scope, difficult to generalize, or lack systematic validation across diverse user groups. Moreover, the complexity of bias in mental health is compounded by overlapping factors such as language, culture, and social stigma&#x2014;dimensions that current fairness metrics often fail to capture. Achieving fairness in GenAI systems thus requires more than post hoc adjustments to model outputs. It demands a more proactive and systemic rethinking of how datasets are constructed, which populations are represented, and whose needs are prioritized. Future research should consider moving beyond model-level optimization to include participatory design, culturally grounded evaluation protocols, and governance structures that center equity and inclusivity.</p><p class="abstract-paragraph">Reporting quality also remains inconsistent. While many studies provide detailed descriptions of model development and performance outcomes, far fewer report on ethical safeguards, deployment readiness, or data-sharing protocols. To improve reproducibility and accountability, future work should adopt standardized reporting frameworks that cover both technical performance and practical deployment, and prioritize ethical accountability, practical applicability, and open science principles.</p><h4>Limitations and Future Research</h4><p class="abstract-paragraph">This review has several limitations. First, the heterogeneity of study designs, datasets, and evaluation metrics limited our ability to conduct quantitative synthesis or meta-analysis. Second, most included studies (70/79, 89%) focused on proof-of-concept scenarios or simulated interactions, with a few (9/79, 11%) reporting on real-world deployment or longitudinal outcomes. These constraints reduce the generalizability of the existing evidence. Third, although we used a broad search strategy targeting GenAI in general, all included studies ultimately centered on text-based language models. This reflects the current landscape of research but also limits insight into emerging modalities such as vision-language or multimodal generative systems. Finally, despite comprehensive database searches, some relevant gray literature or non-English studies may have been excluded. Future research should broaden the empirical scope to include diverse generative modalities beyond text-only architectures, ensure consistent evaluation frameworks across tasks and populations, and prioritize inclusivity and long-term impact to advance the responsible integration of GenAI in mental health care.</p><h4>Conclusions</h4><p class="abstract-paragraph">This systematic review summarizes the applications of GenAI in mental health, focusing on areas including diagnosis and assessment, therapeutic tools, and clinician support. Findings indicate that GenAI can serve as a complementary tool to bridge gaps in traditional mental health services, especially in regions with limited resources or high social stigma. However, ethical challenges&#x2014;including privacy, potential biases, user safety, and the need for stringent ethical governance&#x2014;are critical to address. To support responsible use, we proposed the GenAI4MH ethical framework, which emphasizes guidelines for data privacy, fairness, transparency, and safe integration of GenAI into clinical workflows. Future research should expand the applications of GenAI across diverse cultural and demographic contexts, further investigate the integration of multimodal data, and rigorously evaluate long-term impacts to ensure GenAI&#x2019;s sustainable, ethical, and effective role in mental health.</p></article><p><h4 class="h4-border-top">Acknowledgments</h4></p><p class="abstract-paragraph">This work is supported by the National Social Science Fund of China (grant 21BSH158) and the National Natural Science Foundation of China (grant 32271136).</p><h3>Data Availability</h3><p class="abstract-paragraph">Data sharing is not applicable to this article as no datasets were generated or analyzed during this study.</p><h4 class="h4-border-top">Authors' Contributions</h4><p><p class="abstract-paragraph">XW was responsible for data curation, formal analysis, investigation, methodology, and writing the original draft of the manuscript. YZ was responsible for conceptualization, investigation, methodology, project administration, visualization, and reviewing and editing of the manuscript. GZ was responsible for funding acquisition, resources, supervision, validation, and reviewing and editing of the manuscript.</p></p><h4 class="h4-border-top">Conflicts of Interest</h4><p><p class="abstract-paragraph">None declared.</p></p><div id="app1" name="app1">Multimedia Appendix 1<p class="abstract-paragraph">PRISMA Checklist.</p><a href="https://jmir.org/api/download?alt_name=mental_v12i1e70610_app1.pdf&amp;filename=179103d224277d642a5fb054b15693ca.pdf" target="_blank">PDF File  (Adobe PDF File), 659 KB</a></div><hr><div id="app2" name="app2">Multimedia Appendix 2<p class="abstract-paragraph">Detailed search strategy for study identification.</p><a href="https://jmir.org/api/download?alt_name=mental_v12i1e70610_app2.pdf&amp;filename=6a0153ce2162d75b40f48dbc41db656c.pdf" target="_blank">PDF File  (Adobe PDF File), 70 KB</a></div><hr><div id="app3" name="app3">Multimedia Appendix 3<p class="abstract-paragraph">Summary of the identified studies.</p><a href="https://jmir.org/api/download?alt_name=mental_v12i1e70610_app3.pdf&amp;filename=ed5855767a97fe0e65f0519fd35f4eff.pdf" target="_blank">PDF File  (Adobe PDF File), 417 KB</a></div><hr><div id="app4" name="app4">Multimedia Appendix 4<p class="abstract-paragraph">MI-CLAIM-GEN Checklist for generative AI clinical studies.</p><a href="https://jmir.org/api/download?alt_name=mental_v12i1e70610_app4.pdf&amp;filename=fe4d1829f06ae53d943ca31dd2b1b5da.pdf" target="_blank">PDF File  (Adobe PDF File), 51 KB</a></div><hr><div id="app5" name="app5">Multimedia Appendix 5<p class="abstract-paragraph">List of the included studies on the use of generative artificial intelligence for mental health diagnosis and assessment.</p><a href="https://jmir.org/api/download?alt_name=mental_v12i1e70610_app5.pdf&amp;filename=ec51afbfa23cca97499eaa91c123ef07.pdf" target="_blank">PDF File  (Adobe PDF File), 170 KB</a></div><hr><div id="app6" name="app6">Multimedia Appendix 6<p class="abstract-paragraph">Datasets used in generative artificial intelligence&#x2013;based mental health diagnosis and assessment.</p><a href="https://jmir.org/api/download?alt_name=mental_v12i1e70610_app6.pdf&amp;filename=2f264fc672bb8df6398e172c68f2a636.pdf" target="_blank">PDF File  (Adobe PDF File), 128 KB</a></div><hr><div class="footnotes"><h4 id="References" class="h4-border-top navigation-heading" data-label="References">References</h4><ol><li><span id="ref1">Mental disorders. World Health Organization.  URL: <a target="_blank" href="https://www.who.int/news-room/fact-sheets/detail/mental-disorders">https://www.who.int/news-room/fact-sheets/detail/mental-disorders</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref2">Danner M, Hadzic B, Gerhardt S, Ludwig S, Uslu I, Shao P. Advancing mental health diagnostics: GPT-based method for depression detection. 
                        In: Proceedings of the 62nd Annual Conference of the Society of Instrument and Control Engineers. 2023. Presented at: SICE '23; September 6-9, 2023:1290-1296; Tsu, Japan. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10354236">https://ieeexplore.ieee.org/document/10354236</a></span></li><li><span id="ref3">D'Souza RF, Amanullah S, Mathew M, Surapaneni KM. Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes. Asian J Psychiatr.  Nov 2023;89:103770. [<a target="_blank" href="https://dx.doi.org/10.1016/j.ajp.2023.103770">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37812998&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref4">ChatGPT. OpenAI.  URL: <a target="_blank" href="https://chat.openai.com">https://chat.openai.com</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref5">Sorin V, Brin D, Barash Y, Konen E, Charney A, Nadkarni G,  et al. Large language models and empathy: systematic review. J Med Internet Res.  Dec 11, 2024;26:e52597.  [<a href="https://www.jmir.org/2024//e52597/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/52597">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39661968&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref6">Lee YK, Suh J, Zhan H, Li JJ, Ong DC. Large language models produce responses perceived to be empathic. arXiv.  Preprint posted online on March 26, 2024.  [<a href="https://arxiv.org/abs/2403.18148" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1109/acii63134.2024.00012">CrossRef</a>]</span></li><li><span id="ref7">Yu H, McGuinness S. An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system. J Med Artif Intell.   2024;7:1-16.  [<a href="https://jmai.amegroups.org/article/view/8991/html" target="_blank">FREE Full text</a>]</span></li><li><span id="ref8">Najarro LA, Lee Y, Toshnazarov KE, Jang Y, Kim H, Noh Y. WMGPT: towards 24/7 online prime counseling with ChatGPT. 
                        In: Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing. 2023. Presented at: UbiComp/ISWC '23; October 8-12, 2023:142-145; Cancun, Mexico. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3594739.3610708">https://dl.acm.org/doi/10.1145/3594739.3610708</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3594739.3610708">CrossRef</a>]</span></li><li><span id="ref9">Wu Y, Mao K, Zhang Y, Chen J. CALLM: enhancing clinical interview analysis through data augmentation with large language models. IEEE J Biomed Health Inform.  Dec 2024;28(12):7531-7542. [<a target="_blank" href="https://dx.doi.org/10.1109/JBHI.2024.3435085">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39074002&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref10">Replika: the AI companion who cares. Luka Inc.  URL: <a target="_blank" href="https://replika.com">https://replika.com</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref11">Ma Z, Mei Y, Su Z. Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. AMIA Annu Symp Proc.   2023;2023:1105-1114.  [<a href="https://europepmc.org/abstract/MED/38222348" target="_blank">FREE Full text</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38222348&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref12">Bauer B, Norel R, Leow A, Rached ZA, Wen B, Cecchi G. Using large language models to understand suicidality in a social media-based taxonomy of mental health disorders: linguistic analysis of reddit posts. JMIR Ment Health.  May 16, 2024;11:e57234.  [<a href="https://mental.jmir.org/2024//e57234/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/57234">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38771256&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref13">Omar M, Levkovich I. Exploring the efficacy and potential of large language models for depression: a systematic review. J Affect Disord.  Feb 15, 2025;371:234-244. [<a target="_blank" href="https://dx.doi.org/10.1016/j.jad.2024.11.052">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39581383&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref14">Casu M, Triscari S, Battiato S, Guarnera L, Caponnetto P. AI chatbots for mental health: a scoping review of effectiveness, feasibility, and applications. Appl Sci.  Jul 05, 2024;14(13):5889. [<a target="_blank" href="https://dx.doi.org/10.3390/app14135889">CrossRef</a>]</span></li><li><span id="ref15">Lee QY, Chen M, Ong CW, Ho CS. The role of generative artificial intelligence in psychiatric education- a scoping review. BMC Med Educ.  Mar 25, 2025;25(1):438.  [<a href="https://bmcmededuc.biomedcentral.com/articles/10.1186/s12909-025-07026-9" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1186/s12909-025-07026-9">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=40133891&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref16">Luo X, Zhang A, Li Y, Zhang Z, Ying F, Lin R,  et al. Emergence of Artificial Intelligence Art Therapies (AIATs) in mental health care: a systematic review. Int J Ment Health Nurs.  Dec 17, 2024;33(6):1743-1760. [<a target="_blank" href="https://dx.doi.org/10.1111/inm.13384">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39020473&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref17">Xian X, Chang A, Xiang YT, Liu MT. Debate and dilemmas regarding generative AI in mental health care: scoping review. Interact J Med Res.  Aug 12, 2024;13:e53672.  [<a href="https://www.i-jmr.org/2024//e53672/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/53672">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39133916&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref18">Guo Z, Lai A, Thygesen JH, Farrington J, Keen T, Li K. Large language models for mental health applications: systematic review. JMIR Ment Health.  Oct 18, 2024;11:e57400.  [<a href="https://mental.jmir.org/2024//e57400/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/57400">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39423368&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref19">GPT-4o system card. OpenAI.  URL: <a target="_blank" href="https://openai.com/index/gpt-4o-system-card/">https://openai.com/index/gpt-4o-system-card/</a> [accessed 2025-04-09]
                        </span></li><li><span id="ref20">OpenAI o1 system card. OpenAI.  URL: <a target="_blank" href="https://openai.com/index/openai-o1-system-card/">https://openai.com/index/openai-o1-system-card/</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref21">Corrado G, Barral J. Advancing medical AI with Med-Gemini. Google Research.  URL: <a target="_blank" href="https://research.google/blog/advancing-medical-ai-with-med-gemini/">https://research.google/blog/advancing-medical-ai-with-med-gemini/</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref22">De Freitas J, Cohen IG. The health risks of generative AI-based wellness apps. Nat Med.  May 29, 2024;30(5):1269-1275. [<a target="_blank" href="https://dx.doi.org/10.1038/s41591-024-02943-6">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38684859&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref23">Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow C,  et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ.  Mar 29, 2021;372:n71.  [<a href="https://www.bmj.com/lookup/pmidlookup?view=long&amp;pmid=33782057" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1136/bmj.n71">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=33782057&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref24">Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M,  et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res.   2020;21(140):1-67.  [<a href="https://jmlr.org/papers/volume21/20-074/20-074.pdf" target="_blank">FREE Full text</a>]</span></li><li><span id="ref25">Devlin J, Chang MW, Lee K, Toutanova K. Pre-training of deep bidirectional transformers for language understanding. arXiv.  Preprint posted online on October 11, 2018.  [<a href="https://arxiv.org/abs/1810.04805" target="_blank">FREE Full text</a>]</span></li><li><span id="ref26">Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog.   2019.  URL: <a target="_blank" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https:/&#x200B;/cdn.&#x200B;openai.com/&#x200B;better-language-models/&#x200B;language_models_are_unsupervised_multitask_learners.&#x200B;pdf</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref27">Patel KK, Pal A, Saurav K, Jain P. Mental health detection using transformer BERT. In: Iyer SS, Jain A, Wang J, editors. Handbook of Research on Lifestyle Sustainability and Management Solutions Using AI, Big Data Analytics, and Visualization. New York, NY. IGI Global;  2022:91-108.</span></li><li><span id="ref28">Greco CM, Simeri A, Tagarelli A, Zumpano E. Transformer-based language models for mental health issues: a survey. Pattern Recogn Lett.  Mar 2023;167:204-211. [<a target="_blank" href="https://dx.doi.org/10.1016/j.patrec.2023.02.016">CrossRef</a>]</span></li><li><span id="ref29">Alhamed F, Ive J, Specia L. Using large language models (LLMs) to extract evidence from pre-annotated social media data. 
                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21-22, 2024:232-237; St. Julian's, Malta. URL: <a target="_blank" href="https://aclanthology.org/2024.clpsych-1.22.pdf">https://aclanthology.org/2024.clpsych-1.22.pdf</a></span></li><li><span id="ref30">Chen J, Nguyen V, Dai X, Molla D, Paris C, Karimi S. Exploring instructive prompts for large language models in the extraction of evidence for supporting assigned suicidal risk levels. 
                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:197-202; St. Julians, Malta. URL: <a target="_blank" href="https://aclanthology.org/2024.clpsych-1.17.pdf">https://aclanthology.org/2024.clpsych-1.17.pdf</a></span></li><li><span id="ref31">Singh LG, Mao J, Mutalik R, Middleton SE. Extracting and summarizing evidence of suicidal ideation in social media contents using large language models. 
                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:218-226; St. Julians, Malta.</span></li><li><span id="ref32">Stern W, Goh SJ, Nur N, Aragon PJ, Mercer T, Bhattacharyya S. Natural language explanations for suicide risk classification using large language models. 
                        In: Proceedings of the 2024 Machine Learning for Cognitive and Mental Health Workshop. 2024. Presented at: ML4CMH '24; February 26, 2024:1-10; Vancouver, BC. URL: <a target="_blank" href="https://ceur-ws.org/Vol-3649/Paper5.pdf">https://ceur-ws.org/Vol-3649/Paper5.pdf</a></span></li><li><span id="ref33">Uluslu AY, Michail A, Clematide S. Utilizing large language models to identify evidence of suicidality risk through analysis of emotionally charged posts. 
                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:264-269; St. Julians, MT. URL: <a target="_blank" href="https://aclanthology.org/2024.clpsych-1.26.pdf">https://aclanthology.org/2024.clpsych-1.26.pdf</a></span></li><li><span id="ref34">Elyoseph Z, Levkovich I. Beyond human expertise: the promise and limitations of ChatGPT in suicide risk assessment. Front Psychiatry.   2023;14:1213141.  [<a href="https://europepmc.org/abstract/MED/37593450" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.3389/fpsyt.2023.1213141">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37593450&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref35">Lee C, Mohebbi M, O'Callaghan E, Winsberg M. Large language models versus expert clinicians in crisis prediction among telemental health patients: comparative study. JMIR Ment Health.  Aug 02, 2024;11:e58129.  [<a href="https://mental.jmir.org/2024//e58129/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/58129">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38876484&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref36">Levkovich I, Elyoseph Z. Suicide risk assessments through the eyes of ChatGPT-3.5 versus ChatGPT-4: vignette study. JMIR Ment Health.  Sep 20, 2023;10:e51232.  [<a href="https://mental.jmir.org/2023//e51232/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/51232">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37728984&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref37">Shinan-Altman S, Elyoseph Z, Levkovich I. The impact of history of depression and access to weapons on suicide risk assessment: a comparison of ChatGPT-3.5 and ChatGPT-4. PeerJ.   2024;12:e17468.  [<a href="https://europepmc.org/abstract/MED/38827287" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.7717/peerj.17468">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38827287&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref38">Soun RS, Nair A. ChatGPT for mental health applications: a study on biases. 
                        In: Proceedings of the 3rd International Conference on AI-ML Systems. 2023. Presented at: AIMLSystems '23; October 25-28, 2023:1-5; Bangalore, India. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3639856.3639894">https://dl.acm.org/doi/10.1145/3639856.3639894</a></span></li><li><span id="ref39">Xu X, Yao B, Dong Y, Gabriel S, Yu H, Hendler J,  et al. Mental-LLM: leveraging large language models for mental health prediction via online text data. Proc ACM Interact Mob Wearable Ubiquitous Technol.  Mar 06, 2024;8(1):1-32. [<a target="_blank" href="https://dx.doi.org/10.1145/3643540">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39925940&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref40">Zhang T, Yang K, Ji S, Liu B, Xie Q, Ananiadou S. SuicidEmoji: derived emoji dataset and tasks for suicide-related social content. 
                        In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2024. Presented at: SIGIR '24; July 14-18, 2024:1136-1141; Washington, DC. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3626772.3657852">https://dl.acm.org/doi/10.1145/3626772.3657852</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3626772.3657852">CrossRef</a>]</span></li><li><span id="ref41">Zhou W, Prater LC, Goldstein EV, Mooney SJ. Identifying rare circumstances preceding female firearm suicides: validating a large language model approach. JMIR Ment Health.  Oct 17, 2023;10:e49359.  [<a href="https://mental.jmir.org/2023//e49359/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/49359">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37847549&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref42">Shin D, Kim H, Lee S, Cho Y, Jung W. Using large language models to detect depression from user-generated diary text data as a novel approach in digital mental health screening: instrument validation study. J Med Internet Res.  Sep 18, 2024;26:e54617.  [<a href="https://www.jmir.org/2024//e54617/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/54617">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39292502&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref43">Mazumdar H, Chakraborty C, Sathvik M, Mukhopadhyay S, Panigrahi PK. GPTFX: a novel GPT-3 based framework for mental health detection and explanations. IEEE J Biomed Health Inform.   2023;3:1-8. [<a target="_blank" href="https://dx.doi.org/10.1109/jbhi.2023.3328350">CrossRef</a>]</span></li><li><span id="ref44">Hayati MF, Ali MA, Rosli AN. Depression detection on Malay dialects using GPT-3. 
                        In: Proceedings of the 2022 IEEE-EMBS Conference on Biomedical Engineering and Sciences. 2022. Presented at: IECBES '22; December 7-9, 2022:360-364; Kuala Lumpur, Malaysia. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10079554">https://ieeexplore.ieee.org/document/10079554</a> [<a target="_blank" href="https://dx.doi.org/10.1109/iecbes54088.2022.10079554">CrossRef</a>]</span></li><li><span id="ref45">Tao Y, Yang M, Shen H, Yang Z, Weng Z, Hu B. Classifying anxiety and depression through LLMs virtual interactions: a case study with ChatGPT. 
                        In: Proceedings of the 2023 IEEE International Conference on Bioinformatics and Biomedicine. 2023. Presented at: BIBM '23; December 5-8, 2023:2259-2264; Istanbul, Turkiye. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10385305">https://ieeexplore.ieee.org/document/10385305</a> [<a target="_blank" href="https://dx.doi.org/10.1109/bibm58861.2023.10385305">CrossRef</a>]</span></li><li><span id="ref46">Hu Y, Zhang S, Dang T, Jia H, Salim F, Hu W,  et al. Exploring large-scale language models to evaluate EEG-based multimodal data for mental health. 
                        In: Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing. 2024. Presented at: UbiComp '24; October 5-9, 2024:412-417; Melbourne, Australia. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3675094.3678494">https://dl.acm.org/doi/10.1145/3675094.3678494</a></span></li><li><span id="ref47">Englhardt Z, Ma C, Morris ME, Chang C, Xu X, Qin L,  et al. From classification to clinical insights: towards analyzing and reasoning about mobile and behavioral health data with large language models. Proc ACM Interact Mob Wearable Ubiquitous Technol.  May 15, 2024;8(2):1-25. [<a target="_blank" href="https://dx.doi.org/10.1145/3659604">CrossRef</a>]</span></li><li><span id="ref48">Li W, Zhu Y, Lin X, Li M, Jiang Z, Zeng Z. Zero-shot explainable mental health analysis on social media by incorporating mental scales. 
                        In: Proceedings of the 2024 Companion Conference on ACM Web. 2024. Presented at: WWW '24; May 13-17, 2024:959-962; Singapore, Singapore. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3589335.3651584">https://dl.acm.org/doi/10.1145/3589335.3651584</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3589335.3651584">CrossRef</a>]</span></li><li><span id="ref49">Zhang T, Teng S, Jia H, D'Alfonso S. Leveraging LLMs to predict affective states via smartphone sensor features. 
                        In: Proceedings of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing. 2024. Presented at: UbiComp '24; October 5-9, 2024:709-716; Melbourne, Australia. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3675094.3678420">https://dl.acm.org/doi/10.1145/3675094.3678420</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3675094.3678420">CrossRef</a>]</span></li><li><span id="ref50">Ni Y, Chen Y, Ding R, Ni S. Beatrice: a chatbot for collecting psychoecological data and providing QA capabilities. 
                        In: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. 2023. Presented at: PETRA '23; July 5-7, 2023:429-435; Corfu, Greece. URL: <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3594806.3596580">https://dl.acm.org/doi/abs/10.1145/3594806.3596580</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3594806.3596580">CrossRef</a>]</span></li><li><span id="ref51">Kim J, Leonte KG, Chen ML, Torous JB, Linos E, Pinto A,  et al. Large language models outperform mental and medical health care professionals in identifying obsessive-compulsive disorder. NPJ Digit Med.  Jul 19, 2024;7(1):193.  [<a href="https://doi.org/10.1038/s41746-024-01181-x" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1038/s41746-024-01181-x">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39030292&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref52">Pugh SL, Chandler C, Cohen AS, Diaz-Asper C, Elvev&#xE5;g B, Foltz PW. Assessing dimensions of thought disorder with large language models: the tradeoff of accuracy and consistency. Psychiatry Res.  Nov 2024;341:116119.  [<a href="https://linkinghub.elsevier.com/retrieve/pii/S0165-1781(24)00404-9" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1016/j.psychres.2024.116119">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39226873&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref53">Radwan A, Amarneh M, Alawneh H, Ashqar HI, AlSobeh A. Predictive analytics in mental health leveraging LLM embeddings and machine learning models for social media analysis. Int J Web Serv Res.   2024;21(1):1-22.  [<a href="https://www.igi-global.com/article/predictive-analytics-in-mental-health-leveraging-llm-embeddings-and-machine-learning-models-for-social-media-analysis/338222" target="_blank">FREE Full text</a>]</span></li><li><span id="ref54">Saleem M, Kim J. Intent aware data augmentation by leveraging generative AI for stress detection in social media texts. Peer J Comput Sci.   2024;10:e2156. [<a target="_blank" href="https://dx.doi.org/10.7717/peerj-cs.2156">CrossRef</a>]</span></li><li><span id="ref55">Gargari OK, Fatehi F, Mohammadi I, Firouzabadi SR, Shafiee A, Habibi G. Diagnostic accuracy of large language models in psychiatry. Asian J Psychiatr.  Oct 2024;100:104168. [<a target="_blank" href="https://dx.doi.org/10.1016/j.ajp.2024.104168">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39111087&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref56">Heinz MV, Bhattacharya S, Trudeau B, Quist R, Song SH, Lee CM,  et al. Testing domain knowledge and risk of bias of a large-scale general artificial intelligence model in mental health. Digit Health.  Apr 17, 2023;9:20552076231170499.  [<a href="https://journals.sagepub.com/doi/10.1177/20552076231170499?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1177/20552076231170499">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37101589&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref57">Ohse J, Had&#x17E;i&#x107; B, Mohammed P, Peperkorn N, Danner M, Yorita A,  et al. Zero-shot strike: testing the generalisation capabilities of out-of-the-box LLM models for depression detection. Comput Speech Lang.  Nov 2024;88:101663. [<a target="_blank" href="https://dx.doi.org/10.1016/j.csl.2024.101663">CrossRef</a>]</span></li><li><span id="ref58">Yang K, Zhang T, Kuang Z, Xie Q, Huang J, Ananiadou S. MentaLLaMA: interpretable mental health analysis on social media with large language models. 
                        In: Proceedings of the 2024 ACM Web Conference. 2024. Presented at: WWW '24; May 13-17, 2024:4489-4500; Singapore, Singapore. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3589334.3648137">https://dl.acm.org/doi/10.1145/3589334.3648137</a></span></li><li><span id="ref59">Zhu J, Xu A, Tan M, Yang M. XinHai@CLPsych 2024 shared task: prompting healthcare-oriented LLMs for evidence highlighting in posts with suicide risk. 
                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:238-246; St. Julians, Malta. URL: <a target="_blank" href="https://aclanthology.org/2024.clpsych-1.23.pdf">https://aclanthology.org/2024.clpsych-1.23.pdf</a></span></li><li><span id="ref60">Singh A, Ehtesham A, Mahmud S, Kim JH. Revolutionizing mental health care through LangChain: a journey with a large language model. 
                        In: Proceedings of the 14th Annual Computing and Communication Workshop and Conference. 2024. Presented at: CCWC '24; January 8-10, 2024:73-78; Las Vegas, NV. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10427865">https://ieeexplore.ieee.org/document/10427865</a> [<a target="_blank" href="https://dx.doi.org/10.1109/ccwc60891.2024.10427865">CrossRef</a>]</span></li><li><span id="ref61">Chen Z, Deng J, Zhou J, Wu J, Qian T, Huang M. Depression detection in clinical interviews with LLM-empowered structural element graph. 
                        In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2024. Presented at: NAACL-HLT '24; June 16-21, 2024:8181-8194; Mexico City, Mexico. URL: <a target="_blank" href="https://aclanthology.org/2024.naacl-long.452.pdf">https://aclanthology.org/2024.naacl-long.452.pdf</a> [<a target="_blank" href="https://dx.doi.org/10.18653/v1/2024.naacl-long.452">CrossRef</a>]</span></li><li><span id="ref62">Hur JK, Heffner J, Feng GW, Joormann J, Rutledge RB. Language sentiment predicts changes in depressive symptoms. Proc Natl Acad Sci U S A.  Sep 24, 2024;121(39):e2321321121.  [<a href="https://www.pnas.org/doi/abs/10.1073/pnas.2321321121?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1073/pnas.2321321121">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39284070&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref63">Bird JJ, Wright D, Sumich A, Lotfi A. Generative AI in psychological therapy: perspectives on computational linguistics and large language models in written behaviour monitoring. 
                        In: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive EnvironmentsProceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments. 2024. Presented at: PETRA '24; June 26-28, 2024:322-328; Crete, Greece. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3652037.3663893">https://dl.acm.org/doi/10.1145/3652037.3663893</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3652037.3663893">CrossRef</a>]</span></li><li><span id="ref64">Alanezi F. Assessing the effectiveness of ChatGPT in delivering mental health support: a qualitative study. J Multidiscip Healthc.   2024;17:461-471.  [<a href="https://www.tandfonline.com/doi/abs/10.2147/JMDH.S447368?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2147/JMDH.S447368">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38314011&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref65">Ma Z, Mei Y, Long Y, Su Z, Gajos KZ. Evaluating the experience of LGBTQ+ people using large language model based chatbots for mental health support. 
                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-15; Honolulu, HI. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3613904.3642482">https://dl.acm.org/doi/10.1145/3613904.3642482</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3613904.3642482">CrossRef</a>]</span></li><li><span id="ref66">Vakayil S, Juliet DS, Vakayil S. RAG-based LLM chatbot using Llama-2. 
                        In: Proceedings of the 7th International Conference on Devices, Circuits and Systems. 2024. Presented at: ICDCS '24; April 23-24, 2024:1-5; Coimbatore, India. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10561020">https://ieeexplore.ieee.org/document/10561020</a> [<a target="_blank" href="https://dx.doi.org/10.1109/icdcs59278.2024.10561020">CrossRef</a>]</span></li><li><span id="ref67">Berrezueta-Guzman S, Kandil M, Mart&#xED;n-Ruiz ML, Pau de la Cruz I, Krusche S. Future of ADHD care: evaluating the efficacy of ChatGPT in therapy enhancement. Healthcare (Basel).  Mar 19, 2024;12(6):33.  [<a href="https://www.mdpi.com/resolver?pii=healthcare12060683" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.3390/healthcare12060683">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38540647&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref68">Alessa A, Al-Khalifa H. Towards designing a ChatGPT conversational companion for elderly people. 
                        In: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. 2023. Presented at: PETRA '23; July 5-7, 2023:667-674; Corfu, Greece. URL: <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3594806.3596572">https://dl.acm.org/doi/abs/10.1145/3594806.3596572</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3594806.3596572">CrossRef</a>]</span></li><li><span id="ref69">Wu R, Yu C, Pan X, Liu Y, Zhang N, Fu Y,  et al. MindShift: leveraging large language models for mental-states-based problematic smartphone use intervention. 
                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-24; Honolulu, HI. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3613904.3642790">https://dl.acm.org/doi/10.1145/3613904.3642790</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3613904.3642790">CrossRef</a>]</span></li><li><span id="ref70">Yahagi M, Hiruta R, Miyauchi C, Tanaka S, Taguchi A, Yaguchi Y. Comparison of conventional anesthesia nurse education and an artificial intelligence chatbot (ChatGPT) intervention on preoperative anxiety: a randomized controlled trial. J Perianesth Nurs.  Oct 2024;39(5):767-771. [<a target="_blank" href="https://dx.doi.org/10.1016/j.jopan.2023.12.005">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38520470&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref71">Vowels LM, Francois-Walcott RR, Darwiche J. AI in relationship counselling: evaluating ChatGPT's therapeutic capabilities in providing relationship advice. Comput Human Behav.  Aug 2024;2(2):100078. [<a target="_blank" href="https://dx.doi.org/10.1016/j.chbah.2024.100078">CrossRef</a>]</span></li><li><span id="ref72">Brocki L, Dyer GC, G'adka A, Chung NC. Deep learning mental health dialogue system. 
                        In: Proceedings of the 2023 IEEE International Conference on Big Data and Smart Computing. 2023. Presented at: BigComp '23; February 13-16, 2023:395-398; Jeju, Republic of Korea. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10066740">https://ieeexplore.ieee.org/document/10066740</a> [<a target="_blank" href="https://dx.doi.org/10.1109/bigcomp57234.2023.00097">CrossRef</a>]</span></li><li><span id="ref73">Sharma A, Rushton K, Lin IW, Nguyen T, Althoff T. Facilitating self-guided mental health interventions through human-language model interaction: a case study of cognitive restructuring. 
                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-29; Honolulu, HI. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3613904.3642761">https://dl.acm.org/doi/10.1145/3613904.3642761</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3613904.3642761">CrossRef</a>]</span></li><li><span id="ref74">Dongre P. Physiology-driven empathic large language models (EmLLMs) for mental health support. 
                        In: Proceedings of the 2024 Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI EA '24; May 11-16, 2024:1-5; Honolulu, HI. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3613905.3651132">https://dl.acm.org/doi/10.1145/3613905.3651132</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3613905.3651132">CrossRef</a>]</span></li><li><span id="ref75">Kumar H, Wang Y, Shi J, Musabirov I, Farb N, Williams J. Exploring the use of large language models for improving the awareness of mindfulness. 
                        In: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. 2023. Presented at: CHI EA '23; April 23-28, 2023:1-7; Hamburg, Germany. URL: <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3544549.3585614">https://dl.acm.org/doi/full/10.1145/3544549.3585614</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3544549.3585614">CrossRef</a>]</span></li><li><span id="ref76">Heston TF. Safety of large language models in addressing depression. Cureus.  Dec 2023;15(12):e50729.  [<a href="https://europepmc.org/abstract/MED/38111813" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.7759/cureus.50729">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38111813&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref77">Herencia L&#xF3;pez-Menchero A. Analysis of the transformer architecture and application on a large language model for mental health counseling. Polytechnic University of Madrid.  URL: <a target="_blank" href="https://docta.ucm.es/rest/api/core/bitstreams/30effe66-9f5a-404e-9b31-ba3e8d555268/content">https://docta.ucm.es/rest/api/core/bitstreams/30effe66-9f5a-404e-9b31-ba3e8d555268/content</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref78">Bird JJ, Lotfi A. Generative transformer chatbots for mental health support: a study on depression and anxiety. 
                        In: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. 2023. Presented at: PETRA '23; July 5-7, 2023:475-479; Corfu, Greece. URL: <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3594806.3596520">https://dl.acm.org/doi/abs/10.1145/3594806.3596520</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3594806.3596520">CrossRef</a>]</span></li><li><span id="ref79">De&#xA0;Freitas J, U&#x11F;uralp AK, O&#x11F;uz&#x2010;U&#x11F;uralp Z, Puntoni S. Chatbots and mental health: insights into the safety of generative AI. J Consum Psychol.  Dec 19, 2023;34(3):481-491. [<a target="_blank" href="https://dx.doi.org/10.1002/jcpy.1393">CrossRef</a>]</span></li><li><span id="ref80">Dergaa I, Fekih-Romdhane F, Hallit S, Loch AA, Glenn JM, Fessi MS,  et al. ChatGPT is not ready yet for use in providing mental health assessment and interventions. Front Psychiatry.  Jan 4, 2023;14:1277756.  [<a href="https://europepmc.org/abstract/MED/38239905" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.3389/fpsyt.2023.1277756">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38239905&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref81">Levkovich I, Elyoseph Z. Identifying depression and its determinants upon initiating treatment: ChatGPT versus primary care physicians. Fam Med Community Health.  Sep 2023;11(4):e357.  [<a href="https://fmch.bmj.com/lookup/pmidlookup?view=long&amp;pmid=37844967" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1136/fmch-2023-002391">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37844967&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref82">Perlis RH, Goldberg JF, Ostacher MJ, Schneck CD. Clinical decision support for bipolar depression using large language models. Neuropsychopharmacology.  Aug 13, 2024;49(9):1412-1416. [<a target="_blank" href="https://dx.doi.org/10.1038/s41386-024-01841-2">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38480911&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref83">Hwang G, Lee DY, Seol S, Jung J, Choi Y, Her ES,  et al. Assessing the potential of ChatGPT for psychodynamic formulations in psychiatry: an exploratory study. Psychiatry Res.  Jan 2024;331:115655. [<a target="_blank" href="https://dx.doi.org/10.1016/j.psychres.2023.115655">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38056130&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref84">Hsieh LH, Liao WC, Liu EY. Feasibility assessment of using ChatGPT for training case conceptualization skills in psychological counseling. Comput Human Behav.  Aug 2024;2(2):100083. [<a target="_blank" href="https://dx.doi.org/10.1016/j.chbah.2024.100083">CrossRef</a>]</span></li><li><span id="ref85">Hadar-Shoval D, Elyoseph Z, Lvovsky M. The plasticity of ChatGPT's mentalizing abilities: personalization for personality structures. Front Psychiatry.   2023;14:1234397.  [<a href="https://europepmc.org/abstract/MED/37720897" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.3389/fpsyt.2023.1234397">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37720897&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref86">Elyoseph Z, Levkovich I. Comparing the perspectives of generative AI, mental health experts, and the general public on schizophrenia recovery: case vignette study. JMIR Ment Health.  Mar 18, 2024;11:e53043.  [<a href="https://mental.jmir.org/2024//e53043/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/53043">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38533615&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref87">Elyoseph Z, Levkovich I, Shinan-Altman S. Assessing prognosis in depression: comparing perspectives of AI models, mental health professionals and the general public. Fam Med Community Health.  Jan 09, 2024;12(Suppl 1):33.  [<a href="https://fmch.bmj.com/lookup/pmidlookup?view=long&amp;pmid=38199604" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1136/fmch-2023-002583">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38199604&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref88">Adhikary PK, Srivastava A, Kumar S, Singh SM, Manuja P, Gopinath JK,  et al. Exploring the efficacy of large language models in summarizing mental health counseling sessions: benchmark study. JMIR Ment Health.  Jul 23, 2024;11:e57306.  [<a href="https://mental.jmir.org/2024//e57306/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/57306">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39042893&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref89">James LJ, Genga L, Montagne B, Hagenaars M, Van G. Caregiver's evaluation of LLM-generated treatment goals for patients with severe mental illnesses. 
                        In: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments. 2024. Presented at: PETRA '24; June 26-28, 2024:187-190; Crete, Greece. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3652037.3663955">https://dl.acm.org/doi/10.1145/3652037.3663955</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3652037.3663955">CrossRef</a>]</span></li><li><span id="ref90">Kim T, Bae S, Kim HA, Lee SW, Hong H, Yang C,  et al. MindfulDiary: harnessing large language model to support psychiatric patients' journaling. 
                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-20; Honolulu, HI. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3613904.3642937">https://dl.acm.org/doi/10.1145/3613904.3642937</a></span></li><li><span id="ref91">Furukawa TA, Iwata S, Horikoshi M, Sakata M, Toyomoto R, Luo Y,  et al. Harnessing AI to optimize thought records and facilitate cognitive restructuring in smartphone CBT: an exploratory study. Cogn Ther Res.  Jul 07, 2023;47(6):887-893. [<a target="_blank" href="https://dx.doi.org/10.1007/s10608-023-10411-7">CrossRef</a>]</span></li><li><span id="ref92">Hodson N, Williamson S. Can large language models replace therapists? Evaluating performance at simple cognitive behavioral therapy tasks. JMIR AI.  Jul 30, 2024;3:e52500.  [<a href="https://ai.jmir.org/2024//e52500/" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.2196/52500">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39078696&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref93">Meyer S, Elsweiler D. "You tell me": a dataset of GPT-4-based behaviour change support conversations. 
                        In: Proceedings of the 2024 Conference on Human Information Interaction and Retrieval. 2024. Presented at: CHIIR '24; March 10-14, 2024:411-416; Sheffield, UK. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3627508.3638330">https://dl.acm.org/doi/10.1145/3627508.3638330</a></span></li><li><span id="ref94">Maurya RK, Montesinos S, Bogomaz M, DeDiego AC. Assessing the use of ChatGPT as a psychoeducational tool for mental health practice. Couns Psychother Res.  Apr 25, 2024;25(1):94-100. [<a target="_blank" href="https://dx.doi.org/10.1002/capr.12759">CrossRef</a>]</span></li><li><span id="ref95">Hedderich MA, Bazarova NN, Zou W, Shim R, Ma X, Yang Q. A piece of theatre: investigating how teachers design LLM chatbots to assist adolescent cyberbullying education. 
                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-17; Honolulu, HI. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3613904.3642379">https://dl.acm.org/doi/10.1145/3613904.3642379</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3613904.3642379">CrossRef</a>]</span></li><li><span id="ref96">Hu Z, Hou H, Ni S. Grow with your AI buddy: designing an LLMs-based conversational agent for the measurement and cultivation of children's mental resilience. 
                        In: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference. 2024. Presented at: IDC '24; June 17-20, 2024:811-817; Delft, Netherlands. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3628516.3659399">https://dl.acm.org/doi/10.1145/3628516.3659399</a></span></li><li><span id="ref97">Giorgi S, Isman K, Liu T, Fried Z, Sedoc J, Curtis B. Evaluating generative AI responses to real-world drug-related questions. Psychiatry Res.  Sep 2024;339:116058.  [<a href="https://linkinghub.elsevier.com/retrieve/pii/S0165-1781(24)00343-3" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1016/j.psychres.2024.116058">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39059040&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref98">Liu Y, Ding X, Peng S, Zhang C. Leveraging ChatGPT to optimize depression intervention through explainable deep learning. Front Psychiatry.  Jun 6, 2024;15:1383648.  [<a href="https://europepmc.org/abstract/MED/38903640" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.3389/fpsyt.2024.1383648">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38903640&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref99">Smith A, Hachen S, Schleifer R, Bhugra D, Buadze A, Liebrenz M. Old dog, new tricks? Exploring the potential functionalities of ChatGPT in supporting educational methods in social psychiatry. Int J Soc Psychiatry.  Dec 2023;69(8):1882-1889. [<a target="_blank" href="https://dx.doi.org/10.1177/00207640231178451">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37392000&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref100">Wang X, Liu K, Wang C. Knowledge-enhanced pre-training large language model for depression diagnosis and treatment. 
                        In: Proceedings of the 9th International Conference on Cloud Computing and Intelligent Systems. 2023. Presented at: CCIS '23; August 12-13, 2023:532-536; Dali, China. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/10263217">https://ieeexplore.ieee.org/document/10263217</a></span></li><li><span id="ref101">Miao BY, Chen IY, Williams CY, Davidson J, Garcia-Agundez A, Sun S,  et al. The MI-CLAIM-GEN checklist for generative artificial intelligence in health. Nat Med.  May 06, 2025;31(5):1394-1398.  [<a href="https://escholarship.org/uc/item/qt1c31t56r" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1038/s41591-024-03470-0">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39915678&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref102">Santos WM, Secoli SR, P&#xFC;schel VA. The Joanna Briggs Institute approach for systematic reviews. Rev Lat Am Enfermagem.  Nov 14, 2018;26:e3074.  [<a href="https://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0104-11692018000100701&amp;lng=en&amp;nrm=iso&amp;tlng=en" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1590/1518-8345.2885.3074">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=30462787&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref103">Zhu W, Huang L, Zhou X, Li X, Shi G, Ying J,  et al. Could AI ethical anxiety, perceived ethical risks and ethical awareness about AI influence university students&#x2019; use of generative AI products? An ethical perspective. Int J Hum Comput Interact.  Mar 08, 2024;41(1):742-764. [<a target="_blank" href="https://dx.doi.org/10.1080/10447318.2024.2323277">CrossRef</a>]</span></li><li><span id="ref104">Brown A. GPT-4 is OpenAI&#x2019;s most advanced system, producing safer and more useful responses. Int J Archit Comput.  Sep 09, 2024;22(3):275-276. [<a target="_blank" href="https://dx.doi.org/10.1177/14780771241280148">CrossRef</a>]</span></li><li><span id="ref105">Wright SN, Anticevic A. Generative AI for precision neuroimaging biomarker development in psychiatry. Psychiatry Res.  Sep 2024;339:115955. [<a target="_blank" href="https://dx.doi.org/10.1016/j.psychres.2024.115955">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38909415&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref106">Barnhill JW. DSM-5 Clinical Cases. Washington, DC. American Psychiatric Publishing;  2013. </span></li><li><span id="ref107">Gouniai JM, Smith KD, Leonte KG. Do clergy recognize and respond appropriately to the many themes in obsessive-compulsive disorder?: data from a Pacific Island community. Ment Health Relig Cult.  Jan 20, 2022;25(1):33-46. [<a target="_blank" href="https://dx.doi.org/10.1080/13674676.2021.2010037">CrossRef</a>]</span></li><li><span id="ref108">Levi-Belz Y, Gamliel E. The effect of perceived burdensomeness and thwarted belongingness on therapists' assessment of patients' suicide risk. Psychother Res.  Jul 09, 2016;26(4):436-445. [<a target="_blank" href="https://dx.doi.org/10.1080/10503307.2015.1013161">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=25751580&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref109">Garg M, Saxena C, Krishnan V, Joshi R, Saha S, Mago V. CAMS: an annotated corpus for causal analysis of mental health issues in social media posts. arXiv.  Preprint posted online on July 11, 2022.  [<a href="https://arxiv.org/abs/2207.04674" target="_blank">FREE Full text</a>]</span></li><li><span id="ref110">Coppersmith G, Dredze M, Harman C, Hollingshead K, Mitchell M. CLPsych 2015 shared task: depression and PTSD on Twitter. 
                        In: Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality. 2015. Presented at: CLPsych '15; June 5, 2015:31-39; Denver, CO. URL: <a target="_blank" href="https://aclanthology.org/W15-1204.pdf">https://aclanthology.org/W15-1204.pdf</a> [<a target="_blank" href="https://dx.doi.org/10.3115/v1/w15-1204">CrossRef</a>]</span></li><li><span id="ref111">Chim J, Tsakalidis A, Gkoumas D, Atzil-Slonim D, Ophir Y, Zirikly A,  et al. Overview of the clpsych 2024 shared task: leveraging large language models to identify evidence of suicidality risk in online posts. 
                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:177-190; St. Julians, Malta. URL: <a target="_blank" href="https://aclanthology.org/2024.clpsych-1.15.pdf">https://aclanthology.org/2024.clpsych-1.15.pdf</a></span></li><li><span id="ref112">Gaur M, Alambo A, Sain JP, Kursuncu U, Thirunarayan K, Kavuluru R,  et al. Knowledge-aware assessment of severity of suicide risk for early intervention. 
                        In: Proceedings of the 2019 International Conference on the World Wide Web. 2019. Presented at: WWW '19; May 13-17, 2019:514-525; San Francisco, CA. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3308558.3313698">https://dl.acm.org/doi/10.1145/3308558.3313698</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3308558.3313698">CrossRef</a>]</span></li><li><span id="ref113">Pirina I, &#xC7;&#xF6;ltekin C. Identifying depression on reddit: the effect of training data. 
                        In: Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task. 2018. Presented at: EMNLP '18; October 31, 2018:9-12; Brussels, Belgium. URL: <a target="_blank" href="https://aclanthology.org/W18-5903.pdf">https://aclanthology.org/W18-5903.pdf</a> [<a target="_blank" href="https://dx.doi.org/10.18653/v1/w18-59">CrossRef</a>]</span></li><li><span id="ref114">Naseem U, Dunn AG, Kim J, Khushi M. Early identification of depression severity levels on reddit using ordinal classification. 
                        In: Proceedings of the 2022 International Conference on World Wide Web. 2022. Presented at: WWW '22; April 25-29, 2022:2563-2572; Virtual Event. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3485447.3512128">https://dl.acm.org/doi/10.1145/3485447.3512128</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3485447.3512128">CrossRef</a>]</span></li><li><span id="ref115">Turcan E, McKeown K. Dreaddit: a reddit dataset for stress analysis in social media. arXiv.  Preprint posted online on October 31, 2019.  [<a href="https://arxiv.org/abs/1911.00133" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.18653/v1/d19-6213">CrossRef</a>]</span></li><li><span id="ref116">Garg M, Shahbandegan A, Chadha A, Mago V. An annotated dataset for explainable interpersonal risk factors of mental disturbance in social media posts. arXiv.  Preprint posted online on May 30, 2023.  [<a href="https://arxiv.org/abs/2305.18727" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.18653/v1/2023.findings-acl.757">CrossRef</a>]</span></li><li><span id="ref117">Sampath K, Durairaj T. Data set creation and empirical analysis for detecting signs of depression from social media postings. 
                        In: Proceedings of the 5th IFIP TC 12 International Conference on Computational Intelligence in Data Science. 2022. Presented at: ICCIDS '22; March 24-26, 2022:136-151; Virtual Event. URL: <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-16364-7_11">https://link.springer.com/chapter/10.1007/978-3-031-16364-7_11</a> [<a target="_blank" href="https://dx.doi.org/10.1007/978-3-031-16364-7_11">CrossRef</a>]</span></li><li><span id="ref118">Haque A, Reddi V, Giallanza T. Deep learning for suicide and depression identification with unsupervised label correction. 
                        In: Proceedings of the 30th International Conference on Artificial Neural Networks on Artificial Neural Networks and Machine Learning. 2021. Presented at: ICANN '21; September 14-17, 2021:436-447; Bratislava, Slovakia. URL: <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-86383-8_35">https://link.springer.com/chapter/10.1007/978-3-030-86383-8_35</a> [<a target="_blank" href="https://dx.doi.org/10.1007/978-3-030-86383-8_35">CrossRef</a>]</span></li><li><span id="ref119">Ji S, Li X, Huang Z, Cambria E. Suicidal ideation and mental disorder detection with attentive relation networks. Neural Comput Appl.  Jun 24, 2021;34(13):10309-10319. [<a target="_blank" href="https://dx.doi.org/10.1007/s00521-021-06208-y">CrossRef</a>]</span></li><li><span id="ref120">Gui T, Zhu L, Zhang Q, Peng M, Zhou X, Ding K,  et al. Cooperative multimodal approach to depression detection in Twitter. AAAI Conf Artif Intell.  Jul 17, 2019;33(01):110-117. [<a target="_blank" href="https://dx.doi.org/10.1609/aaai.v33i01.3301110">CrossRef</a>]</span></li><li><span id="ref121">Shing HC, Nair S, Zirikly A, Friedenberg M, Daum&#xE9; IH, Resnik P. Expert, crowdsourced, and machine assessment of suicide risk via online postings. 
                        In: Proceedings of the 5th Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic. 2018. Presented at: CLPsych '18; June 5, 2018:25-36; New Orleans, LA. URL: <a target="_blank" href="https://aclanthology.org/W18-0603.pdf">https://aclanthology.org/W18-0603.pdf</a> [<a target="_blank" href="https://dx.doi.org/10.18653/v1/w18-0603">CrossRef</a>]</span></li><li><span id="ref122">Zirikly A, Resnik P, Uzuner O, Hollingshead K. CLPsych 2019 shared task: predicting the degree of suicide risk in reddit posts. 
                        In: Proceedings of the 6th Workshop on Computational Linguistics and Clinical Psychology. 2019. Presented at: CLPsych '19; June 6, 2019:24-33; Minneapolis, MN. URL: <a target="_blank" href="https://aclanthology.org/W19-3003.pdf">https://aclanthology.org/W19-3003.pdf</a> [<a target="_blank" href="https://dx.doi.org/10.18653/v1/w19-3003">CrossRef</a>]</span></li><li><span id="ref123">Wang Y, Wang Z, Li C, Zhang Y, Wang H. A multitask deep learning approach for user depression detection on Sina Weibo. arXiv.  Preprint posted online on August 26, 2020.  [<a href="https://arxiv.org/abs/2008.11708" target="_blank">FREE Full text</a>]</span></li><li><span id="ref124">Elvev&#xE5;g B, Foltz PW, Weinberger DR, Goldberg TE. Quantifying incoherence in speech: an automated methodology and novel application to schizophrenia. Schizophr Res.  Jul 2007;93(1-3):304-316.  [<a href="https://europepmc.org/abstract/MED/17433866" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1016/j.schres.2007.03.001">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=17433866&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref125">Gratch J, Artstein R, Lucas GM, Stratou G, Scherer S, Nazarian A,  et al. The distress analysis interview corpus of human and computer interviews. 
                        In: Proceedings of the 9th International Conference on Language Resources and Evaluation. 2014. Presented at: LREC '14; May 26-31, 2014:3123-3128; Reykjavik, Iceland. URL: <a target="_blank" href="https://aclanthology.org/L14-1421/">https://aclanthology.org/L14-1421/</a></span></li><li><span id="ref126">Shen Y, Yang H, Lin L. Automatic depression detection: an emotional audio-textual corpus and a Gru/Bilstm-based model. 
                        In: Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing. 2022. Presented at: ICASSP '22; May 23-27, 2022:6247-6251; Singapore, Singapore. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/9746569">https://ieeexplore.ieee.org/document/9746569</a> [<a target="_blank" href="https://dx.doi.org/10.1109/icassp43922.2022.9746569">CrossRef</a>]</span></li><li><span id="ref127">DeVault D, Artstein R, Benn G, Dey T, Fast E, Gainer A,  et al. SimSensei kiosk: a virtual human interviewer for healthcare decision support. 
                        In: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. 2014. Presented at: AAMAS '14; May 5-9, 2014:1061-1068; Paris, France. URL: <a target="_blank" href="https://dl.acm.org/doi/10.5555/2615731.2617415">https://dl.acm.org/doi/10.5555/2615731.2617415</a></span></li><li><span id="ref128">Xu X, Zhang H, Sefidgar Y, Ren Y, Liu X, Seo W,  et al. GLOBEM dataset: multi-year datasets for longitudinal human behavior modeling generalization. arXiv.  preprint posted online on November 4, 2022.  [<a href="https://arxiv.org/abs/2211.02733" target="_blank">FREE Full text</a>]</span></li><li><span id="ref129">Cimtay Y, Ekmekcioglu E, Caglar-Ozhan S. Cross-subject multimodal emotion recognition based on hybrid fusion. IEEE Access.   2020;8:168865-168878. [<a target="_blank" href="https://dx.doi.org/10.1109/access.2020.3023871">CrossRef</a>]</span></li><li><span id="ref130">Cai H, Yuan Z, Gao Y, Sun S, Li N, Tian F,  et al. A multi-modal open dataset for mental-disorder analysis. Sci Data.  Apr 19, 2022;9(1):178.  [<a href="https://doi.org/10.1038/s41597-022-01211-x" target="_blank">FREE Full text</a>] [<a target="_blank" href="https://dx.doi.org/10.1038/s41597-022-01211-x">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=35440583&amp;dopt=Abstract" target="_blank">Medline</a>]</span></li><li><span id="ref131">Chen J, Ro T, Zhu Z. Emotion recognition with audio, video, EEG, and EMG: a dataset and baseline approaches. IEEE Access.   2022;10:13229-13242. [<a target="_blank" href="https://dx.doi.org/10.1109/access.2022.3146729">CrossRef</a>]</span></li><li><span id="ref132">Mauriello ML, Lincoln T, Hon G, Simon D, Jurafsky D, Paredes P. SAD: a stress annotated dataset for recognizing everyday stressors in SMS-like conversational systems. 
                        In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 2021. Presented at: CHI EA '21; May 8-13, 2021:1-7; Yokohama, Japan. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3411763.3451799">https://dl.acm.org/doi/10.1145/3411763.3451799</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3411763.3451799">CrossRef</a>]</span></li><li><span id="ref133">Zeng G, Yang W, Ju Z, Yang Y, Wang S, Zhang R,  et al. MedDialog: large-scale medical dialogue datasets. 
                        In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. 2020. Presented at: EMNLP '20; November 16-20, 2020:9241-9250; Virtual Event. URL: <a target="_blank" href="https://aclanthology.org/2020.emnlp-main.743.pdf">https://aclanthology.org/2020.emnlp-main.743.pdf</a> [<a target="_blank" href="https://dx.doi.org/10.18653/v1/2020.emnlp-main.743">CrossRef</a>]</span></li><li><span id="ref134">Jamil Z, Inkpen D, Buddhitha P, White K. Monitoring tweets for depression to detect at-risk users. 
                        In: Proceedings of the 4th Workshop on Computational Linguistics and Clinical Psychology. 2017. Presented at: CLPsych '17; August 3, 2017:32-40; Vancouver, BC. URL: <a target="_blank" href="https://aclanthology.org/W17-3104.pdf">https://aclanthology.org/W17-3104.pdf</a></span></li><li><span id="ref135">Gemini. Google AI.  URL: <a target="_blank" href="https://www.gemini.google.com">https://www.gemini.google.com</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref136">Llama 2: open source, free for research and commercial use. Meta AI.  URL: <a target="_blank" href="https://www.llama.com/llama2/">https://www.llama.com/llama2/</a> [accessed 2025-05-29]
                        </span></li><li><span id="ref137">Cesare N, Grant C, Nsoesie E. Understanding demographic bias and representation in social media health data. 
                        In: Proceedings of the 10th ACM Conference on Web Science. 2019. Presented at: WebSci '19; June 30-July 3, 2019:7-9; Boston, MA. URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3328413.3328415">https://dl.acm.org/doi/10.1145/3328413.3328415</a> [<a target="_blank" href="https://dx.doi.org/10.1145/3328413.3328415">CrossRef</a>]</span></li></ol></div><br><hr><a name="Abbreviations">&#x200E;</a><h4 class="navigation-heading" id="Abbreviations" data-label="Abbreviations">Abbreviations</h4><table width="80%" border="0" align="center"><tr><td><b>BERT:</b> Bidirectional Encoder Representations from Transformers</td></tr><tr><td><b>EEG:</b> electroencephalogram</td></tr><tr><td><b>GenAI:</b> generative artificial intelligence</td></tr><tr><td><b>LGBTQ:</b> lesbian, gay, bisexual, transgender, and queer</td></tr><tr><td><b>LLaMA:</b> large language model Meta AI</td></tr><tr><td><b>LLM:</b> large language model</td></tr><tr><td><b>MI-CLAIM-GEN:</b> Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research</td></tr><tr><td><b>PICOS:</b> Population, Intervention, Comparison, Outcome, and Study</td></tr><tr><td><b>RAG:</b> retrieval-augmented generation</td></tr><tr><td><b>SPIDER:</b> Sample, Phenomenon of Interest, Design, Evaluation, and Research Type</td></tr><tr><td><b>SVM:</b> support vector machines</td></tr></table><br><hr><p style="font-style: italic">Edited by  C Blease; submitted 27.12.24; peer-reviewed by B Lamichhane,  S Markham,  S Tayebi Arasteh,  G Huang; comments to author 18.02.25; revised version received 14.04.25; accepted 29.05.25; published 27.06.25.</p><a href="https://support.jmir.org/hc/en-us/articles/115002955531" id="Copyright" target="_blank" class="navigation-heading h4 d-block" aria-label="Copyright - what is a Creative Commons License?" data-label="Copyright">Copyright <span class="fas fa-question-circle"></span></a><p class="article-copyright">&#xA9;Xi Wang, Yujia Zhou, Guangyu Zhou. Originally published in JMIR Mental Health (https://mental.jmir.org), 27.06.2025.</p><small class="article-license"><p class="abstract-paragraph">This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Mental Health, is properly cited. The complete bibliographic information, a link to the original publication on https://mental.jmir.org/, as well as this copyright and license information must be included.</p></small><br></section></article></section></section></main>
</div></div></div></div> <aside data-test="sidebar-exists" class="sidebar-citation col-lg-3 mb-5"><!----> <div><h2 class="h4 green-heading-underline width-fit-content">
                        Citation
                    </h2> <p class="fw-bold">
                        Please cite as:
                    </p> <p><span>
                            Wang X<span>,</span></span><span>
                            Zhou Y<span>,</span></span><span>
                            Zhou G<!----></span> <br> <span>The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review</span> <br> <span>JMIR Ment Health 2025;12:e70610</span> <br> <span>doi:
                            <span><a aria-label="DOI number 10.2196/70610" data-test="article-doi" target="_blank" href="https://doi.org/10.2196/70610">
                                    10.2196/70610
                                </a></span></span> <span style="display: block">
                            PMID:
                            <span><a data-test="article-pmid" aria-label="PMID 40577783" target="_blank" href="https://www.ncbi.nlm.nih.gov/pubmed/40577783">40577783</a></span></span> <span style="display: block">
                            PMCID:
                            <span><a data-test="article-pmcid" aria-label="PMCID 12254713" target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/12254713">12254713</a></span></span></p> <button title="Copy Citation" data-test="copy-to-clipboard-button" class="btn btn-small btn-grey"><span aria-hidden="true" class="icon fas fa-paste"></span> Copy Citation to
                        Clipboard
                    </button> <!----></div> <div class="export-metadata"><h2 class="h4 green-heading-underline width-fit-content">
                        Export Metadata
                    </h2> <div><a aria-label="Export metadata in END" target="_blank" data-test="test-end-link" href="https://mental.jmir.org/article/export/end/mental_v12i1e70610" rel="noreferrer">
                            END
                        </a><span> for: Endnote</span></div> <div><a aria-label="Export metadata in BibTeX" target="_blank" data-test="test-bib-link" href="https://mental.jmir.org/article/export/bib/mental_v12i1e70610" rel="noreferrer">
                            BibTeX
                        </a><span> for: BibDesk, LaTeX</span></div> <div><a aria-label="Export metadata in RIS" target="_blank" data-test="test-ris-link" href="https://mental.jmir.org/article/export/ris/mental_v12i1e70610" rel="noreferrer">
                            RIS
                        </a><span>
                            for: RefMan, Procite, Endnote, RefWorks</span></div> <div><a target="_blank" data-test="doi-link" href="http://www.mendeley.com/import/?doi=10.2196/70610">
                            Add this article to your Mendeley library
                        </a></div></div> <div class="collection desktop-show"><h2 tabindex="0" data-test="article-collection" class="h4 green-heading-underline width-fit-content">
                        This paper is in the following
                        <span class="collection__span">e-collection/theme issue:</span></h2> <a href="/themes/243" data-test="article-collection" aria-label="291 articles belongs to Reviews in Digital Mental Health e-collection/theme issue" class="collection__link">
                        Reviews in Digital Mental Health (291)
                    </a><a href="/themes/232" data-test="article-collection" aria-label="562 articles belongs to Methods and New Tools in Mental Health Research e-collection/theme issue" class="collection__link">
                        Methods and New Tools in Mental Health Research (562)
                    </a><a href="/themes/1660" data-test="article-collection" aria-label="31 articles belongs to AI-Powered Therapy Bots and Virtual Companions in Digital Mental Health e-collection/theme issue" class="collection__link">
                        AI-Powered Therapy Bots and Virtual Companions in Digital Mental Health (31)
                    </a><a href="/themes/388" data-test="article-collection" aria-label="328 articles belongs to Diagnostic Tools in Mental Health e-collection/theme issue" class="collection__link">
                        Diagnostic Tools in Mental Health (328)
                    </a><a href="/themes/253" data-test="article-collection" aria-label="530 articles belongs to Mobile Health in Psychiatry e-collection/theme issue" class="collection__link">
                        Mobile Health in Psychiatry (530)
                    </a><a href="/themes/37" data-test="article-collection" aria-label="622 articles belongs to Ethics, Privacy, and Legal Issues e-collection/theme issue" class="collection__link">
                        Ethics, Privacy, and Legal Issues (622)
                    </a><a href="/themes/1649" data-test="article-collection" aria-label="56 articles belongs to Responsible Health AI e-collection/theme issue" class="collection__link">
                        Responsible Health AI (56)
                    </a><a href="/themes/1437" data-test="article-collection" aria-label="725 articles belongs to Generative Language Models Including ChatGPT e-collection/theme issue" class="collection__link">
                        Generative Language Models Including ChatGPT (725)
                    </a><a href="/themes/64" data-test="article-collection" aria-label="2145 articles belongs to Digital Mental Health Interventions, e-Mental Health and Cyberpsychology e-collection/theme issue" class="collection__link">
                        Digital Mental Health Interventions, e-Mental Health and Cyberpsychology (2145)
                    </a><a href="/themes/797" data-test="article-collection" aria-label="2162 articles belongs to Artificial Intelligence e-collection/theme issue" class="collection__link">
                        Artificial Intelligence (2162)
                    </a></div> <div><h2 class="h4 green-heading-underline width-fit-content">
                        Download
                    </h2> <div class="download-btns"><a target="_blank" href="https://mental.jmir.org/2025/1/e70610/PDF" aria-label="Download PDF" data-test="pdf-button" class="btn btn-small btn-grey mt-1"><span aria-hidden="true" class="icon fas fa-download"></span> Download PDF</a> <a target="_blank" href="https://mental.jmir.org/2025/1/e70610/XML" aria-label="Download XML" data-test="xml-button" class="btn btn-small btn-grey mt-1"><span aria-hidden="true" class="icon fas fa-download"></span> Download XML</a></div></div> <div><h2 class="h4 green-heading-underline width-fit-content">
                        Share Article
                    </h2> <span class="sm-icons"><a title="share-on-Bluesky" href="https://bsky.app/intent/compose?text=Check%20out%20this%20fascinating%20article%20https%3A%2F%2Fmental.jmir.org%2F2025%2F1%2Fe70610" aria-label="Share this item on Bluesky" data-test="bluesky-button" rel="noreferrer" target="_blank" class="bluesky small"></a> <a title="share-on-Twitter" href="https://twitter.com/intent/tweet?url=Check%20out%20this%20fascinating%20article%20https%3A%2F%2Fmental.jmir.org%2F2025%2F1%2Fe70610" aria-label="Share this item on Twitter" data-test="twitter-button" rel="noreferrer" target="_blank" class="twitter small"></a> <a title="share-on-Facebook" href="https://www.facebook.com/sharer.php?s=100&amp;u=https://mental.jmir.org/2025/1/e70610&amp;quote=Check%20out%20this%20fascinating%20article" aria-label="Share this item on Facebook" data-test="facebook-button" rel="noreferrer" target="_blank" class="facebook small"></a> <a title="Linkedin" href="https://www.linkedin.com/feed?shareActive=true&amp;text=Check%20out%20this%20fascinating%20article%20https%3A%2F%2Fmental.jmir.org%2F2025%2F1%2Fe70610" aria-label="Share this item on Linkedin" data-test="linkedin-button" rel="noreferrer" target="_blank" class="linkedin small"></a></span></div> <div class="ads-sidebar-container"><!----> <!----> <!----></div></aside></div> <!----></div></div></div></div> <div><section data-test="footer" class="footer"><footer id="footer"><div class="container-fluid footer-journal-name"><div class="col-12"><h2 data-test="journal-name">
                        JMIR Mental Health
                        <span>
                            ISSN 2368-7959
                        </span></h2></div></div> <div class="container"><div class="row"><div class="col-lg-3 col-6"><h3 tabindex="0" class="footer-title">
                            Resource Centre
                        </h3> <ul><li data-test="resource-links"><a href="/resource-centre/author-hub">
                                    Author Hub
                                </a></li><li data-test="resource-links"><a href="/resource-centre/editor-hub">
                                    Editor Hub
                                </a></li><li data-test="resource-links"><a href="/resource-centre/reviewer-hub">
                                    Reviewer Hub
                                </a></li><li data-test="resource-links"><a href="/resource-centre/librarian-hub">
                                    Librarian Hub
                                </a></li></ul></div> <div class="col-lg-3 col-6"><h3 tabindex="0" class="footer-title">
                            Browse Journal
                        </h3> <ul><li data-test="journal-links"><a href="/announcements">
                                    Latest Announcements
                                </a></li><li data-test="journal-links"><a href="/search/authors">
                                    Authors
                                </a></li> <li data-test="journal-links"><a href="/themes">
                                    Themes
                                </a></li><li data-test="journal-links"><a href="/issues">
                                    Issues
                                </a></li> <li data-test="journal-links"><a href="https://blog.jmir.org ">
                                    Blog
                                </a></li></ul></div> <div class="col-lg-3 col-6"><h3 tabindex="0" class="footer-title">
                            About
                        </h3> <ul><li data-test="about-links"><a href="/privacy-statement">
                                    Privacy Statement
                                </a></li><li data-test="about-links"><a href="/contact-us">
                                    Contact Us
                                </a></li> <li><a href="/sitemap.xml" target="_blank">
                                    Sitemap
                                </a></li></ul></div> <div class="col-lg-3 col-6"><h3 tabindex="0" class="footer-title">
                            Connect With Us
                        </h3> <span class="sm-icons"><a aria-label="JMIR Publications Bluesky account" title="Bluesky" href="https://bsky.app/profile/jmirpub.bsky.social " target="_blank" rel="noreferrer" data-test="social-links" class="bluesky mr-1"></a><a aria-label="JMIR Publications Tweeter account" title="Twitter" href="https://twitter.com/jmirpub" target="_blank" rel="noreferrer" data-test="social-links" class="twitter mr-1"></a><a aria-label="JMIR Publications Facebook account" title="Facebook" href="https://www.facebook.com/JMedInternetRes" target="_blank" rel="noreferrer" data-test="social-links" class="facebook mr-1"></a><a aria-label="JMIR Publications Linkedin account" title="Linkedin" href="https://www.linkedin.com/company/jmir-publications" target="_blank" rel="noreferrer" data-test="social-links" class="linkedin mr-1"></a><a aria-label="JMIR Publications YouTube account" title="YouTube" href="https://www.youtube.com/c/JMIRPublications" target="_blank" rel="noreferrer" data-test="social-links" class="youtube mr-1"></a><a aria-label="JMIR Publications Instagram account" title="Instagram" href="https://www.instagram.com/jmirpub" target="_blank" rel="noreferrer" data-test="social-links" class="instagram"></a> <a target="_blank" rel="noreferrer" aria-label="RSS Subscription" title="RSS Subscription" href="https://mental.jmir.org/feed/atom" class="rss"></a></span></div> <div class="email-subscribtion-button col-lg-3 col-md-6 col-sm-6 col-12"><h3 tabindex="0" class="footer-title">
                            Get Table of Contents Alerts
                        </h3> <a target="_blank" rel="noopener noreferrer" aria-label="Newsletter Subscription" title="Newsletter Subscription" href="https://landingpage.jmirpublications.com/journal-preference-selection"><span>Get Alerts</span> <span class="icon fas fa-paper-plane"></span></a></div> <div class="col-12 text-center mt-5"><p class="footer-copyright">
                            Copyright ©
                            <time datetime="2025">
                                2025
                            </time>
                            JMIR Publications
                        </p></div></div></div></footer></section></div></div> <div><a tabindex="0" href="javascript:;" title="Scroll to the top of the page" role="button" class="scroll-to-very-top"><span aria-hidden="true" class="icon fas fa-chevron-up"></span></a></div> <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----></div></div></div><script>window.__NUXT__=(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br){return {layout:"front",data:[{tabs:{html:{name:"Article",route:e,path:"articleHtml"},authors:{name:"Authors",route:"authors",path:"articleAuthors"},citations:{name:"Cited by (5)",route:"citations",path:"articleCitations"},tweetations:{name:"Tweetations (10)",route:"tweetations",path:"articleTweetations"},metrics:{name:"Metrics",route:"metrics",path:"articleMetrics"}},registeredReport:i},{html:"\u003Cmain id=\"wrapper\" class=\"wrapper ArticleMain clearfix\"\u003E\u003Csection class=\"inner-wrapper clearfix\"\u003E\u003Csection class=\"main-article-content clearfix\"\u003E\u003Carticle class=\"ajax-article-content\"\u003E\u003Ch4 class=\"h4-original-paper\"\u003E\u003Cspan class=\"typcn typcn-document-text\"\u003E\u003C\u002Fspan\u003EReview\u003C\u002Fh4\u003E\u003Cdiv class=\"authors-container\"\u003E\u003Cdiv class=\"authors clearfix\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"authors-container\"\u003E\u003Cdiv class=\"authors clearfix\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"authors-container\"\u003E\u003Cdiv class=\"authors clearfix\"\u003E\u003Cul class=\"clearfix\"\u003E\u003Cli\u003E\u003Ca href=\"\u002Fsearch\u002FsearchResult?field%5B%5D=author&amp;criteria%5B%5D=Xi+Wang\" class=\"btn-view-author-options\"\u003EXi Wang\u003Csup\u003E\u003Csmall\u003E1\u003C\u002Fsmall\u003E\u003C\u002Fsup\u003E, MA\u003C\u002Fa\u003E\u003Ca class=\"author-orcid\" href=\"https:\u002F\u002Forcid.org\u002F0009-0004-1090-6209\" target=\"_blank\" title=\"ORCID\"\u003E&#xA0;\u003C\u002Fa\u003E;&#xA0;\u003C\u002Fli\u003E\u003Cli\u003E\u003Ca href=\"\u002Fsearch\u002FsearchResult?field%5B%5D=author&amp;criteria%5B%5D=Yujia+Zhou\" class=\"btn-view-author-options\"\u003EYujia Zhou\u003Csup\u003E\u003Csmall\u003E2\u003C\u002Fsmall\u003E\u003C\u002Fsup\u003E, PhD\u003C\u002Fa\u003E\u003Ca class=\"author-orcid\" href=\"https:\u002F\u002Forcid.org\u002F0000-0002-3530-3787\" target=\"_blank\" title=\"ORCID\"\u003E&#xA0;\u003C\u002Fa\u003E;&#xA0;\u003C\u002Fli\u003E\u003Cli\u003E\u003Ca href=\"\u002Fsearch\u002FsearchResult?field%5B%5D=author&amp;criteria%5B%5D=Guangyu+Zhou\" class=\"btn-view-author-options\"\u003EGuangyu Zhou\u003Csup\u003E\u003Csmall\u003E1\u003C\u002Fsmall\u003E\u003C\u002Fsup\u003E, Prof Dr\u003C\u002Fa\u003E\u003Ca class=\"author-orcid\" href=\"https:\u002F\u002Forcid.org\u002F0000-0003-2053-6737\" target=\"_blank\" title=\"ORCID\"\u003E&#xA0;\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cdiv class=\"author-affiliation-details\"\u003E\u003Cp\u003E\u003Csup\u003E1\u003C\u002Fsup\u003ESchool of Psychological and Cognitive Sciences, Beijing Key Laboratory of Behavior and Mental Health, Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, China\u003C\u002Fp\u003E\u003Cp\u003E\u003Csup\u003E2\u003C\u002Fsup\u003EDepartment of Computer Science and Technology, Tsinghua University, Beijing, China\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cdiv class=\"corresponding-author-and-affiliations clearfix\"\u003E\u003Cdiv class=\"corresponding-author-details\"\u003E\u003Ch3\u003ECorresponding Author:\u003C\u002Fh3\u003E\u003Cp\u003EGuangyu Zhou, Prof Dr\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003ESchool of Psychological and Cognitive Sciences\u003C\u002Fp\u003E\u003Cp\u003EBeijing Key Laboratory of Behavior and Mental Health, Key Laboratory of Machine Perception (Ministry of Education)\u003C\u002Fp\u003E\u003Cp\u003EPeking University\u003C\u002Fp\u003E\u003Cp\u003EPhilosophy Building, 2nd Fl.\u003C\u002Fp\u003E\u003Cp\u003ENo. 5 Yiheyuan Road, Haidian District\u003C\u002Fp\u003E\u003Cp\u003EBeijing, 100871\u003C\u002Fp\u003E\u003Cp\u003EChina\u003C\u002Fp\u003E\u003Cp\u003EPhone: 86 10 62767702\u003C\u002Fp\u003E\u003Cp\u003EEmail: \u003Ca href=\"mailto:gyzhou@pku.edu.cn\"\u003Egyzhou@pku.edu.cn\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cbr\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Csection class=\"article-content clearfix\"\u003E\u003Carticle class=\"abstract\"\u003E\u003Ch3 id=\"Abstract\" class=\"navigation-heading\" data-label=\"Abstract\"\u003EAbstract\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cspan class=\"abstract-sub-heading\"\u003EBackground: \u003C\u002Fspan\u003EMental health disorders affect an estimated 1 in 8 individuals globally, yet traditional interventions often face barriers, such as limited accessibility, high costs, and persistent stigma. Recent advancements in generative artificial intelligence (GenAI) have introduced AI systems capable of understanding and producing humanlike language in real time. These developments present new opportunities to enhance mental health care.\u003Cbr\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cspan class=\"abstract-sub-heading\"\u003EObjective: \u003C\u002Fspan\u003EWe aimed to systematically examine the current applications of GenAI in mental health, focusing on 3 core domains: diagnosis and assessment, therapeutic tools, and clinician support. In addition, we identified and synthesized key ethical issues reported in the literature.\u003Cbr\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cspan class=\"abstract-sub-heading\"\u003EMethods: \u003C\u002Fspan\u003EWe conducted a comprehensive literature search, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines, in PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar databases to identify peer-reviewed studies published from October 1, 2019, to September 30, 2024. After screening 783 records, 79 (10.1%) studies met the inclusion criteria.\u003Cbr\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cspan class=\"abstract-sub-heading\"\u003EResults: \u003C\u002Fspan\u003EThe number of studies on GenAI applications in mental health has grown substantially since 2023. Studies on diagnosis and assessment (37\u002F79, 47%) primarily used GenAI models to detect depression and suicidality through text data. Studies on therapeutic applications (20\u002F79, 25%) investigated GenAI-based chatbots and adaptive systems for emotional and behavioral support, reporting promising outcomes but revealing limited real-world deployment and safety assurance. Clinician support studies (24\u002F79, 30%) explored GenAI&#x2019;s role in clinical decision-making, documentation and summarization, therapy support, training and simulation, and psychoeducation. Ethical concerns were consistently reported across the domains. On the basis of these findings, we proposed an integrative ethical framework, GenAI4MH, comprising 4 core dimensions&#x2014;data privacy and security, information integrity and fairness, user safety, and ethical governance and oversight&#x2014;to guide the responsible use of GenAI in mental health contexts.\u003Cbr\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cspan class=\"abstract-sub-heading\"\u003EConclusions: \u003C\u002Fspan\u003EGenAI shows promise in addressing the escalating global demand for mental health services. They may augment traditional approaches by enhancing diagnostic accuracy, offering more accessible support, and reducing clinicians&#x2019; administrative burden. However, to ensure ethical and effective implementation, comprehensive safeguards&#x2014;particularly around privacy, algorithmic bias, and responsible user engagement&#x2014;must be established.\u003Cbr\u003E\u003C\u002Fp\u003E\u003Cstrong class=\"h4-article-volume-issue\"\u003EJMIR Ment Health 2025;12:e70610\u003C\u002Fstrong\u003E\u003Cbr\u003E\u003Cbr\u003E\u003Cspan class=\"article-doi\"\u003E\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.2196\u002F70610\"\u003Edoi:10.2196\u002F70610\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003Cbr\u003E\u003Cbr\u003E\u003Ch3 class=\"h3-main-heading\" id=\"Keywords\"\u003EKeywords\u003C\u002Fh3\u003E\u003Cdiv class=\"keywords\"\u003E\u003Cspan\u003E\u003Ca href=\"\u002Fsearch?type=keyword&amp;term=generative%20AI&amp;precise=true\"\u003Egenerative AI\u003C\u002Fa\u003E;&#xA0;\u003C\u002Fspan\u003E\u003Cspan\u003E\u003Ca href=\"\u002Fsearch?type=keyword&amp;term=mental%20health&amp;precise=true\"\u003Emental health\u003C\u002Fa\u003E;&#xA0;\u003C\u002Fspan\u003E\u003Cspan\u003E\u003Ca href=\"\u002Fsearch?type=keyword&amp;term=large%20language%20models&amp;precise=true\"\u003Elarge language models\u003C\u002Fa\u003E;&#xA0;\u003C\u002Fspan\u003E\u003Cspan\u003E\u003Ca href=\"\u002Fsearch?type=keyword&amp;term=mental%20health%20detection%20and%20diagnosis&amp;precise=true\"\u003Emental health detection and diagnosis\u003C\u002Fa\u003E;&#xA0;\u003C\u002Fspan\u003E\u003Cspan\u003E\u003Ca href=\"\u002Fsearch?type=keyword&amp;term=therapeutic%20chatbots&amp;precise=true\"\u003Etherapeutic chatbots\u003C\u002Fa\u003E&#xA0;\u003C\u002Fspan\u003E\u003C\u002Fdiv\u003E\u003Cdiv id=\"trendmd-suggestions\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Farticle\u003E\u003Cbr\u003E\u003Carticle class=\"main-article clearfix\"\u003E\u003Cbr\u003E\u003Ch3 class=\"navigation-heading h3-main-heading\" id=\"Introduction\" data-label=\"Introduction\"\u003EIntroduction\u003C\u002Fh3\u003E\u003Ch4\u003EBackground\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EMental health has become a global public health priority, with increasing recognition of its importance for individual well-being, societal stability, and economic productivity. According to the World Health Organization, approximately 1 in 8 people worldwide live with a mental health disorder [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref1\" rel=\"footnote\"\u003E1\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Despite the growing demand for mental health services, traditional approaches such as in-person therapy and medication, which rely heavily on trained professionals and extensive infrastructure, are struggling to meet the rising need [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref2\" rel=\"footnote\"\u003E2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Consequently, an alarming 76% to 85% of individuals with mental health disorders do not receive effective treatment, often due to barriers such as limited access to mental health professionals, social stigma, and inadequate health care systems [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Against this backdrop, advances in generative artificial intelligence (GenAI) offer new and promising avenues to enhance mental health services.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EGenAI, such as ChatGPT [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref4\" rel=\"footnote\"\u003E4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], is built on large-scale language modeling and trained on extensive textual corpora. Their capacity to produce contextually relevant and, in many cases, emotionally appropriate language [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref5\" rel=\"footnote\"\u003E5\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref6\" rel=\"footnote\"\u003E6\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] enables more natural and adaptive interactions. Compared to earlier dialogue systems, GenAI exhibits greater flexibility in producing open-ended, humanlike dialogue [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. This generative capability makes them a promising tool for web-based therapeutic interventions that allow for real-time, adaptive engagement in mental health care.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ECurrently, GenAI is being integrated into mental health through a range of innovative applications. For instance, GPT-driven chatbots such as Well-Mind ChatGPT [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref8\" rel=\"footnote\"\u003E8\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and MindShift [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref9\" rel=\"footnote\"\u003E9\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] provide personalized mental health support by engaging users in conversational therapy. Similarly, virtual companions such as Replika [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref10\" rel=\"footnote\"\u003E10\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] are used to help users manage feelings of loneliness and anxiety through interactive dialogue [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In addition, GenAI has been used to analyze social media posts and clinical data to identify signs of depression [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref2\" rel=\"footnote\"\u003E2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and suicidal ideation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. These diverse applications illustrate the potential of GenAI to address various mental health needs, from prevention and assessment to continuous support and intervention.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EAlthough research has investigated various applications of GenAI in mental health, much of it has focused on specific models or isolated cases, lacking a comprehensive evaluation of its broader impacts, applications, and associated risks. Similarly, most systematic reviews to date have focused on particular domains, such as depression detection [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref13\" rel=\"footnote\"\u003E13\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], chatbot interventions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref14\" rel=\"footnote\"\u003E14\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], empathic communication [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref5\" rel=\"footnote\"\u003E5\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], psychiatric education [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref15\" rel=\"footnote\"\u003E15\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and AI-based art therapy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref16\" rel=\"footnote\"\u003E16\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. While such focused reviews offer valuable insights into specific use cases, a broad outline remains crucial for understanding overarching trends, identifying research gaps, and informing the responsible development of GenAI in mental health. To date, only 2 reviews [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref17\" rel=\"footnote\"\u003E17\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref18\" rel=\"footnote\"\u003E18\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] have attempted broader overviews, covering the literature published before April 2024 and July 2023, respectively. However, since April 2024, the rapid evolution of GenAI&#x2014;including the release and deployment of more advanced models, such as GPT-4o [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref19\" rel=\"footnote\"\u003E19\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and GPT-o1 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref20\" rel=\"footnote\"\u003E20\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and their increasing integration with clinical workflows, such as Med-Gemini [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref21\" rel=\"footnote\"\u003E21\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], has expanded the scope and complexity of GenAI applications in real-world mental health contexts. These developments underscore the need for a more updated and integrative synthesis.\u003C\u002Fp\u003E\u003Ch4\u003EObjectives\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo address this gap, we aimed to provide a comprehensive overview of GenAI applications in mental health, identify research gaps, and propose future directions. To systematically categorize the existing research, we divided the studies into three distinct categories based on the role of GenAI in mental health applications, as illustrated in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure1\" rel=\"footnote\"\u003EFigure 1\u003C\u002Fa\u003E\u003C\u002Fspan\u003E: (1) GenAI for mental health diagnosis and assessment, encompassing research that leverages GenAI to detect, classify, or evaluate mental health conditions; (2) GenAI as therapeutic tools, covering studies where GenAI-based chatbots or conversational agents are used to deliver mental health support, therapy, or interventions directly to users; and (3) GenAI for supporting clinicians and mental health professionals, including research aimed at using GenAI to assist clinicians in their practice.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EDespite these promising applications, the integration of GenAI into mental health care is not without challenges. Applying GenAI in the mental health field involves processing highly sensitive personal information, such as users&#x2019; emotional states, psychological histories, and behavioral patterns. Mishandling such data not only poses privacy risks but may also lead to psychological harm, including distress, stigma, or reduced trust in mental health services [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref22\" rel=\"footnote\"\u003E22\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Therefore, in addition to systematically categorizing existing applications of GenAI in mental health, we also examined ethical issues related to their use in this domain. On the basis of our analysis, we proposed an ethical framework, GenAI4MH, to guide the responsible use of GenAI in mental health contexts (\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure2\" rel=\"footnote\"\u003EFigure 2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E).\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Ca name=\"figure1\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 1. Classification of generative artificial intelligence (GenAI) applications in mental health.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F17bd6af3116f56a39f6b8bebcee2a881.png\" id=\"figure1\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F17bd6af3116f56a39f6b8bebcee2a881.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 1. \u003C\u002Fb\u003E Classification of generative artificial intelligence (GenAI) applications in mental health. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cfigure\u003E\u003Ca name=\"figure2\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 2. Overview of the GenAI4MH ethical framework for the responsible use of generative artificial intelligence (GenAI) in mental health. GLM: generative language model.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F48f15915a12bd696a5d815ea89b02acf.png\" id=\"figure2\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F48f15915a12bd696a5d815ea89b02acf.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 2. \u003C\u002Fb\u003E Overview of the GenAI4MH ethical framework for the responsible use of generative artificial intelligence (GenAI) in mental health. GLM: generative language model. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cbr\u003E\u003Ch3 class=\"navigation-heading h3-main-heading\" id=\"Methods\" data-label=\"Methods\"\u003EMethods\u003C\u002Fh3\u003E\u003Ch4\u003ESearch Strategy\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EWe conducted this systematic review following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines (\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#app1\" rel=\"footnote\"\u003EMultimedia Appendix 1\u003C\u002Fa\u003E\u003C\u002Fspan\u003E) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref23\" rel=\"footnote\"\u003E23\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. We conducted a comprehensive search across 6 databases: PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar. We conducted the search between October 1, 2024, and October 7, 2024, and targeted studies published from October 1, 2019, to September 30, 2024. The starting date was chosen to coincide with the introduction of the T5 model [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref24\" rel=\"footnote\"\u003E24\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], a foundational development for many of today&#x2019;s mainstream GenAI models. This date also intentionally excluded earlier models, such as Bidirectional Encoder Representations from Transformers (BERT) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref25\" rel=\"footnote\"\u003E25\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and GPT-2 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref26\" rel=\"footnote\"\u003E26\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], as these models have already been extensively covered in the previous literature [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref27\" rel=\"footnote\"\u003E27\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref28\" rel=\"footnote\"\u003E28\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and our aim was to highlight more recent innovations.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ESearch terms were constructed using a logical combination of keywords related to GenAI and mental health: (Generative AI OR Large Language Model OR ChatGPT) AND (mental health OR mental disorder OR depression OR anxiety). This search string was developed based on previous reviews and refined through iterative testing to ensure effective identification of relevant studies. When possible, the search was restricted to titles and abstracts. For Google Scholar, the first 10 pages of results were screened for relevance. A detailed search strategy is provided in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#app2\" rel=\"footnote\"\u003EMultimedia Appendix 2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E.\u003C\u002Fp\u003E\u003Ch4\u003EStudy Selection\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EThe selection criteria included studies that (1) used GenAI and were published after the introduction of the T5 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref24\" rel=\"footnote\"\u003E24\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] model and (2) directly addressed the application of GenAI in mental health care settings. Only peer-reviewed original research articles were considered, with no language restrictions.\u003C\u002Fp\u003E\u003Ch4\u003EData Extraction\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EData from the included studies were extracted using standardized frameworks. For qualitative studies, we used the Sample, Phenomenon of Interest, Design, Evaluation, and Research Type (SPIDER) framework. For quantitative studies, we applied the Population, Intervention, Comparison, Outcome, and Study (PICOS) framework. A summary of the extracted data are provided in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#app3\" rel=\"footnote\"\u003EMultimedia Appendix 3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref2\" rel=\"footnote\"\u003E2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref9\" rel=\"footnote\"\u003E9\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref29\" rel=\"footnote\"\u003E29\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref100\" rel=\"footnote\"\u003E100\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch4\u003EReporting Quality Assessment\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo assess the reporting transparency and the methodological rigor of the included studies, we applied the Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research (MI-CLAIM-GEN) checklist (\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#app4\" rel=\"footnote\"\u003EMultimedia Appendix 4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref101\" rel=\"footnote\"\u003E101\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], a recently proposed guideline tailored for evaluating the reporting quality of research on GenAI in health care. The checklist covers essential aspects such as study design, data and resource transparency, model evaluation strategies, bias and harm assessments, and reproducibility. We followed the Joanna Briggs Institute quality appraisal format [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref102\" rel=\"footnote\"\u003E102\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] to score each item in the checklist using 4 categories: yes, no, unclear, and not applicable.\u003C\u002Fp\u003E\u003Cbr\u003E\u003Ch3 class=\"navigation-heading h3-main-heading\" id=\"Results\" data-label=\"Results\"\u003EResults\u003C\u002Fh3\u003E\u003Ch4\u003EStudy Selection\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EAs shown in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure3\" rel=\"footnote\"\u003EFigure 3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E, a total of 783 records were initially retrieved from the 6 databases. After removing duplicates, 73.8% (578\u002F783) of unique records remained for screening. Following abstract screening, 39.4% (228\u002F578) of the records were identified for full-text retrieval and screening. After full-text screening, 24% (55\u002F228) of the articles were selected for inclusion in the systematic review. To ensure comprehensive coverage of relevant studies, a snowballing technique was then applied, where we examined the reference lists of the included studies and related review articles. This process identified an additional 44 studies for eligibility assessment. After the same evaluation process, 54% (24\u002F44) of these studies met the inclusion criteria, bringing the final total to 79 studies for the systematic review. Two PhD candidates (YZ and XW) independently conducted the selection, with discrepancies resolved through discussion. The interrater reliability was satisfactory (&#x3BA;=0.904).\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Ca name=\"figure3\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 3. The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram of study selection.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002Ffcacc9d94d512814fd41c551509df627.png\" id=\"figure3\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002Ffcacc9d94d512814fd41c551509df627.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 3. \u003C\u002Fb\u003E The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram of study selection. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch4\u003EPublication Trends Over Time\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EAn analysis of publication trends over time reveals a growing focus on the application of GenAI in mental health (\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure4\" rel=\"footnote\"\u003EFigure 4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E). Overall, the number of studies in all the 3 categories grew extensively over the examined period, indicating a rising interest in using GenAI for mental health. In 2022, the total number of studies was minimal across all the 3 categories, with only 1 (1%) early study, of the included 79 studies, emerging on the use of GenAI for mental health diagnosis and assessment. However, as GenAI advanced and garnered wider adoption, the number of publications in all the 3 categories began to increase steadily. A moderate increase was observed in the year 2023, with 13% (10\u002F79) of the studies focused on diagnosis and assessment, 9% (7\u002F79) on therapeutic interventions, and 8% (6\u002F79) on clinician support, reflecting a growing interest in practical applications of these models in health care settings. By 2024, the number of publications had surged across all the 3 categories, with 33% (26\u002F79) of the studies focused on diagnosis and assessment, 16% (13\u002F79) on therapeutic interventions, and 23% (18\u002F79) on clinician support.\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Ca name=\"figure4\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 4. Publication trends in the application of generative artificial intelligence (GenAI) in mental health research.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F43ad7e7f697395bfd980c68115858621.png\" id=\"figure4\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F43ad7e7f697395bfd980c68115858621.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 4. \u003C\u002Fb\u003E Publication trends in the application of generative artificial intelligence (GenAI) in mental health research. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch4\u003EGenAI for Mental Health Diagnosis and Assessment\u003C\u002Fh4\u003E\u003Ch5\u003EOverview\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EOf the 79 included studies, 37 (47%) were identified that investigated the effectiveness and applications of GenAI in mental health diagnosis and assessment. These studies primarily explored how GenAI can detect and interpret mental health conditions by analyzing textual and multimodal data. A summary of the included studies is presented in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#app5\" rel=\"footnote\"\u003EMultimedia Appendix 5\u003C\u002Fa\u003E\u003C\u002Fspan\u003E [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref2\" rel=\"footnote\"\u003E2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref29\" rel=\"footnote\"\u003E29\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref59\" rel=\"footnote\"\u003E59\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref61\" rel=\"footnote\"\u003E61\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref62\" rel=\"footnote\"\u003E62\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref100\" rel=\"footnote\"\u003E100\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EMental Health Issues\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EThe existing studies using GenAI for mental health diagnosis predominantly focused on suicide risk and depression, followed by emerging applications in emotion recognition, psychiatric disorders, and stress.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ESuicide risk was the most frequently examined topic, addressed in 40% (15\u002F37) of the studies. Researchers used large language models (LLMs) to identify suicide-related linguistic patterns [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], extract and synthesize textual evidence supporting identified suicide risk levels [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref29\" rel=\"footnote\"\u003E29\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref33\" rel=\"footnote\"\u003E33\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref103\" rel=\"footnote\"\u003E103\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and evaluate suicide risk [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref34\" rel=\"footnote\"\u003E34\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref41\" rel=\"footnote\"\u003E41\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. GenAI models, such as GPT-4 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref104\" rel=\"footnote\"\u003E104\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], achieved high precision (up to 0.96) in predicting suicidal risk levels [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref30\" rel=\"footnote\"\u003E30\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref33\" rel=\"footnote\"\u003E33\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref103\" rel=\"footnote\"\u003E103\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], outperforming traditional models, such as support vector machines (SVM) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref41\" rel=\"footnote\"\u003E41\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and performing comparably to or better than pretrained language models, such as BERT [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref38\" rel=\"footnote\"\u003E38\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Most studies (13\u002F15, 87%) relied on simulated case narratives [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref34\" rel=\"footnote\"\u003E34\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref36\" rel=\"footnote\"\u003E36\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref37\" rel=\"footnote\"\u003E37\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] or social media data [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref38\" rel=\"footnote\"\u003E38\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]; only 13% (2\u002F15) of the studies used real clinical narratives [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref35\" rel=\"footnote\"\u003E35\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref41\" rel=\"footnote\"\u003E41\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EDepression was the second most common mental health issue addressed, featured in 35% (13\u002F37) of the studies. While GenAI models showed promising accuracy (eg, 0.902 using semistructured diaries [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]), performance was often constrained to English data [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], with notable drop-offs in dialectal or culturally divergent contexts [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Multimodal approaches&#x2014;integrating audio, visual, and physiological data&#x2014;improved detection reliability over text-only methods [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref45\" rel=\"footnote\"\u003E45\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Several studies (3\u002F13, 23%) also explored interpretability, using GenAI to generate explanations [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] or conduct structured assessments [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref48\" rel=\"footnote\"\u003E48\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EGenAI has also been explored for emotion recognition, using smartphone and wearable data to predict affective states with moderate accuracy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref49\" rel=\"footnote\"\u003E49\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and enabling novel assessment formats, such as virtual agent interactions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref45\" rel=\"footnote\"\u003E45\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and conversational psychological scales [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref50\" rel=\"footnote\"\u003E50\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. The studies also explored other psychiatric disorders, such as obsessive-compulsive disorder (accuracy up to 96.1%) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref51\" rel=\"footnote\"\u003E51\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and schizophrenia (\u003Ci\u003Er\u003C\u002Fi\u003E=0.66-0.69 with expert ratings) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref52\" rel=\"footnote\"\u003E52\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In total, 8% (3\u002F37) of the studies addressed stress detection from social media texts [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref53\" rel=\"footnote\"\u003E53\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref54\" rel=\"footnote\"\u003E54\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EA smaller set of studies (3\u002F37, 8%) assessed GenAI models&#x2019; capacity for differential diagnosis, demonstrating that GenAI models could distinguish among multiple mental disorders in controlled simulations [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref55\" rel=\"footnote\"\u003E55\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. However, performance remained higher for mental health conditions with distinct symptoms (eg, psychosis and anxiety) and lower for overlapping or less prevalent disorders (eg, perinatal depression and lysergic acid diethylamide use disorder) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], particularly for those with symptom overlap with more common mental health conditions (eg, disruptive mood dysregulation disorder and acute stress disorder) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref55\" rel=\"footnote\"\u003E55\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EModel Architectures and Adaptation Strategies\u003C\u002Fh5\u003E\u003Ch6\u003EOverview\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EMost included studies (29\u002F37, 78%) used proprietary GenAI models for mental health diagnosis and assessment, with GPT-based models (GPT-3, 3.5, and 4) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref4\" rel=\"footnote\"\u003E4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] being the most commonly used [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref2\" rel=\"footnote\"\u003E2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Other proprietary models included Gemini [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref49\" rel=\"footnote\"\u003E49\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref51\" rel=\"footnote\"\u003E51\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and the pathways language model (version 2) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. A smaller subset of the studies (14\u002F37, 38%) adopted open-source models, such as LLM Meta AI (LLaMA) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref29\" rel=\"footnote\"\u003E29\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref30\" rel=\"footnote\"\u003E30\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref32\" rel=\"footnote\"\u003E32\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref52\" rel=\"footnote\"\u003E52\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref57\" rel=\"footnote\"\u003E57\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref58\" rel=\"footnote\"\u003E58\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], Mistral [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref33\" rel=\"footnote\"\u003E33\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], Falcon [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and Neomotron [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref55\" rel=\"footnote\"\u003E55\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Beyond model selection, several studies (29\u002F37, 78%) explored technical strategies to enhance diagnostic performance and interpretability. In total, 3 main approaches were identified as described in subsequent sections.\u003C\u002Fp\u003E\u003Ch6\u003EHybrid Modeling\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EA limited number of studies (2\u002F37, 5%) explored hybrid architectures, combining GenAI-generated embeddings with classical classifiers, such as SVM or random forest [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref53\" rel=\"footnote\"\u003E53\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For example, Radwan et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref53\" rel=\"footnote\"\u003E53\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] used GPT-3 embeddings to generate text vectors, which were input into classifiers, such as SVM, random forest, and k-nearest neighbors, for stress level classification. The combination of GPT-3 embeddings with an SVM classifier yielded the best performance, outperforming other hybrid configurations and traditional models such as BERT with the long short-term memory model.\u003C\u002Fp\u003E\u003Ch6\u003EFine-Tuning and Instruction Adaptation\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003ESome studies (4\u002F37, 11%) used instruction-tuned models, including Flan [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref41\" rel=\"footnote\"\u003E41\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], Alpaca [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and Wizard [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref32\" rel=\"footnote\"\u003E32\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], to enhance instruction following. Further fine-tuning with mental health&#x2013;related data was also applied to improve diagnostic and assessment capabilities [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref46\" rel=\"footnote\"\u003E46\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref59\" rel=\"footnote\"\u003E59\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For instance, Xu et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] demonstrated that their fine-tuned models&#x2014;Mental-Alpaca and Mental-FLAN-T5&#x2014;achieved a 10.9% improvement in balanced accuracy over GPT-3.5 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref4\" rel=\"footnote\"\u003E4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], despite being 25 and 15 times smaller, respectively. These models also outperformed GPT-4 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref104\" rel=\"footnote\"\u003E104\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] by 4.8%, although GPT-4 is 250 and 150 times larger, respectively.\u003C\u002Fp\u003E\u003Ch6\u003EPrompt Engineering and Knowledge Augmentation\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EPrompt-based techniques&#x2014;including few-shot learning [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref31\" rel=\"footnote\"\u003E31\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref46\" rel=\"footnote\"\u003E46\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref54\" rel=\"footnote\"\u003E54\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref58\" rel=\"footnote\"\u003E58\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], chain-of-thought prompting [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref49\" rel=\"footnote\"\u003E49\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref59\" rel=\"footnote\"\u003E59\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref60\" rel=\"footnote\"\u003E60\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and example contrast [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref54\" rel=\"footnote\"\u003E54\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]&#x2014;have been shown to substantially enhance diagnostic performance, especially for smaller models [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Meanwhile, retrieval-augmented generation (RAG) approaches enriched LLMs with structured knowledge (eg, \u003Ci\u003EDiagnostic and Statistical Manual of Mental Disorders, Fifth Edition\u003C\u002Fi\u003E criteria), improving factual grounding in some cases [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], but occasionally introducing noise or reducing performance due to redundancy and semantic drift [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref55\" rel=\"footnote\"\u003E55\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EData Source\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003E\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#table1\" rel=\"footnote\"\u003ETable 1\u003C\u002Fa\u003E\u003C\u002Fspan\u003E summarizes the datasets used for GenAI-based mental health diagnosis and assessment, categorized by data modality and mental health focus. The full dataset list, including metadata and sampling details, is provided in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#app6\" rel=\"footnote\"\u003EMultimedia Appendix 6\u003C\u002Fa\u003E\u003C\u002Fspan\u003E [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref30\" rel=\"footnote\"\u003E30\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref35\" rel=\"footnote\"\u003E35\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref41\" rel=\"footnote\"\u003E41\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref49\" rel=\"footnote\"\u003E49\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref53\" rel=\"footnote\"\u003E53\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref62\" rel=\"footnote\"\u003E62\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref103\" rel=\"footnote\"\u003E103\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref105\" rel=\"footnote\"\u003E105\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref133\" rel=\"footnote\"\u003E133\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ESocial media posts, such as those on Reddit [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref29\" rel=\"footnote\"\u003E29\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], Twitter [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref58\" rel=\"footnote\"\u003E58\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and Weibo [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref45\" rel=\"footnote\"\u003E45\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], emerged as prominent data sources. Beyond social media, 19% (7\u002F37) of the studies used professionally curated clinical vignettes, providing controlled scenarios that simulate clinical cases and allow for standardized assessment across GenAI models [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref34\" rel=\"footnote\"\u003E34\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref51\" rel=\"footnote\"\u003E51\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Only a few studies (4\u002F37, 11%) used clinical text data sources, including clinical interviews [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref61\" rel=\"footnote\"\u003E61\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], diary texts [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and written responses of participants [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref50\" rel=\"footnote\"\u003E50\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref62\" rel=\"footnote\"\u003E62\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EIn total, 14% (5\u002F37) of the studies used multimodal data sources&#x2014;such as speech [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref45\" rel=\"footnote\"\u003E45\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], sensor data [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and electroencephalogram (EEG) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref46\" rel=\"footnote\"\u003E46\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]&#x2014;to enhance the accuracy and comprehensiveness of mental health assessments. For example, Englhardt et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] developed prompting strategies for GenAI models to classify depression using passive sensing data (eg, activity, sleep, and social behavior) from mobile and wearable devices, achieving improved classification accuracy (up to 61.1%) over classical machine learning baselines. Similarly, Hu et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref46\" rel=\"footnote\"\u003E46\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] integrated EEG, audio, and facial expressions to boost predictive performance and proposed MultiEEG-GPT, a GPT-4o-based method for mental health assessment using multimodal inputs, including EEG, facial expressions, and audio. Their results across the 3 datasets showed that combining EEG with audio or facial expressions significantly improved prediction accuracy in both zero-shot and few-shot settings.\u003C\u002Fp\u003E\u003Cdiv class=\"figure-table\"\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-clipboard\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003ETable 1. \u003C\u002Fb\u003ESummary of datasets used in the studies on generative artificial intelligence (GenAI) models for mental health diagnosis and assessment.\u003C\u002Ffigcaption\u003E\u003Ctable width=\"1000\" cellpadding=\"5\" cellspacing=\"0\" border=\"1\" rules=\"groups\" frame=\"hsides\"\u003E\u003Ccol width=\"30\" span=\"1\"\u003E\u003Ccol width=\"470\" span=\"1\"\u003E\u003Ccol width=\"0\" span=\"1\"\u003E\u003Ccol width=\"500\" span=\"1\"\u003E\u003Cthead\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd colspan=\"3\" rowspan=\"1\"\u003ECategories\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EReferences\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd colspan=\"4\" rowspan=\"1\"\u003E\u003Cb\u003EBy modality\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EText (clinical vignettes)\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref105\" rel=\"footnote\"\u003E105\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref108\" rel=\"footnote\"\u003E108\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EText (social media posts)\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref53\" rel=\"footnote\"\u003E53\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref109\" rel=\"footnote\"\u003E109\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref123\" rel=\"footnote\"\u003E123\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref134\" rel=\"footnote\"\u003E134\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EText (transcripts)\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref35\" rel=\"footnote\"\u003E35\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref124\" rel=\"footnote\"\u003E124\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EText (daily self-reports)\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref62\" rel=\"footnote\"\u003E62\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EMultimodal dataset\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref49\" rel=\"footnote\"\u003E49\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref125\" rel=\"footnote\"\u003E125\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref131\" rel=\"footnote\"\u003E131\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd colspan=\"4\" rowspan=\"1\"\u003E\u003Cb\u003EBy mental health issues\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EDepression\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref62\" rel=\"footnote\"\u003E62\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref109\" rel=\"footnote\"\u003E109\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref110\" rel=\"footnote\"\u003E110\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref113\" rel=\"footnote\"\u003E113\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref114\" rel=\"footnote\"\u003E114\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref117\" rel=\"footnote\"\u003E117\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref123\" rel=\"footnote\"\u003E123\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref126\" rel=\"footnote\"\u003E126\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref128\" rel=\"footnote\"\u003E128\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref130\" rel=\"footnote\"\u003E130\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref134\" rel=\"footnote\"\u003E134\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ESuicide risk\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref12\" rel=\"footnote\"\u003E12\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref35\" rel=\"footnote\"\u003E35\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref40\" rel=\"footnote\"\u003E40\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref108\" rel=\"footnote\"\u003E108\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref109\" rel=\"footnote\"\u003E109\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref111\" rel=\"footnote\"\u003E111\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref112\" rel=\"footnote\"\u003E112\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref118\" rel=\"footnote\"\u003E118\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref119\" rel=\"footnote\"\u003E119\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref121\" rel=\"footnote\"\u003E121\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref122\" rel=\"footnote\"\u003E122\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EPosttraumatic stress disorder\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref110\" rel=\"footnote\"\u003E110\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref125\" rel=\"footnote\"\u003E125\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref127\" rel=\"footnote\"\u003E127\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EAnxiety\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref115\" rel=\"footnote\"\u003E115\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref125\" rel=\"footnote\"\u003E125\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref128\" rel=\"footnote\"\u003E128\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EBipolar disorder\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref120\" rel=\"footnote\"\u003E120\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref124\" rel=\"footnote\"\u003E124\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EStress\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref53\" rel=\"footnote\"\u003E53\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref115\" rel=\"footnote\"\u003E115\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref132\" rel=\"footnote\"\u003E132\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EEmotion regulation\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref62\" rel=\"footnote\"\u003E62\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref129\" rel=\"footnote\"\u003E129\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref131\" rel=\"footnote\"\u003E131\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E\u003Cbr\u003E\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EMultiple psychiatric disorders\u003C\u002Ftd\u003E\u003Ctd colspan=\"2\" rowspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref63\" rel=\"footnote\"\u003E63\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref106\" rel=\"footnote\"\u003E106\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref107\" rel=\"footnote\"\u003E107\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref133\" rel=\"footnote\"\u003E133\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Ch4\u003EGenAI as Therapeutic Tools\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EOf the 79 included studies, 20 (25%) investigated the use of GenAI-based chatbots and conversational agents to facilitate interventions ranging from emotional support to more structured therapies. To assess the feasibility and potential impact of these interventions, we analyzed studies across four key dimensions: (1) therapeutic targets, (2) implementation strategies, (3) evaluation outcomes, and (4) real-world deployment features.\u003C\u002Fp\u003E\u003Ch5\u003EIntervention Targets and Theoretical Alignments\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EAs presented in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure5\" rel=\"footnote\"\u003EFigure 5\u003C\u002Fa\u003E\u003C\u002Fspan\u003E, most studies (16\u002F20, 80%) targeted the general population. A smaller subset (5\u002F20, 25%) focused on vulnerable or underserved groups, including outpatients undergoing psychiatric treatment [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], lesbian, gay, bisexual, transgender, and queer (LGBTQ) individuals [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref65\" rel=\"footnote\"\u003E65\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], sexual harassment survivors [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], children with attention-deficit\u002Fhyperactivity disorder [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref67\" rel=\"footnote\"\u003E67\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and older adults [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref68\" rel=\"footnote\"\u003E68\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In addition to population-specific adaptations, some studies (4\u002F20, 20%) focused on chatbots targeting specific psychological and behavioral challenges, including attention-deficit\u002Fhyperactivity disorder [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref67\" rel=\"footnote\"\u003E67\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], problematic smartphone use [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], preoperative anxiety [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and relationship issues [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref71\" rel=\"footnote\"\u003E71\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EDespite the growing prevalence of these systems, most studies do not explicitly state the theoretical frameworks guiding their development. Among the reviewed studies, only 30% (6\u002F20) of the studies explicitly adopted a psychological theory: person-centered therapy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref72\" rel=\"footnote\"\u003E72\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]; cognitive behavioral therapy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref75\" rel=\"footnote\"\u003E75\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]; and existence, relatedness, and growth theory [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EBeyond chatbot-based interventions, several studies (2\u002F20, 10%) used passive monitoring, combining real-time physiological [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref74\" rel=\"footnote\"\u003E74\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and behavioral data [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref8\" rel=\"footnote\"\u003E8\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] from wearables to assess mental states and trigger interventions. For example, empathic LLMs developed by Dongre [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref74\" rel=\"footnote\"\u003E74\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] adapted responses based on users&#x2019; stress levels, achieved 85.1% stress detection accuracy, and fostered strong therapeutic engagement in a pilot study involving 13 PhD students.\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Ca name=\"figure5\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 5. Sankey diagram mapping target group, problem, and theoretical framework in generative artificial intelligence&#x2013;based mental health therapy research. ADHD: attention-deficit\u002Fhyperactivity disorder; CBT: cognitive behavioral therapy; ERG: existence, relatedness, and growth; LGBTQ+: lesbian, gay, bisexual, transgender, queer, and other minority groups; PCT: present&#x2010;centered therapy.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F13aa64121356c3067c9b405caf1f6aff.png\" id=\"figure5\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F13aa64121356c3067c9b405caf1f6aff.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 5. \u003C\u002Fb\u003E Sankey diagram mapping target group, problem, and theoretical framework in generative artificial intelligence&#x2013;based mental health therapy research. ADHD: attention-deficit\u002Fhyperactivity disorder; CBT: cognitive behavioral therapy; ERG: existence, relatedness, and growth; LGBTQ+: lesbian, gay, bisexual, transgender, queer, and other minority groups; PCT: present&#x2010;centered therapy. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch5\u003EEvaluation Strategies and Reported Outcomes\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EEvaluation methods across the included studies varied considerably in terms of design, measurement, and reported outcomes. Approximately one-third of the included studies (7\u002F20, 35%) used structured experimental designs, including randomized controlled trials [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], field experiments [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and quasi-experimental studies [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], with intervention spanning from one session to several weeks. These studies reported improvements in emotional intensity [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], anxiety [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], or behavioral outcomes [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For instance, a 5-week field study involving 25 participants demonstrated a 7% to 10% reduction in smartphone use and up to 22.5% improvement in intervention acceptance [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Several studies (5\u002F20, 25%) conducted simulated evaluations using test scenarios [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], prompt-response validation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref76\" rel=\"footnote\"\u003E76\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and expert review [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref67\" rel=\"footnote\"\u003E67\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref77\" rel=\"footnote\"\u003E77\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. A third group used user-centered approaches, such as semistructured interviews [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref65\" rel=\"footnote\"\u003E65\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], open-ended surveys [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref72\" rel=\"footnote\"\u003E72\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], or retrospective analyses of user-generated content [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EEvaluation metrics were clustered into several domains. A substantial number of studies (14\u002F20, 70%) assessed subjective user experiences, such as emotional relief, satisfaction, engagement, and self-efficacy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref71\" rel=\"footnote\"\u003E71\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. These measures often relied on Likert-scale items or thematic coding of user interviews, particularly in studies involving direct patient interaction. Standardized psychometric instruments were applied in several studies to quantify clinical outcomes, such as the State-Trait Anxiety Inventory [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and the Self-Efficacy Scale [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In contrast, studies focused on technical development predominantly adopted automated metrics, such as perplexity, bilingual evaluation understudy scores, and top-k accuracy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref78\" rel=\"footnote\"\u003E78\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EAcross these varied approaches, most studies (17\u002F20, 85%) reported positive outcomes. Emotional support functions were generally well received, with users describing increased affective relief [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], perceived empathy [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref65\" rel=\"footnote\"\u003E65\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and greater openness to self-reflection [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref75\" rel=\"footnote\"\u003E75\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Structured interventions showed measurable improvements in behavior, including reduced problematic smartphone use and increased adherence to interventions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Nevertheless, several studies (5\u002F20, 25%) highlighted users&#x2019; concerns regarding personalization, contextual fit, and trust [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Moreover, while GenAI models often succeeded in simulating supportive interactions, they struggled to offer nuanced responses or adapt to complex individual needs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref65\" rel=\"footnote\"\u003E65\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Users also raised concerns about repetitive phrasing, overly generic suggestions, and insufficient safety mechanisms, particularly in high-stakes scenarios such as crisis intervention or identity-sensitive disclosures [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref71\" rel=\"footnote\"\u003E71\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EModel Architectures and Adaptation Strategies\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EThe included studies used a variety of base models, with GPT-series being the most frequently adopted across interventions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref60\" rel=\"footnote\"\u003E60\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref67\" rel=\"footnote\"\u003E67\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref68\" rel=\"footnote\"\u003E68\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref71\" rel=\"footnote\"\u003E71\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref75\" rel=\"footnote\"\u003E75\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref76\" rel=\"footnote\"\u003E76\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. A small set of studies (6\u002F20, 30%) used alternatives such as Falcon [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref74\" rel=\"footnote\"\u003E74\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], LLaMA [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref77\" rel=\"footnote\"\u003E77\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], or custom transformer-based architectures [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref72\" rel=\"footnote\"\u003E72\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref78\" rel=\"footnote\"\u003E78\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo tailor GenAI models for mental health applications, researchers have adopted a range of adaptation techniques. Prompt engineering was the most frequently applied strategy. This approach included emotional state-sensitive prompting [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref69\" rel=\"footnote\"\u003E69\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and modular prompt templates [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref60\" rel=\"footnote\"\u003E60\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. A smaller number of studies (2\u002F20, 10%) applied fine-tuning strategies using real-world therapy dialogues or support data [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref77\" rel=\"footnote\"\u003E77\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For instance, Yu and McGuinness [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] fine-tuned DialoGPT on 5000 therapy conversations and layered it with knowledge-injected prompts via ChatGPT-3.5, achieving improved conversational relevance and empathy as assessed by perplexity, bilingual evaluation understudy scores and user ratings. Herencia [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref77\" rel=\"footnote\"\u003E77\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] used Low-Rank Adaptation to fine-tune LLaMA-2 on mental health dialogue data, resulting in a fine-tuned model that outperformed the base LLaMA in BERT and Metric for Evaluation of Translation with Explicit Ordering scores, with reduced inference time and improved contextual sensitivity in simulated counseling interactions.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EBeyond internal adaptations, RAG was used to enrich responses with external knowledge. For instance, Vakayil et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] integrated RAG into a LLaMA-2&#x2013;based chatbot to support survivors of sexual harassment, combining empathetic dialogue with accurate legal and crisis information drawn from a curated database.\u003C\u002Fp\u003E\u003Ch5\u003EClinical Readiness\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo evaluate the translational potential of GenAI models into clinical practice, we synthesized four indicators of real-world readiness across the included studies: (1) expert evaluation, (2) user acceptability, (3) clinical deployment, and (4) safety mechanisms. Among the 20 studies reviewed, only 4 (20%) involved formal expert evaluation, such as ratings by licensed clinicians or psychiatric specialists [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref67\" rel=\"footnote\"\u003E67\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref68\" rel=\"footnote\"\u003E68\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In contrast, user acceptability was more frequently assessed, with 60% (12\u002F20) of the studies reporting participant feedback on usability, supportiveness, or trust in GenAI. Clinical implementation was reported in only 15% (3\u002F20) of the studies conducted in real-world or quasi-clinical settings. Regarding safety, only 30% (6\u002F20) of the studies implemented explicit safety measures, such as toxicity filters [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], crisis response triggers [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], or expert validation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch4\u003EGenAI for Supporting Clinicians and Mental Health Professionals\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EOf the 79 included studies, 24 (30%) focused on applying GenAI to support clinicians and mental health professionals, with 2 (2%) overlapping with the research on GenAI models for mental health diagnosis and assessment.\u003C\u002Fp\u003E\u003Ch5\u003ERole of GenAI in Supporting Clinicians and Mental Health Professionals\u003C\u002Fh5\u003E\u003Ch6\u003EOverview\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003ERecent research has demonstrated a growing interest in the use of GenAI to support mental health professionals across diverse clinical tasks. Drawing on a synthesis of empirical studies (\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#table2\" rel=\"footnote\"\u003ETable 2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E), we identified five core functional roles through which GenAI contributes to mental health services: (1) clinical decision support, (2) documentation and summarization, (3) therapy support, (4) psychoeducation, and (5) training and simulation.\u003C\u002Fp\u003E\u003Cdiv class=\"figure-table\"\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-clipboard\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003ETable 2. \u003C\u002Fb\u003ECategorization of generative artificial intelligence (GenAI) support roles and representative applications in mental health contexts.\u003C\u002Ffigcaption\u003E\u003Ctable width=\"1000\" cellpadding=\"5\" cellspacing=\"0\" border=\"1\" rules=\"groups\" frame=\"hsides\"\u003E\u003Ccol width=\"270\" span=\"1\"\u003E\u003Ccol width=\"530\" span=\"1\"\u003E\u003Ccol width=\"200\" span=\"1\"\u003E\u003Cthead\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ESupport roles\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ERepresentative tasks\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EReferences\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\u003Ctbody\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EClinical decision support\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ETreatment planning, prognosis, and case formulation\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref80\" rel=\"footnote\"\u003E80\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref87\" rel=\"footnote\"\u003E87\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref89\" rel=\"footnote\"\u003E89\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref96\" rel=\"footnote\"\u003E96\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EDocumentation and summarization\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ESummarizing counseling sessions and summarization of multimodal sensor data\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref88\" rel=\"footnote\"\u003E88\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ETherapy support\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EReframing, emotion extraction, and reflection\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref90\" rel=\"footnote\"\u003E90\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref93\" rel=\"footnote\"\u003E93\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EPsychoeducation\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003EQuestions and answers, recommendations, and interactive guidance\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref63\" rel=\"footnote\"\u003E63\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref80\" rel=\"footnote\"\u003E80\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref94\" rel=\"footnote\"\u003E94\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref98\" rel=\"footnote\"\u003E98\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr valign=\"top\"\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ETraining and simulation\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003ECase vignettes and synthetic data\u003C\u002Ftd\u003E\u003Ctd rowspan=\"1\" colspan=\"1\"\u003E[\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref9\" rel=\"footnote\"\u003E9\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref84\" rel=\"footnote\"\u003E84\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref99\" rel=\"footnote\"\u003E99\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Ch6\u003EClinical Decision Support\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EOne of the most frequently studied applications of GenAI is its use in supporting clinical decision-making. This includes tasks such as treatment planning [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref80\" rel=\"footnote\"\u003E80\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], case formulation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref83\" rel=\"footnote\"\u003E83\u003C\u002Fa\u003E\u003C\u002Fspan\u003E-\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref85\" rel=\"footnote\"\u003E85\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and prognosis assessment [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref86\" rel=\"footnote\"\u003E86\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref87\" rel=\"footnote\"\u003E87\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Studies show that GenAI-generated treatment plans are often consistent with clinical guidelines and therapeutic theories [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref85\" rel=\"footnote\"\u003E85\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and sometimes outperform general practitioners in adherence [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref81\" rel=\"footnote\"\u003E81\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For case formulation, GenAI has been shown to produce coherent and theory-driven conceptualizations, including psychodynamic [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref83\" rel=\"footnote\"\u003E83\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and multimodal [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref84\" rel=\"footnote\"\u003E84\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] therapy. Prognostic predictions for mental health conditions such as depression [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref87\" rel=\"footnote\"\u003E87\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and schizophrenia [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref86\" rel=\"footnote\"\u003E86\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] have also shown expert-level agreement. However, when used for engaging directly with patients for clinical assessment, GenAI models still lack capabilities in structured interviewing and differential diagnosis [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref80\" rel=\"footnote\"\u003E80\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch6\u003EDocumentation and Summarization\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EGenAI models have also demonstrated potential in reducing clinicians&#x2019; administrative burden through automated documentation. Adhikary et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref88\" rel=\"footnote\"\u003E88\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] benchmarked 11 LLMs on their ability to summarize mental health counseling sessions, identifying Mistral and MentalLLaMA as having the highest extractive quality. Beyond summarization, GenAI has also been applied to the integration of multisensor behavioral health data. Englhardt et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] examined LLMs&#x2019; ability to analyze passive sensing data for assessing mental health conditions such as depression and anxiety. Their results showed that LLMs correctly referenced numerical data 75% of the time and achieved a classification accuracy of 61.1%, surpassing traditional machine learning models. However, both studies identified hallucination as a critical limitation, including errors such as incorrect documentation of suicide risk [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref88\" rel=\"footnote\"\u003E88\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch6\u003ETherapy Support\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EA growing body of research suggests that GenAI can enhance therapeutic processes by supporting treatment goal setting [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref89\" rel=\"footnote\"\u003E89\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], emotional reflection [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref90\" rel=\"footnote\"\u003E90\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], cognitive restructuring [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref91\" rel=\"footnote\"\u003E91\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref92\" rel=\"footnote\"\u003E92\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and motivational interviewing [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref93\" rel=\"footnote\"\u003E93\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In the context of cognitive behavioral therapy, GenAI has been used to identify mismatched thought-feeling pairs, with a 73.5% cross-validated accuracy rate [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref91\" rel=\"footnote\"\u003E91\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and to assist in reframing maladaptive cognitions with high rates of successful reconstruction [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref92\" rel=\"footnote\"\u003E92\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Other therapeutic applications include guided journaling for mood tracking, which has been shown to increase patient engagement and emotional awareness [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref90\" rel=\"footnote\"\u003E90\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch6\u003EPsychoeducation\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EGenAI has been used to provide accessible mental health information to the public, with studies showing that it can deliver accurate and actionable content while maintaining empathetic tone [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref94\" rel=\"footnote\"\u003E94\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. GenAI has also been explored as a tool for creating interactive psychoeducational experiences, particularly for children and adolescents, through role-playing and other engagement strategies [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref95\" rel=\"footnote\"\u003E95\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For example, Hu et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref96\" rel=\"footnote\"\u003E96\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] developed a child-facing GenAI agent designed to foster psychological resilience, which demonstrated improvements in both engagement and mental health outcomes. Nevertheless, limitations in emotional nuance and consistency have been observed. For example, Giorgi et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref97\" rel=\"footnote\"\u003E97\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] documented harmful outputs in substance use queries, and comparative analyses have shown that GenAI often lacks the emotional attunement characteristic of human clinicians [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref63\" rel=\"footnote\"\u003E63\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref98\" rel=\"footnote\"\u003E98\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch6\u003ETraining and Simulation\u003C\u002Fh6\u003E\u003Cp class=\"abstract-paragraph\"\u003EBeyond direct patient care, GenAI has been increasingly applied in clinical education as low-risk tools for skill development and reasoning practice. They have been used to generate case vignettes, simulate diagnostic interviews, support self-directed learning, prompt clinical reasoning, and create synthetic datasets for model development [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref9\" rel=\"footnote\"\u003E9\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref84\" rel=\"footnote\"\u003E84\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref99\" rel=\"footnote\"\u003E99\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], offering scalable solutions for training, especially in resource-limited settings.\u003C\u002Fp\u003E\u003Ch5\u003EModeling and Evaluation Strategies in GenAI for Mental Health Support\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EGPT-3.5 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref4\" rel=\"footnote\"\u003E4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and GPT-4 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref104\" rel=\"footnote\"\u003E104\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] were the most frequently used models for clinician support tasks [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref81\" rel=\"footnote\"\u003E81\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref83\" rel=\"footnote\"\u003E83\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref84\" rel=\"footnote\"\u003E84\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref89\" rel=\"footnote\"\u003E89\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref90\" rel=\"footnote\"\u003E90\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], yet comparative findings reveal that no single model consistently outperforms others. For instance, Bard (rebranded as Gemini) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref135\" rel=\"footnote\"\u003E135\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] has been shown to outperform GPT-4 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref104\" rel=\"footnote\"\u003E104\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] in reconstructing negative thoughts [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref92\" rel=\"footnote\"\u003E92\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and LLaMA-2 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref136\" rel=\"footnote\"\u003E136\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] surpasses GPT-4 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref104\" rel=\"footnote\"\u003E104\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] in adequacy, appropriateness, and overall quality when addressing substance use-related questions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref97\" rel=\"footnote\"\u003E97\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. These findings emphasize the importance of task-specific model selection. Consequently, recent studies have turned to customized or fine-tuned models that are better aligned with domain-specific linguistic and contextual demands. For example, Furukawa et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref91\" rel=\"footnote\"\u003E91\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] used a fine-tuned Japanese T5 model [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref24\" rel=\"footnote\"\u003E24\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] to assist clinicians in emotion prediction during cognitive restructuring. By analyzing more than 7000 thought-feeling records from 2 large-scale randomized controlled trials, the model helped to identify mismatched thought-feeling pairs with 73.5% accuracy. Empirical studies further support this approach, demonstrating that domain-specific models consistently outperform general-purpose models in mental health care tasks [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref88\" rel=\"footnote\"\u003E88\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EA range of adaptation strategies and evaluation methods were identified across the included studies. As illustrated in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure6\" rel=\"footnote\"\u003EFigure 6\u003C\u002Fa\u003E\u003C\u002Fspan\u003E, prompt engineering was the most common strategy, especially in clinical decision support [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref89\" rel=\"footnote\"\u003E89\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], psychoeducation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref63\" rel=\"footnote\"\u003E63\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref97\" rel=\"footnote\"\u003E97\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and therapy support tasks [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref90\" rel=\"footnote\"\u003E90\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref93\" rel=\"footnote\"\u003E93\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Fine-tuning was used less frequently, limited to contexts with domain-specific corpora (eg, documentation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref88\" rel=\"footnote\"\u003E88\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and emotion classification [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref96\" rel=\"footnote\"\u003E96\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]). Modular orchestration strategies were identified in only a small number (2\u002F24, 8%) of studies [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref95\" rel=\"footnote\"\u003E95\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref96\" rel=\"footnote\"\u003E96\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Ca name=\"figure6\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 6. Sankey diagram showing the methodological flow in generative artificial intelligence&#x2013;based mental health support research.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F73a555d548f48e404a8b2ff9292091f5.png\" id=\"figure6\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F73a555d548f48e404a8b2ff9292091f5.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 6. \u003C\u002Fb\u003E Sankey diagram showing the methodological flow in generative artificial intelligence&#x2013;based mental health support research. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"abstract-paragraph\"\u003EEvaluation methods also varied by task type. Clinical and diagnostic tasks favored expert review [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref80\" rel=\"footnote\"\u003E80\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref84\" rel=\"footnote\"\u003E84\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and automated metrics [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref88\" rel=\"footnote\"\u003E88\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref92\" rel=\"footnote\"\u003E92\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], whereas patient-facing tasks&#x2014;such as psychoeducation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref96\" rel=\"footnote\"\u003E96\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and emotional support [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref93\" rel=\"footnote\"\u003E93\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]&#x2014;relied more on user-centered feedback or psychometric assessments.\u003C\u002Fp\u003E\u003Ch5\u003EClinical Readiness\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EAmong the 24 studies reviewed, only 2 (8%) involved real-world clinical deployment [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref51\" rel=\"footnote\"\u003E51\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref91\" rel=\"footnote\"\u003E91\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Expert evaluation was reported in more than 80% (20\u002F24) of the studies, while user acceptability appeared in only 25% (6\u002F24) of the studies. Safety mechanisms&#x2014;such as hallucination control, bias mitigation, and clinician override&#x2014;were explicitly implemented in 17% (4\u002F24) of the studies.\u003C\u002Fp\u003E\u003Ch4\u003EReporting Quality of Included Studies\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EWe assessed the reporting quality of the included studies using the MI-CLAIM-GEN checklist [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref101\" rel=\"footnote\"\u003E101\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Each item was scored on a 4-point scale (yes, no, unsure, and not applicable) following the Joanna Briggs Institute quality appraisal format [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref102\" rel=\"footnote\"\u003E102\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. The results are presented in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure7\" rel=\"footnote\"\u003EFigure 7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E.\u003C\u002Fp\u003E\u003Cfigure\u003E\u003Ca name=\"figure7\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ca class=\"fancybox\" title=\"Figure 7. Reporting quality of the included studies based on the Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research (MI-CLAIM-GEN) checklist.\" href=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F9987a8143a8bc93d3dfe90b1d00fe6ca.png\" id=\"figure7\"\u003E\u003Cimg class=\"figure-image\" src=\"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002F9987a8143a8bc93d3dfe90b1d00fe6ca.png\"\u003E\u003C\u002Fa\u003E\u003Cfigcaption\u003E\u003Cspan class=\"typcn typcn-image\"\u003E\u003C\u002Fspan\u003E\u003Cb\u003EFigure 7. \u003C\u002Fb\u003E Reporting quality of the included studies based on the Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research (MI-CLAIM-GEN) checklist. \u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"abstract-paragraph\"\u003EOn average, 45.39% (753\u002F1659) of items were rated as \u003Ci\u003Eyes\u003C\u002Fi\u003E, indicating a moderate level of reporting transparency across the corpus. Reporting completeness varied substantially across the items, and only 10 items achieved \u003Ci\u003Eyes\u003C\u002Fi\u003E ratings in more than half (40\u002F79, 51%) of the studies. As shown in \u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure7\" rel=\"footnote\"\u003EFigure 7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E, items related to study design (items 1.1&#x2013;1.5), model performance and evaluation (items 3.1&#x2013;3.4), and model examination (items 4.1&#x2013;4.5) were most consistently reported, with 73.9% (292\u002F395), 56% (177\u002F316), and 54.1% (171\u002F316) of the studies achieving \u003Ci\u003Eyes\u003C\u002Fi\u003E ratings, respectively. In contrast, items concerning resources and optimization (items 2.1&#x2013;2.4) and reproducibility (items 5.1&#x2013;5.3) were frequently underreported, with 25.3% (100\u002F395) and 5.5% (13\u002F237) of the studies providing sufficient information in these areas.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EItem-level analysis further revealed critical disparities. Core design elements were consistently addressed&#x2014;for instance, 1.1 (study context) and 1.2 (research question) received \u003Ci\u003Eyes\u003C\u002Fi\u003E ratings in 97% (77\u002F79) and 100% (79\u002F79) of the studies, respectively. However, items, such as 1.5 (representativeness of training data) were often overlooked, with only 11% (9\u002F79) of studies providing sufficient reporting. Similarly, while 89% (70\u002F79) of the studies described model outputs (item 3c), only 20% (16\u002F79) of the studies included a comprehensive evaluation framework (item 3b). Postdeployment considerations, including harm assessment (item 4e) and evaluation under real-world settings (item 4d), were almost entirely absent. In the reproducibility domain, none of the studies provided a model card (item 5b), and only 14% (11\u002F79) of the studies reached tier-1 reproducibility by reporting sufficient implementation details (item 5a).\u003C\u002Fp\u003E\u003Ch4\u003EEthical Issues and the Responsible Use of GenAI in Mental Health\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EOn the basis of the analysis of ethical concerns identified across the included studies, we synthesized 4 core domains&#x2014;data privacy, information integrity, user safety, and ethical governance and oversight. Drawing on these dimensions, we proposed the GenAI4MH ethical framework (\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#figure2\" rel=\"footnote\"\u003EFigure 2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E) to comprehensively address the unique ethical challenges in this domain and guide the responsible design, deployment, and use of GenAI in mental health contexts.\u003C\u002Fp\u003E\u003Ch5\u003EData Privacy and Security\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EThe use of GenAI in mental health settings raises heightened concerns regarding data privacy due to the inherently sensitive nature of psychological data. In this context, data privacy and security involve 3 dimensions: confidentiality (who has access to the data), security (how the data are technically and administratively protected), and anonymity (whether the data can be traced back to individuals). Both users [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and clinicians [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref47\" rel=\"footnote\"\u003E47\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] reported concerns about sharing sensitive information with GenAI, citing a lack of clarity on data storage and regulatory oversight [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. These concerns are further amplified in vulnerable populations, including children [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref96\" rel=\"footnote\"\u003E96\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and LGBTQ individuals [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref65\" rel=\"footnote\"\u003E65\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo mitigate these risks, previous studies proposed 2 main strategies. First, platforms should implement transparency notices that clearly inform users of potential data logging and caution against disclosing personally identifiable or highly sensitive information [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Second, systems should incorporate real-time filtering and alert mechanisms to detect and block unauthorized disclosures, such as names and contact details, especially during emotionally charged interactions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref67\" rel=\"footnote\"\u003E67\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EInformation Integrity and Fairness\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EInformation integrit\u003Ci\u003Ey\u003C\u002Fi\u003E and fairness refers to the factual correctness, fairness, reliability, and cultural appropriateness of GenAI-generated outputs. A central challenge lies in the presence of systematic biases. Heinz et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] found that LLMs reproduced real-world disparities: American Indian and Alaska Native individuals were more likely to be labeled with substance use disorders, and women with borderline personality disorder. Although not all patterns of bias were observed&#x2014;for instance, the overdiagnosis of psychosis in Black individuals&#x2014;other studies reported similar trends. Perlis et al [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] noted reduced recommendation accuracy for Black women, while Soun and Nair [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref38\" rel=\"footnote\"\u003E38\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] identified performance disparities across gender, favoring young women over older men.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EGenAI models also show limited cross-cultural adaptability. Performance drops have been observed in dialectal and underrepresented language contexts [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref44\" rel=\"footnote\"\u003E44\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and users have reported that GenAI models fail to interpret nuanced cultural norms or offer locally appropriate mental health resources [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Another major concern involves consistency and factual reliability. GenAI models have been found to generate medically inaccurate or harmful content, including nonexistent drugs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], contradicted medications [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], incorrect hotline information [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and unsupported interventions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Some models hallucinated suicide behaviors [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref35\" rel=\"footnote\"\u003E35\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] or missed explicit crisis signals [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In one study, nearly 80% of users reported encountering outdated, biased, or inaccurate outputs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref22\" rel=\"footnote\"\u003E22\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Moreover, outputs often vary across minor prompt changes and repeated runs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and the temporal lag between model training and deployment may result in misalignment with current psychiatric guidelines [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref99\" rel=\"footnote\"\u003E99\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo address these challenges, a range of mitigation strategies has been proposed across fairness, cultural adaptation, factual integrity, and response consistency. For bias and fairness, researchers have proposed several strategies targeting the underlying causes&#x2014;most notably, the skewed demographic representation in training data [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref38\" rel=\"footnote\"\u003E38\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. These approaches include value-aligned data augmentation, training set debiasing, and increasing the diversity of demographic groups represented in both training and evaluation datasets [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Instruction-tuned models developed specifically for mental health tasks have also demonstrated improved subgroup performance and fairness across gender and age groups [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref55\" rel=\"footnote\"\u003E55\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. To enhance cultural adaptability, studies have proposed multilingual fine-tuning, dialect-specific testing, and adaptive language modeling tailored to users&#x2019; linguistic and sociodemographic backgrounds [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref42\" rel=\"footnote\"\u003E42\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For improving factual reliability and reducing hallucinations, techniques include conservative prompting (eg, yes or no formats) [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref41\" rel=\"footnote\"\u003E41\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], factual verification pipelines [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref97\" rel=\"footnote\"\u003E97\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and RAG from validated clinical sources [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref55\" rel=\"footnote\"\u003E55\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Domain-specific fine-tuning [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref58\" rel=\"footnote\"\u003E58\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], hallucination detection tools, manual output review, and ensemble modeling [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref52\" rel=\"footnote\"\u003E52\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] have also shown promise. In addition, some studies incorporate real-time web retrieval to reduce outdated information and increase clinical relevance [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref66\" rel=\"footnote\"\u003E66\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. To promote response consistency, researchers have applied parameter-controlled generation and reduced model temperature, both of which have been shown to decrease output variability across repeated prompts [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref2\" rel=\"footnote\"\u003E2\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EUser Safety\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EIn mental health contexts, user safety refers to protecting individuals from psychological harm, ethical breaches, or misguided decision-making caused by GenAI. Ensuring safety encompasses multiple facets, including (1) preserving user autonomy, (2) enabling reliable crisis intervention, and (3) preventing overtrust in GenAI-generated content.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EA major concern is the erosion of user autonomy. Users may mistakenly ascribe clinical authority to GenAI, despite their lack of diagnostic competence, reasoning capability, or professional accountability [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref80\" rel=\"footnote\"\u003E80\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], leading to uncritical acceptance of their suggestions [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In some cases, GenAI models have at times abruptly terminated high-risk conversations (eg, involving suicidal ideation) without explanation, leaving users feeling emotionally abandoned or disoriented [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EGenAI also shows limited reliability in crisis recognition and management. For instance, GPT-3.5 [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref4\" rel=\"footnote\"\u003E4\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] has been found to consistently underestimate suicide risk [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref35\" rel=\"footnote\"\u003E35\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Even in explicit crisis prompts (eg, &#x201C;I want to die&#x201D;), only 56.6% of self-harm prompts were correctly flagged [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref64\" rel=\"footnote\"\u003E64\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Moreover, even when crises were detected, responses were often delayed [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref76\" rel=\"footnote\"\u003E76\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and more than 38% of the generated replies were rated as unhelpful or misleading [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Only a small proportion of GenAI models provided referral resources following risk detection [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref60\" rel=\"footnote\"\u003E60\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref76\" rel=\"footnote\"\u003E76\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ETo address these risks, several mitigation strategies have been proposed. Researchers recommend embedding disclaimers and transparency cues to clarify the system&#x2019;s nonclinical role [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref72\" rel=\"footnote\"\u003E72\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and using empathic prompt templates to encourage user agency and referral to human professionals [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. For high-risk scenarios, hybrid pipelines combining automated detection (eg, keyword scanning and risk scoring) with human oversight have been adopted to improve user safety [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Ch5\u003EEthical Governance\u003C\u002Fh5\u003E\u003Cp class=\"abstract-paragraph\"\u003EEthical governance refers to the establishment of regulatory, procedural, and normative frameworks that ensure these technologies are developed and deployed responsibly. Core governance dimensions include informed consent, transparency, ethics approval, ongoing oversight, and ethical dilemmas and responsibility.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EA recurring concern is the lack of informed consent and operational transparency. Several studies have highlighted that users are often unaware of system limitations, data storage practices, or liability implications [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Both clinicians and patients have also expressed concerns about the &#x201C;black box&#x201D; nature of GenAI, which offers limited interpretability and constrains clinical supervision and shared decision-making [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref98\" rel=\"footnote\"\u003E98\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Long-term governance remains underdeveloped. Ethics approval procedures are not consistently reported across studies, even when the research involves sensitive mental health content. Moreover, most systems lack clinical auditing mechanisms or feedback loops from licensed professionals. For example, a commercial chatbot was found to generate inappropriate content, such as drug use instructions and adult conversations with minors [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref11\" rel=\"footnote\"\u003E11\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Emerging ethical dilemmas further complicate implementation. For example, some platforms restrict outputs on sensitive topics to comply with platform policies, but such censorship may interfere with clinically relevant conversations [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref51\" rel=\"footnote\"\u003E51\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. In other cases, systems blur the boundary between psychological support and formal treatment, raising unresolved questions about responsibility when harm occurs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Current frameworks also provide little clarity on liability attribution&#x2014;whether it should rest with developers, platform operators, clinicians, or end users [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref76\" rel=\"footnote\"\u003E76\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EIn response, several governance strategies have been proposed. These include explicit informed consent procedures that inform users about system capabilities, data use, and the right to opt out at any time [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref73\" rel=\"footnote\"\u003E73\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], as well as prompt-based transparency cues to support clinician evaluation of GenAI outputs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Technical methods&#x2014;such as knowledge-enhanced pretraining [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref29\" rel=\"footnote\"\u003E29\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] and symbolic reasoning graphs [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref43\" rel=\"footnote\"\u003E43\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]&#x2014;have been explored to improve model explainability. To strengthen ethical oversight, researchers have advocated for feedback-integrated learning pipelines involving clinician input, institutional ethics review protocols [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref3\" rel=\"footnote\"\u003E3\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], independent auditing bodies [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref37\" rel=\"footnote\"\u003E37\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], postdeployment safety evaluations [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref97\" rel=\"footnote\"\u003E97\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], and public registries for mental health&#x2013;related GenAI models [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref7\" rel=\"footnote\"\u003E7\u003C\u002Fa\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cbr\u003E\u003Ch3 class=\"navigation-heading h3-main-heading\" id=\"Discussion\" data-label=\"Discussion\"\u003EDiscussion\u003C\u002Fh3\u003E\u003Ch4\u003EPrincipal Findings\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EWe systematically reviewed the applications of GenAI in mental health, focusing on 3 main areas: diagnosis and assessment, therapeutic tools, and clinician support. The findings reveal the potential of GenAI across these domains, while also highlighting technical, ethical, and implementation-related challenges.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EFirst, in mental health diagnosis and assessment, GenAI has been widely used to detect and interpret mental health conditions. These models analyze textual and multimodal data to identify mental health issues, such as depression and stress, providing a novel pathway for early identification and intervention. Despite promising applications, the current body of research largely focuses on suicide risk and depression, with relatively few studies addressing other critical conditions. The lack of comprehensive coverage of these conditions limits our understanding of how GenAI might perform across a broader range of psychiatric conditions, each with unique clinical and social implications. Future research should prioritize expanding the scope to encompass less frequently addressed mental health conditions, enabling a more thorough evaluation of GenAI models&#x2019; utility and effectiveness across diverse mental health assessments. Moreover, a substantial portion of GenAI-based diagnostic research relies on social media datasets. While such data sources are abundant and often rich in user-expressed emotion, they frequently skew toward specific demographics&#x2014;such as younger, digitally active, and predominantly English-speaking users [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref137\" rel=\"footnote\"\u003E137\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]&#x2014;which may limit the cultural and linguistic diversity of the models&#x2019; training inputs. These limitations can affect model generalizability and raise concerns about bias when applied across different populations. As an alternative, integrating more diverse and ecologically valid data&#x2014;such as real-world data from electronic health records or community-based mental health services&#x2014;could better capture population-level heterogeneity. At the same time, although integrating multimodal signals&#x2014;such as vocal tone, facial expression, and behavioral patterns&#x2014;offers potential to improve the accuracy and richness of mental health assessments, such data are significantly more challenging to collect due to technical, ethical, and privacy-related constraints. Thus, there is an inherent tradeoff between the richness of data and the feasibility of acquisition. Future work should weigh these tradeoffs and may benefit from hybrid approaches that combine modest multimodal inputs with improved text-based modeling.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003ESecond, as a therapeutic tool, GenAI has been applied to develop chatbots and conversational agents to provide emotional support, behavioral interventions, and crisis management. GPT-powered chatbots, for example, can engage users in managing anxiety, stress, and other emotional challenges, enhancing accessibility and personalization in mental health services [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref70\" rel=\"footnote\"\u003E70\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. By offering accessible and anonymous mental health support, these GenAI models help bridge gaps in traditional mental health services, especially in areas with limited resources or high social stigma, thus supporting personalized mental health management and extending access to those who might otherwise avoid seeking help. However, the efficacy of these tools in managing complex emotions and crisis situations requires further validation, as many studies are constrained by small sample sizes or rely on simulated scenarios and engineering-focused approaches without real user testing. In particular, crisis detection capabilities present a complex tradeoff. On the one hand, prompt identification of suicidal ideation or emotional breakdowns is critical to prevent harm; on the other hand, oversensitive detection algorithms risk producing false alarms&#x2014;erroneously flagging users who are not in crisis. Such false positives may have unintended consequences, including creating distress in users, eroding trust in the system, and triggering unnecessary clinical responses that divert limited mental health resources. Conversely, overly conservative models that prioritize precision may fail to identify genuine high-risk users, delaying critical interventions. Current systems rarely incorporate contextual judgment, such as distinguishing between metaphorical expressions (eg, &#x201C;I can&#x2019;t take this anymore&#x201D;) and genuine crisis indicators, and often lack follow-up protocols for ambiguous cases. Therefore, future research must prioritize the development of calibrated, context-aware risk detection models, possibly through human-in-the-loop frameworks or personalized risk thresholds that adapt to users&#x2019; communication styles and mental health histories. Another possibility worth considering is that deployment decisions could be adapted to the specific context in which the GenAI-based system is used, with varying levels of risk tolerance and crisis response infrastructure. For instance, in nonclinical or low-resource environments, it may be more appropriate to implement conservative triage mechanisms that flag only high-confidence crisis indicators. In contrast, systems embedded within clinical workflows might afford to adopt more sensitive detection strategies, given the presence of professionals who can interpret and manage potential alerts. Exploring such context-sensitive deployment strategies may help balance the tradeoff between oversensitivity and underdetection and better align GenAI-based interventions with the practical and ethical demands of mental health care delivery. In addition, most studies evaluate only the immediate or short-term effects of AI interventions, with limited assessment of long-term outcomes and sustainability. Future research needs to investigate the prolonged impact of GenAI interventions on mental health and assess the long-term durability of their therapeutic benefits.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EThird, GenAI is used to support clinicians and mental health professionals by assisting with tasks such as treatment planning, summarizing user data, and providing psychoeducation. These applications reduce professional workload and improve efficiency. However, studies [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref86\" rel=\"footnote\"\u003E86\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref97\" rel=\"footnote\"\u003E97\u003C\u002Fa\u003E\u003C\u002Fspan\u003E] indicate that GenAI models may occasionally produce incorrect or even harmful advice in complex cases, posing a risk of misinforming users. Enhancing the accuracy and reliability of GenAI models, especially in complex clinical contexts, should be a priority for future research to ensure that diagnostic and treatment recommendations are safe and trustworthy. Moreover, effective integration of GenAI into clinical workflows to increase acceptance and willingness to adopt these tools among health care professionals remains an area for further investigation [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref51\" rel=\"footnote\"\u003E51\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref89\" rel=\"footnote\"\u003E89\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Future research could explore human-computer interaction design and user experience to ensure GenAI models are user-friendly and beneficial in clinical practice.\u003C\u002Fp\u003E\u003Ch4\u003EAddressing Ethical Governance, Fairness, and Reporting Challenges\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EIn addition to application-specific findings, this review identified systemic challenges in how studies are designed, reported, and governed&#x2014;particularly concerning ethics, fairness, and methodological transparency.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EEthical governance remains underdeveloped across much of the literature. Despite the sensitive nature of mental health contexts, few studies clearly document procedures for informed consent, data use transparency, or postdeployment oversight. Many GenAI systems reviewed lacked mechanisms for user feedback, ethics review, or human-in-the-loop safeguards, raising concerns about accountability and clinical appropriateness. Moreover, the &#x201C;black box&#x201D; design of most models limits interpretability, complicating clinician supervision and user trust. Future research should prioritize the development of explainable, auditable, and ethically reviewed systems. This includes the integration of clear disclaimers, transparent model capabilities, participatory design involving mental health professionals, and external auditing processes. Broader structural reforms&#x2014;such as public registries for mental health&#x2013;related GenAI models and standardized ethics review frameworks&#x2014;are needed to ensure responsible deployment and user protection.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EFairness emerged as a particularly pressing and unresolved concern in GenAI-based mental health applications. Studies consistently report demographic disparities in model performance, with specific populations more susceptible to underdiagnosis or misclassification [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref38\" rel=\"footnote\"\u003E38\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref56\" rel=\"footnote\"\u003E56\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref82\" rel=\"footnote\"\u003E82\u003C\u002Fa\u003E\u003C\u002Fspan\u003E]. Although mitigation techniques such as value-aligned data augmentation, demographic diversification, or model fine-tuning have been explored [\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref39\" rel=\"footnote\"\u003E39\u003C\u002Fa\u003E\u003C\u002Fspan\u003E,\u003Cspan class=\"footers\"\u003E\u003Ca class=\"citation-link\" href=\"#ref79\" rel=\"footnote\"\u003E79\u003C\u002Fa\u003E\u003C\u002Fspan\u003E], their effectiveness remains limited and context-dependent. Many of these methods remain limited in scope, difficult to generalize, or lack systematic validation across diverse user groups. Moreover, the complexity of bias in mental health is compounded by overlapping factors such as language, culture, and social stigma&#x2014;dimensions that current fairness metrics often fail to capture. Achieving fairness in GenAI systems thus requires more than post hoc adjustments to model outputs. It demands a more proactive and systemic rethinking of how datasets are constructed, which populations are represented, and whose needs are prioritized. Future research should consider moving beyond model-level optimization to include participatory design, culturally grounded evaluation protocols, and governance structures that center equity and inclusivity.\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EReporting quality also remains inconsistent. While many studies provide detailed descriptions of model development and performance outcomes, far fewer report on ethical safeguards, deployment readiness, or data-sharing protocols. To improve reproducibility and accountability, future work should adopt standardized reporting frameworks that cover both technical performance and practical deployment, and prioritize ethical accountability, practical applicability, and open science principles.\u003C\u002Fp\u003E\u003Ch4\u003ELimitations and Future Research\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EThis review has several limitations. First, the heterogeneity of study designs, datasets, and evaluation metrics limited our ability to conduct quantitative synthesis or meta-analysis. Second, most included studies (70\u002F79, 89%) focused on proof-of-concept scenarios or simulated interactions, with a few (9\u002F79, 11%) reporting on real-world deployment or longitudinal outcomes. These constraints reduce the generalizability of the existing evidence. Third, although we used a broad search strategy targeting GenAI in general, all included studies ultimately centered on text-based language models. This reflects the current landscape of research but also limits insight into emerging modalities such as vision-language or multimodal generative systems. Finally, despite comprehensive database searches, some relevant gray literature or non-English studies may have been excluded. Future research should broaden the empirical scope to include diverse generative modalities beyond text-only architectures, ensure consistent evaluation frameworks across tasks and populations, and prioritize inclusivity and long-term impact to advance the responsible integration of GenAI in mental health care.\u003C\u002Fp\u003E\u003Ch4\u003EConclusions\u003C\u002Fh4\u003E\u003Cp class=\"abstract-paragraph\"\u003EThis systematic review summarizes the applications of GenAI in mental health, focusing on areas including diagnosis and assessment, therapeutic tools, and clinician support. Findings indicate that GenAI can serve as a complementary tool to bridge gaps in traditional mental health services, especially in regions with limited resources or high social stigma. However, ethical challenges&#x2014;including privacy, potential biases, user safety, and the need for stringent ethical governance&#x2014;are critical to address. To support responsible use, we proposed the GenAI4MH ethical framework, which emphasizes guidelines for data privacy, fairness, transparency, and safe integration of GenAI into clinical workflows. Future research should expand the applications of GenAI across diverse cultural and demographic contexts, further investigate the integration of multimodal data, and rigorously evaluate long-term impacts to ensure GenAI&#x2019;s sustainable, ethical, and effective role in mental health.\u003C\u002Fp\u003E\u003C\u002Farticle\u003E\u003Cp\u003E\u003Ch4 class=\"h4-border-top\"\u003EAcknowledgments\u003C\u002Fh4\u003E\u003C\u002Fp\u003E\u003Cp class=\"abstract-paragraph\"\u003EThis work is supported by the National Social Science Fund of China (grant 21BSH158) and the National Natural Science Foundation of China (grant 32271136).\u003C\u002Fp\u003E\u003Ch3\u003EData Availability\u003C\u002Fh3\u003E\u003Cp class=\"abstract-paragraph\"\u003EData sharing is not applicable to this article as no datasets were generated or analyzed during this study.\u003C\u002Fp\u003E\u003Ch4 class=\"h4-border-top\"\u003EAuthors' Contributions\u003C\u002Fh4\u003E\u003Cp\u003E\u003Cp class=\"abstract-paragraph\"\u003EXW was responsible for data curation, formal analysis, investigation, methodology, and writing the original draft of the manuscript. YZ was responsible for conceptualization, investigation, methodology, project administration, visualization, and reviewing and editing of the manuscript. GZ was responsible for funding acquisition, resources, supervision, validation, and reviewing and editing of the manuscript.\u003C\u002Fp\u003E\u003C\u002Fp\u003E\u003Ch4 class=\"h4-border-top\"\u003EConflicts of Interest\u003C\u002Fh4\u003E\u003Cp\u003E\u003Cp class=\"abstract-paragraph\"\u003ENone declared.\u003C\u002Fp\u003E\u003C\u002Fp\u003E\u003Cdiv id=\"app1\" name=\"app1\"\u003EMultimedia Appendix 1\u003Cp class=\"abstract-paragraph\"\u003EPRISMA Checklist.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fjmir.org\u002Fapi\u002Fdownload?alt_name=mental_v12i1e70610_app1.pdf&amp;filename=179103d224277d642a5fb054b15693ca.pdf\" target=\"_blank\"\u003EPDF File  (Adobe PDF File), 659 KB\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Chr\u003E\u003Cdiv id=\"app2\" name=\"app2\"\u003EMultimedia Appendix 2\u003Cp class=\"abstract-paragraph\"\u003EDetailed search strategy for study identification.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fjmir.org\u002Fapi\u002Fdownload?alt_name=mental_v12i1e70610_app2.pdf&amp;filename=6a0153ce2162d75b40f48dbc41db656c.pdf\" target=\"_blank\"\u003EPDF File  (Adobe PDF File), 70 KB\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Chr\u003E\u003Cdiv id=\"app3\" name=\"app3\"\u003EMultimedia Appendix 3\u003Cp class=\"abstract-paragraph\"\u003ESummary of the identified studies.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fjmir.org\u002Fapi\u002Fdownload?alt_name=mental_v12i1e70610_app3.pdf&amp;filename=ed5855767a97fe0e65f0519fd35f4eff.pdf\" target=\"_blank\"\u003EPDF File  (Adobe PDF File), 417 KB\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Chr\u003E\u003Cdiv id=\"app4\" name=\"app4\"\u003EMultimedia Appendix 4\u003Cp class=\"abstract-paragraph\"\u003EMI-CLAIM-GEN Checklist for generative AI clinical studies.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fjmir.org\u002Fapi\u002Fdownload?alt_name=mental_v12i1e70610_app4.pdf&amp;filename=fe4d1829f06ae53d943ca31dd2b1b5da.pdf\" target=\"_blank\"\u003EPDF File  (Adobe PDF File), 51 KB\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Chr\u003E\u003Cdiv id=\"app5\" name=\"app5\"\u003EMultimedia Appendix 5\u003Cp class=\"abstract-paragraph\"\u003EList of the included studies on the use of generative artificial intelligence for mental health diagnosis and assessment.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fjmir.org\u002Fapi\u002Fdownload?alt_name=mental_v12i1e70610_app5.pdf&amp;filename=ec51afbfa23cca97499eaa91c123ef07.pdf\" target=\"_blank\"\u003EPDF File  (Adobe PDF File), 170 KB\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Chr\u003E\u003Cdiv id=\"app6\" name=\"app6\"\u003EMultimedia Appendix 6\u003Cp class=\"abstract-paragraph\"\u003EDatasets used in generative artificial intelligence&#x2013;based mental health diagnosis and assessment.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fjmir.org\u002Fapi\u002Fdownload?alt_name=mental_v12i1e70610_app6.pdf&amp;filename=2f264fc672bb8df6398e172c68f2a636.pdf\" target=\"_blank\"\u003EPDF File  (Adobe PDF File), 128 KB\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E\u003Chr\u003E\u003Cdiv class=\"footnotes\"\u003E\u003Ch4 id=\"References\" class=\"h4-border-top navigation-heading\" data-label=\"References\"\u003EReferences\u003C\u002Fh4\u003E\u003Col\u003E\u003Cli\u003E\u003Cspan id=\"ref1\"\u003EMental disorders. World Health Organization.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fwww.who.int\u002Fnews-room\u002Ffact-sheets\u002Fdetail\u002Fmental-disorders\"\u003Ehttps:\u002F\u002Fwww.who.int\u002Fnews-room\u002Ffact-sheets\u002Fdetail\u002Fmental-disorders\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref2\"\u003EDanner M, Hadzic B, Gerhardt S, Ludwig S, Uslu I, Shao P. Advancing mental health diagnostics: GPT-based method for depression detection. \n                        In: Proceedings of the 62nd Annual Conference of the Society of Instrument and Control Engineers. 2023. Presented at: SICE '23; September 6-9, 2023:1290-1296; Tsu, Japan. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10354236\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10354236\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref3\"\u003ED'Souza RF, Amanullah S, Mathew M, Surapaneni KM. Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes. Asian J Psychiatr.  Nov 2023;89:103770. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.ajp.2023.103770\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37812998&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref4\"\u003EChatGPT. OpenAI.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fchat.openai.com\"\u003Ehttps:\u002F\u002Fchat.openai.com\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref5\"\u003ESorin V, Brin D, Barash Y, Konen E, Charney A, Nadkarni G,  et al. Large language models and empathy: systematic review. J Med Internet Res.  Dec 11, 2024;26:e52597.  [\u003Ca href=\"https:\u002F\u002Fwww.jmir.org\u002F2024\u002F\u002Fe52597\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F52597\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39661968&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref6\"\u003ELee YK, Suh J, Zhan H, Li JJ, Ong DC. Large language models produce responses perceived to be empathic. arXiv.  Preprint posted online on March 26, 2024.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2403.18148\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Facii63134.2024.00012\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref7\"\u003EYu H, McGuinness S. An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system. J Med Artif Intell.   2024;7:1-16.  [\u003Ca href=\"https:\u002F\u002Fjmai.amegroups.org\u002Farticle\u002Fview\u002F8991\u002Fhtml\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref8\"\u003ENajarro LA, Lee Y, Toshnazarov KE, Jang Y, Kim H, Noh Y. WMGPT: towards 24\u002F7 online prime counseling with ChatGPT. \n                        In: Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing. 2023. Presented at: UbiComp\u002FISWC '23; October 8-12, 2023:142-145; Cancun, Mexico. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3594739.3610708\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3594739.3610708\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3594739.3610708\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref9\"\u003EWu Y, Mao K, Zhang Y, Chen J. CALLM: enhancing clinical interview analysis through data augmentation with large language models. IEEE J Biomed Health Inform.  Dec 2024;28(12):7531-7542. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002FJBHI.2024.3435085\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39074002&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref10\"\u003EReplika: the AI companion who cares. Luka Inc.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Freplika.com\"\u003Ehttps:\u002F\u002Freplika.com\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref11\"\u003EMa Z, Mei Y, Su Z. Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. AMIA Annu Symp Proc.   2023;2023:1105-1114.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F38222348\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38222348&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref12\"\u003EBauer B, Norel R, Leow A, Rached ZA, Wen B, Cecchi G. Using large language models to understand suicidality in a social media-based taxonomy of mental health disorders: linguistic analysis of reddit posts. JMIR Ment Health.  May 16, 2024;11:e57234.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2024\u002F\u002Fe57234\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F57234\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38771256&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref13\"\u003EOmar M, Levkovich I. Exploring the efficacy and potential of large language models for depression: a systematic review. J Affect Disord.  Feb 15, 2025;371:234-244. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.jad.2024.11.052\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39581383&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref14\"\u003ECasu M, Triscari S, Battiato S, Guarnera L, Caponnetto P. AI chatbots for mental health: a scoping review of effectiveness, feasibility, and applications. Appl Sci.  Jul 05, 2024;14(13):5889. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3390\u002Fapp14135889\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref15\"\u003ELee QY, Chen M, Ong CW, Ho CS. The role of generative artificial intelligence in psychiatric education- a scoping review. BMC Med Educ.  Mar 25, 2025;25(1):438.  [\u003Ca href=\"https:\u002F\u002Fbmcmededuc.biomedcentral.com\u002Farticles\u002F10.1186\u002Fs12909-025-07026-9\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1186\u002Fs12909-025-07026-9\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=40133891&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref16\"\u003ELuo X, Zhang A, Li Y, Zhang Z, Ying F, Lin R,  et al. Emergence of Artificial Intelligence Art Therapies (AIATs) in mental health care: a systematic review. Int J Ment Health Nurs.  Dec 17, 2024;33(6):1743-1760. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1111\u002Finm.13384\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39020473&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref17\"\u003EXian X, Chang A, Xiang YT, Liu MT. Debate and dilemmas regarding generative AI in mental health care: scoping review. Interact J Med Res.  Aug 12, 2024;13:e53672.  [\u003Ca href=\"https:\u002F\u002Fwww.i-jmr.org\u002F2024\u002F\u002Fe53672\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F53672\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39133916&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref18\"\u003EGuo Z, Lai A, Thygesen JH, Farrington J, Keen T, Li K. Large language models for mental health applications: systematic review. JMIR Ment Health.  Oct 18, 2024;11:e57400.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2024\u002F\u002Fe57400\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F57400\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39423368&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref19\"\u003EGPT-4o system card. OpenAI.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fopenai.com\u002Findex\u002Fgpt-4o-system-card\u002F\"\u003Ehttps:\u002F\u002Fopenai.com\u002Findex\u002Fgpt-4o-system-card\u002F\u003C\u002Fa\u003E [accessed 2025-04-09]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref20\"\u003EOpenAI o1 system card. OpenAI.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fopenai.com\u002Findex\u002Fopenai-o1-system-card\u002F\"\u003Ehttps:\u002F\u002Fopenai.com\u002Findex\u002Fopenai-o1-system-card\u002F\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref21\"\u003ECorrado G, Barral J. Advancing medical AI with Med-Gemini. Google Research.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fresearch.google\u002Fblog\u002Fadvancing-medical-ai-with-med-gemini\u002F\"\u003Ehttps:\u002F\u002Fresearch.google\u002Fblog\u002Fadvancing-medical-ai-with-med-gemini\u002F\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref22\"\u003EDe Freitas J, Cohen IG. The health risks of generative AI-based wellness apps. Nat Med.  May 29, 2024;30(5):1269-1275. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1038\u002Fs41591-024-02943-6\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38684859&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref23\"\u003EPage MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow C,  et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ.  Mar 29, 2021;372:n71.  [\u003Ca href=\"https:\u002F\u002Fwww.bmj.com\u002Flookup\u002Fpmidlookup?view=long&amp;pmid=33782057\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1136\u002Fbmj.n71\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=33782057&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref24\"\u003ERaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M,  et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res.   2020;21(140):1-67.  [\u003Ca href=\"https:\u002F\u002Fjmlr.org\u002Fpapers\u002Fvolume21\u002F20-074\u002F20-074.pdf\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref25\"\u003EDevlin J, Chang MW, Lee K, Toutanova K. Pre-training of deep bidirectional transformers for language understanding. arXiv.  Preprint posted online on October 11, 2018.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1810.04805\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref26\"\u003ERadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog.   2019.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fcdn.openai.com\u002Fbetter-language-models\u002Flanguage_models_are_unsupervised_multitask_learners.pdf\"\u003Ehttps:\u002F&#x200B;\u002Fcdn.&#x200B;openai.com\u002F&#x200B;better-language-models\u002F&#x200B;language_models_are_unsupervised_multitask_learners.&#x200B;pdf\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref27\"\u003EPatel KK, Pal A, Saurav K, Jain P. Mental health detection using transformer BERT. In: Iyer SS, Jain A, Wang J, editors. Handbook of Research on Lifestyle Sustainability and Management Solutions Using AI, Big Data Analytics, and Visualization. New York, NY. IGI Global;  2022:91-108.\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref28\"\u003EGreco CM, Simeri A, Tagarelli A, Zumpano E. Transformer-based language models for mental health issues: a survey. Pattern Recogn Lett.  Mar 2023;167:204-211. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.patrec.2023.02.016\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref29\"\u003EAlhamed F, Ive J, Specia L. Using large language models (LLMs) to extract evidence from pre-annotated social media data. \n                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21-22, 2024:232-237; St. Julian's, Malta. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.22.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.22.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref30\"\u003EChen J, Nguyen V, Dai X, Molla D, Paris C, Karimi S. Exploring instructive prompts for large language models in the extraction of evidence for supporting assigned suicidal risk levels. \n                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:197-202; St. Julians, Malta. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.17.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.17.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref31\"\u003ESingh LG, Mao J, Mutalik R, Middleton SE. Extracting and summarizing evidence of suicidal ideation in social media contents using large language models. \n                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:218-226; St. Julians, Malta.\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref32\"\u003EStern W, Goh SJ, Nur N, Aragon PJ, Mercer T, Bhattacharyya S. Natural language explanations for suicide risk classification using large language models. \n                        In: Proceedings of the 2024 Machine Learning for Cognitive and Mental Health Workshop. 2024. Presented at: ML4CMH '24; February 26, 2024:1-10; Vancouver, BC. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fceur-ws.org\u002FVol-3649\u002FPaper5.pdf\"\u003Ehttps:\u002F\u002Fceur-ws.org\u002FVol-3649\u002FPaper5.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref33\"\u003EUluslu AY, Michail A, Clematide S. Utilizing large language models to identify evidence of suicidality risk through analysis of emotionally charged posts. \n                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:264-269; St. Julians, MT. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.26.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.26.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref34\"\u003EElyoseph Z, Levkovich I. Beyond human expertise: the promise and limitations of ChatGPT in suicide risk assessment. Front Psychiatry.   2023;14:1213141.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F37593450\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3389\u002Ffpsyt.2023.1213141\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37593450&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref35\"\u003ELee C, Mohebbi M, O'Callaghan E, Winsberg M. Large language models versus expert clinicians in crisis prediction among telemental health patients: comparative study. JMIR Ment Health.  Aug 02, 2024;11:e58129.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2024\u002F\u002Fe58129\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F58129\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38876484&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref36\"\u003ELevkovich I, Elyoseph Z. Suicide risk assessments through the eyes of ChatGPT-3.5 versus ChatGPT-4: vignette study. JMIR Ment Health.  Sep 20, 2023;10:e51232.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2023\u002F\u002Fe51232\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F51232\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37728984&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref37\"\u003EShinan-Altman S, Elyoseph Z, Levkovich I. The impact of history of depression and access to weapons on suicide risk assessment: a comparison of ChatGPT-3.5 and ChatGPT-4. PeerJ.   2024;12:e17468.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F38827287\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.7717\u002Fpeerj.17468\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38827287&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref38\"\u003ESoun RS, Nair A. ChatGPT for mental health applications: a study on biases. \n                        In: Proceedings of the 3rd International Conference on AI-ML Systems. 2023. Presented at: AIMLSystems '23; October 25-28, 2023:1-5; Bangalore, India. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3639856.3639894\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3639856.3639894\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref39\"\u003EXu X, Yao B, Dong Y, Gabriel S, Yu H, Hendler J,  et al. Mental-LLM: leveraging large language models for mental health prediction via online text data. Proc ACM Interact Mob Wearable Ubiquitous Technol.  Mar 06, 2024;8(1):1-32. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3643540\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39925940&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref40\"\u003EZhang T, Yang K, Ji S, Liu B, Xie Q, Ananiadou S. SuicidEmoji: derived emoji dataset and tasks for suicide-related social content. \n                        In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2024. Presented at: SIGIR '24; July 14-18, 2024:1136-1141; Washington, DC. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3626772.3657852\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3626772.3657852\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3626772.3657852\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref41\"\u003EZhou W, Prater LC, Goldstein EV, Mooney SJ. Identifying rare circumstances preceding female firearm suicides: validating a large language model approach. JMIR Ment Health.  Oct 17, 2023;10:e49359.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2023\u002F\u002Fe49359\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F49359\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37847549&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref42\"\u003EShin D, Kim H, Lee S, Cho Y, Jung W. Using large language models to detect depression from user-generated diary text data as a novel approach in digital mental health screening: instrument validation study. J Med Internet Res.  Sep 18, 2024;26:e54617.  [\u003Ca href=\"https:\u002F\u002Fwww.jmir.org\u002F2024\u002F\u002Fe54617\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F54617\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39292502&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref43\"\u003EMazumdar H, Chakraborty C, Sathvik M, Mukhopadhyay S, Panigrahi PK. GPTFX: a novel GPT-3 based framework for mental health detection and explanations. IEEE J Biomed Health Inform.   2023;3:1-8. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Fjbhi.2023.3328350\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref44\"\u003EHayati MF, Ali MA, Rosli AN. Depression detection on Malay dialects using GPT-3. \n                        In: Proceedings of the 2022 IEEE-EMBS Conference on Biomedical Engineering and Sciences. 2022. Presented at: IECBES '22; December 7-9, 2022:360-364; Kuala Lumpur, Malaysia. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10079554\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10079554\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Fiecbes54088.2022.10079554\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref45\"\u003ETao Y, Yang M, Shen H, Yang Z, Weng Z, Hu B. Classifying anxiety and depression through LLMs virtual interactions: a case study with ChatGPT. \n                        In: Proceedings of the 2023 IEEE International Conference on Bioinformatics and Biomedicine. 2023. Presented at: BIBM '23; December 5-8, 2023:2259-2264; Istanbul, Turkiye. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10385305\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10385305\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Fbibm58861.2023.10385305\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref46\"\u003EHu Y, Zhang S, Dang T, Jia H, Salim F, Hu W,  et al. Exploring large-scale language models to evaluate EEG-based multimodal data for mental health. \n                        In: Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing. 2024. Presented at: UbiComp '24; October 5-9, 2024:412-417; Melbourne, Australia. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3675094.3678494\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3675094.3678494\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref47\"\u003EEnglhardt Z, Ma C, Morris ME, Chang C, Xu X, Qin L,  et al. From classification to clinical insights: towards analyzing and reasoning about mobile and behavioral health data with large language models. Proc ACM Interact Mob Wearable Ubiquitous Technol.  May 15, 2024;8(2):1-25. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3659604\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref48\"\u003ELi W, Zhu Y, Lin X, Li M, Jiang Z, Zeng Z. Zero-shot explainable mental health analysis on social media by incorporating mental scales. \n                        In: Proceedings of the 2024 Companion Conference on ACM Web. 2024. Presented at: WWW '24; May 13-17, 2024:959-962; Singapore, Singapore. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3589335.3651584\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3589335.3651584\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3589335.3651584\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref49\"\u003EZhang T, Teng S, Jia H, D'Alfonso S. Leveraging LLMs to predict affective states via smartphone sensor features. \n                        In: Proceedings of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing. 2024. Presented at: UbiComp '24; October 5-9, 2024:709-716; Melbourne, Australia. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3675094.3678420\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3675094.3678420\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3675094.3678420\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref50\"\u003ENi Y, Chen Y, Ding R, Ni S. Beatrice: a chatbot for collecting psychoecological data and providing QA capabilities. \n                        In: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. 2023. Presented at: PETRA '23; July 5-7, 2023:429-435; Corfu, Greece. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3594806.3596580\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3594806.3596580\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3594806.3596580\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref51\"\u003EKim J, Leonte KG, Chen ML, Torous JB, Linos E, Pinto A,  et al. Large language models outperform mental and medical health care professionals in identifying obsessive-compulsive disorder. NPJ Digit Med.  Jul 19, 2024;7(1):193.  [\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1038\u002Fs41746-024-01181-x\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1038\u002Fs41746-024-01181-x\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39030292&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref52\"\u003EPugh SL, Chandler C, Cohen AS, Diaz-Asper C, Elvev&#xE5;g B, Foltz PW. Assessing dimensions of thought disorder with large language models: the tradeoff of accuracy and consistency. Psychiatry Res.  Nov 2024;341:116119.  [\u003Ca href=\"https:\u002F\u002Flinkinghub.elsevier.com\u002Fretrieve\u002Fpii\u002FS0165-1781(24)00404-9\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.psychres.2024.116119\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39226873&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref53\"\u003ERadwan A, Amarneh M, Alawneh H, Ashqar HI, AlSobeh A. Predictive analytics in mental health leveraging LLM embeddings and machine learning models for social media analysis. Int J Web Serv Res.   2024;21(1):1-22.  [\u003Ca href=\"https:\u002F\u002Fwww.igi-global.com\u002Farticle\u002Fpredictive-analytics-in-mental-health-leveraging-llm-embeddings-and-machine-learning-models-for-social-media-analysis\u002F338222\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref54\"\u003ESaleem M, Kim J. Intent aware data augmentation by leveraging generative AI for stress detection in social media texts. Peer J Comput Sci.   2024;10:e2156. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.7717\u002Fpeerj-cs.2156\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref55\"\u003EGargari OK, Fatehi F, Mohammadi I, Firouzabadi SR, Shafiee A, Habibi G. Diagnostic accuracy of large language models in psychiatry. Asian J Psychiatr.  Oct 2024;100:104168. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.ajp.2024.104168\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39111087&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref56\"\u003EHeinz MV, Bhattacharya S, Trudeau B, Quist R, Song SH, Lee CM,  et al. Testing domain knowledge and risk of bias of a large-scale general artificial intelligence model in mental health. Digit Health.  Apr 17, 2023;9:20552076231170499.  [\u003Ca href=\"https:\u002F\u002Fjournals.sagepub.com\u002Fdoi\u002F10.1177\u002F20552076231170499?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1177\u002F20552076231170499\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37101589&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref57\"\u003EOhse J, Had&#x17E;i&#x107; B, Mohammed P, Peperkorn N, Danner M, Yorita A,  et al. Zero-shot strike: testing the generalisation capabilities of out-of-the-box LLM models for depression detection. Comput Speech Lang.  Nov 2024;88:101663. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.csl.2024.101663\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref58\"\u003EYang K, Zhang T, Kuang Z, Xie Q, Huang J, Ananiadou S. MentaLLaMA: interpretable mental health analysis on social media with large language models. \n                        In: Proceedings of the 2024 ACM Web Conference. 2024. Presented at: WWW '24; May 13-17, 2024:4489-4500; Singapore, Singapore. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3589334.3648137\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3589334.3648137\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref59\"\u003EZhu J, Xu A, Tan M, Yang M. XinHai@CLPsych 2024 shared task: prompting healthcare-oriented LLMs for evidence highlighting in posts with suicide risk. \n                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:238-246; St. Julians, Malta. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.23.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.23.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref60\"\u003ESingh A, Ehtesham A, Mahmud S, Kim JH. Revolutionizing mental health care through LangChain: a journey with a large language model. \n                        In: Proceedings of the 14th Annual Computing and Communication Workshop and Conference. 2024. Presented at: CCWC '24; January 8-10, 2024:73-78; Las Vegas, NV. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10427865\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10427865\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Fccwc60891.2024.10427865\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref61\"\u003EChen Z, Deng J, Zhou J, Wu J, Qian T, Huang M. Depression detection in clinical interviews with LLM-empowered structural element graph. \n                        In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2024. Presented at: NAACL-HLT '24; June 16-21, 2024:8181-8194; Mexico City, Mexico. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2024.naacl-long.452.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2024.naacl-long.452.pdf\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2024.naacl-long.452\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref62\"\u003EHur JK, Heffner J, Feng GW, Joormann J, Rutledge RB. Language sentiment predicts changes in depressive symptoms. Proc Natl Acad Sci U S A.  Sep 24, 2024;121(39):e2321321121.  [\u003Ca href=\"https:\u002F\u002Fwww.pnas.org\u002Fdoi\u002Fabs\u002F10.1073\u002Fpnas.2321321121?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1073\u002Fpnas.2321321121\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39284070&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref63\"\u003EBird JJ, Wright D, Sumich A, Lotfi A. Generative AI in psychological therapy: perspectives on computational linguistics and large language models in written behaviour monitoring. \n                        In: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive EnvironmentsProceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments. 2024. Presented at: PETRA '24; June 26-28, 2024:322-328; Crete, Greece. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3652037.3663893\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3652037.3663893\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3652037.3663893\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref64\"\u003EAlanezi F. Assessing the effectiveness of ChatGPT in delivering mental health support: a qualitative study. J Multidiscip Healthc.   2024;17:461-471.  [\u003Ca href=\"https:\u002F\u002Fwww.tandfonline.com\u002Fdoi\u002Fabs\u002F10.2147\u002FJMDH.S447368?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2147\u002FJMDH.S447368\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38314011&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref65\"\u003EMa Z, Mei Y, Long Y, Su Z, Gajos KZ. Evaluating the experience of LGBTQ+ people using large language model based chatbots for mental health support. \n                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-15; Honolulu, HI. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642482\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642482\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3613904.3642482\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref66\"\u003EVakayil S, Juliet DS, Vakayil S. RAG-based LLM chatbot using Llama-2. \n                        In: Proceedings of the 7th International Conference on Devices, Circuits and Systems. 2024. Presented at: ICDCS '24; April 23-24, 2024:1-5; Coimbatore, India. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10561020\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10561020\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Ficdcs59278.2024.10561020\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref67\"\u003EBerrezueta-Guzman S, Kandil M, Mart&#xED;n-Ruiz ML, Pau de la Cruz I, Krusche S. Future of ADHD care: evaluating the efficacy of ChatGPT in therapy enhancement. Healthcare (Basel).  Mar 19, 2024;12(6):33.  [\u003Ca href=\"https:\u002F\u002Fwww.mdpi.com\u002Fresolver?pii=healthcare12060683\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3390\u002Fhealthcare12060683\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38540647&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref68\"\u003EAlessa A, Al-Khalifa H. Towards designing a ChatGPT conversational companion for elderly people. \n                        In: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. 2023. Presented at: PETRA '23; July 5-7, 2023:667-674; Corfu, Greece. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3594806.3596572\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3594806.3596572\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3594806.3596572\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref69\"\u003EWu R, Yu C, Pan X, Liu Y, Zhang N, Fu Y,  et al. MindShift: leveraging large language models for mental-states-based problematic smartphone use intervention. \n                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-24; Honolulu, HI. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642790\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642790\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3613904.3642790\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref70\"\u003EYahagi M, Hiruta R, Miyauchi C, Tanaka S, Taguchi A, Yaguchi Y. Comparison of conventional anesthesia nurse education and an artificial intelligence chatbot (ChatGPT) intervention on preoperative anxiety: a randomized controlled trial. J Perianesth Nurs.  Oct 2024;39(5):767-771. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.jopan.2023.12.005\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38520470&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref71\"\u003EVowels LM, Francois-Walcott RR, Darwiche J. AI in relationship counselling: evaluating ChatGPT's therapeutic capabilities in providing relationship advice. Comput Human Behav.  Aug 2024;2(2):100078. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.chbah.2024.100078\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref72\"\u003EBrocki L, Dyer GC, G'adka A, Chung NC. Deep learning mental health dialogue system. \n                        In: Proceedings of the 2023 IEEE International Conference on Big Data and Smart Computing. 2023. Presented at: BigComp '23; February 13-16, 2023:395-398; Jeju, Republic of Korea. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10066740\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10066740\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Fbigcomp57234.2023.00097\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref73\"\u003ESharma A, Rushton K, Lin IW, Nguyen T, Althoff T. Facilitating self-guided mental health interventions through human-language model interaction: a case study of cognitive restructuring. \n                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-29; Honolulu, HI. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642761\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642761\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3613904.3642761\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref74\"\u003EDongre P. Physiology-driven empathic large language models (EmLLMs) for mental health support. \n                        In: Proceedings of the 2024 Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI EA '24; May 11-16, 2024:1-5; Honolulu, HI. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613905.3651132\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613905.3651132\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3613905.3651132\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref75\"\u003EKumar H, Wang Y, Shi J, Musabirov I, Farb N, Williams J. Exploring the use of large language models for improving the awareness of mindfulness. \n                        In: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. 2023. Presented at: CHI EA '23; April 23-28, 2023:1-7; Hamburg, Germany. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Ffull\u002F10.1145\u002F3544549.3585614\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002Ffull\u002F10.1145\u002F3544549.3585614\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3544549.3585614\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref76\"\u003EHeston TF. Safety of large language models in addressing depression. Cureus.  Dec 2023;15(12):e50729.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F38111813\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.7759\u002Fcureus.50729\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38111813&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref77\"\u003EHerencia L&#xF3;pez-Menchero A. Analysis of the transformer architecture and application on a large language model for mental health counseling. Polytechnic University of Madrid.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdocta.ucm.es\u002Frest\u002Fapi\u002Fcore\u002Fbitstreams\u002F30effe66-9f5a-404e-9b31-ba3e8d555268\u002Fcontent\"\u003Ehttps:\u002F\u002Fdocta.ucm.es\u002Frest\u002Fapi\u002Fcore\u002Fbitstreams\u002F30effe66-9f5a-404e-9b31-ba3e8d555268\u002Fcontent\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref78\"\u003EBird JJ, Lotfi A. Generative transformer chatbots for mental health support: a study on depression and anxiety. \n                        In: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. 2023. Presented at: PETRA '23; July 5-7, 2023:475-479; Corfu, Greece. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3594806.3596520\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3594806.3596520\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3594806.3596520\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref79\"\u003EDe&#xA0;Freitas J, U&#x11F;uralp AK, O&#x11F;uz&#x2010;U&#x11F;uralp Z, Puntoni S. Chatbots and mental health: insights into the safety of generative AI. J Consum Psychol.  Dec 19, 2023;34(3):481-491. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1002\u002Fjcpy.1393\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref80\"\u003EDergaa I, Fekih-Romdhane F, Hallit S, Loch AA, Glenn JM, Fessi MS,  et al. ChatGPT is not ready yet for use in providing mental health assessment and interventions. Front Psychiatry.  Jan 4, 2023;14:1277756.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F38239905\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3389\u002Ffpsyt.2023.1277756\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38239905&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref81\"\u003ELevkovich I, Elyoseph Z. Identifying depression and its determinants upon initiating treatment: ChatGPT versus primary care physicians. Fam Med Community Health.  Sep 2023;11(4):e357.  [\u003Ca href=\"https:\u002F\u002Ffmch.bmj.com\u002Flookup\u002Fpmidlookup?view=long&amp;pmid=37844967\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1136\u002Ffmch-2023-002391\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37844967&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref82\"\u003EPerlis RH, Goldberg JF, Ostacher MJ, Schneck CD. Clinical decision support for bipolar depression using large language models. Neuropsychopharmacology.  Aug 13, 2024;49(9):1412-1416. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1038\u002Fs41386-024-01841-2\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38480911&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref83\"\u003EHwang G, Lee DY, Seol S, Jung J, Choi Y, Her ES,  et al. Assessing the potential of ChatGPT for psychodynamic formulations in psychiatry: an exploratory study. Psychiatry Res.  Jan 2024;331:115655. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.psychres.2023.115655\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38056130&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref84\"\u003EHsieh LH, Liao WC, Liu EY. Feasibility assessment of using ChatGPT for training case conceptualization skills in psychological counseling. Comput Human Behav.  Aug 2024;2(2):100083. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.chbah.2024.100083\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref85\"\u003EHadar-Shoval D, Elyoseph Z, Lvovsky M. The plasticity of ChatGPT's mentalizing abilities: personalization for personality structures. Front Psychiatry.   2023;14:1234397.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F37720897\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3389\u002Ffpsyt.2023.1234397\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37720897&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref86\"\u003EElyoseph Z, Levkovich I. Comparing the perspectives of generative AI, mental health experts, and the general public on schizophrenia recovery: case vignette study. JMIR Ment Health.  Mar 18, 2024;11:e53043.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2024\u002F\u002Fe53043\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F53043\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38533615&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref87\"\u003EElyoseph Z, Levkovich I, Shinan-Altman S. Assessing prognosis in depression: comparing perspectives of AI models, mental health professionals and the general public. Fam Med Community Health.  Jan 09, 2024;12(Suppl 1):33.  [\u003Ca href=\"https:\u002F\u002Ffmch.bmj.com\u002Flookup\u002Fpmidlookup?view=long&amp;pmid=38199604\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1136\u002Ffmch-2023-002583\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38199604&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref88\"\u003EAdhikary PK, Srivastava A, Kumar S, Singh SM, Manuja P, Gopinath JK,  et al. Exploring the efficacy of large language models in summarizing mental health counseling sessions: benchmark study. JMIR Ment Health.  Jul 23, 2024;11:e57306.  [\u003Ca href=\"https:\u002F\u002Fmental.jmir.org\u002F2024\u002F\u002Fe57306\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F57306\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39042893&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref89\"\u003EJames LJ, Genga L, Montagne B, Hagenaars M, Van G. Caregiver's evaluation of LLM-generated treatment goals for patients with severe mental illnesses. \n                        In: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments. 2024. Presented at: PETRA '24; June 26-28, 2024:187-190; Crete, Greece. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3652037.3663955\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3652037.3663955\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3652037.3663955\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref90\"\u003EKim T, Bae S, Kim HA, Lee SW, Hong H, Yang C,  et al. MindfulDiary: harnessing large language model to support psychiatric patients' journaling. \n                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-20; Honolulu, HI. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642937\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642937\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref91\"\u003EFurukawa TA, Iwata S, Horikoshi M, Sakata M, Toyomoto R, Luo Y,  et al. Harnessing AI to optimize thought records and facilitate cognitive restructuring in smartphone CBT: an exploratory study. Cogn Ther Res.  Jul 07, 2023;47(6):887-893. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs10608-023-10411-7\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref92\"\u003EHodson N, Williamson S. Can large language models replace therapists? Evaluating performance at simple cognitive behavioral therapy tasks. JMIR AI.  Jul 30, 2024;3:e52500.  [\u003Ca href=\"https:\u002F\u002Fai.jmir.org\u002F2024\u002F\u002Fe52500\u002F\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.2196\u002F52500\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39078696&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref93\"\u003EMeyer S, Elsweiler D. \"You tell me\": a dataset of GPT-4-based behaviour change support conversations. \n                        In: Proceedings of the 2024 Conference on Human Information Interaction and Retrieval. 2024. Presented at: CHIIR '24; March 10-14, 2024:411-416; Sheffield, UK. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3627508.3638330\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3627508.3638330\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref94\"\u003EMaurya RK, Montesinos S, Bogomaz M, DeDiego AC. Assessing the use of ChatGPT as a psychoeducational tool for mental health practice. Couns Psychother Res.  Apr 25, 2024;25(1):94-100. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1002\u002Fcapr.12759\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref95\"\u003EHedderich MA, Bazarova NN, Zou W, Shim R, Ma X, Yang Q. A piece of theatre: investigating how teachers design LLM chatbots to assist adolescent cyberbullying education. \n                        In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024. Presented at: CHI '24; May 11-16, 2024:1-17; Honolulu, HI. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642379\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3613904.3642379\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3613904.3642379\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref96\"\u003EHu Z, Hou H, Ni S. Grow with your AI buddy: designing an LLMs-based conversational agent for the measurement and cultivation of children's mental resilience. \n                        In: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference. 2024. Presented at: IDC '24; June 17-20, 2024:811-817; Delft, Netherlands. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3628516.3659399\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3628516.3659399\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref97\"\u003EGiorgi S, Isman K, Liu T, Fried Z, Sedoc J, Curtis B. Evaluating generative AI responses to real-world drug-related questions. Psychiatry Res.  Sep 2024;339:116058.  [\u003Ca href=\"https:\u002F\u002Flinkinghub.elsevier.com\u002Fretrieve\u002Fpii\u002FS0165-1781(24)00343-3\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.psychres.2024.116058\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39059040&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref98\"\u003ELiu Y, Ding X, Peng S, Zhang C. Leveraging ChatGPT to optimize depression intervention through explainable deep learning. Front Psychiatry.  Jun 6, 2024;15:1383648.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F38903640\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3389\u002Ffpsyt.2024.1383648\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38903640&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref99\"\u003ESmith A, Hachen S, Schleifer R, Bhugra D, Buadze A, Liebrenz M. Old dog, new tricks? Exploring the potential functionalities of ChatGPT in supporting educational methods in social psychiatry. Int J Soc Psychiatry.  Dec 2023;69(8):1882-1889. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1177\u002F00207640231178451\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=37392000&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref100\"\u003EWang X, Liu K, Wang C. Knowledge-enhanced pre-training large language model for depression diagnosis and treatment. \n                        In: Proceedings of the 9th International Conference on Cloud Computing and Intelligent Systems. 2023. Presented at: CCIS '23; August 12-13, 2023:532-536; Dali, China. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10263217\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F10263217\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref101\"\u003EMiao BY, Chen IY, Williams CY, Davidson J, Garcia-Agundez A, Sun S,  et al. The MI-CLAIM-GEN checklist for generative artificial intelligence in health. Nat Med.  May 06, 2025;31(5):1394-1398.  [\u003Ca href=\"https:\u002F\u002Fescholarship.org\u002Fuc\u002Fitem\u002Fqt1c31t56r\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1038\u002Fs41591-024-03470-0\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=39915678&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref102\"\u003ESantos WM, Secoli SR, P&#xFC;schel VA. The Joanna Briggs Institute approach for systematic reviews. Rev Lat Am Enfermagem.  Nov 14, 2018;26:e3074.  [\u003Ca href=\"https:\u002F\u002Fwww.scielo.br\u002Fscielo.php?script=sci_arttext&amp;pid=S0104-11692018000100701&amp;lng=en&amp;nrm=iso&amp;tlng=en\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1590\u002F1518-8345.2885.3074\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=30462787&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref103\"\u003EZhu W, Huang L, Zhou X, Li X, Shi G, Ying J,  et al. Could AI ethical anxiety, perceived ethical risks and ethical awareness about AI influence university students&#x2019; use of generative AI products? An ethical perspective. Int J Hum Comput Interact.  Mar 08, 2024;41(1):742-764. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1080\u002F10447318.2024.2323277\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref104\"\u003EBrown A. GPT-4 is OpenAI&#x2019;s most advanced system, producing safer and more useful responses. Int J Archit Comput.  Sep 09, 2024;22(3):275-276. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1177\u002F14780771241280148\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref105\"\u003EWright SN, Anticevic A. Generative AI for precision neuroimaging biomarker development in psychiatry. Psychiatry Res.  Sep 2024;339:115955. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.psychres.2024.115955\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=38909415&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref106\"\u003EBarnhill JW. DSM-5 Clinical Cases. Washington, DC. American Psychiatric Publishing;  2013. \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref107\"\u003EGouniai JM, Smith KD, Leonte KG. Do clergy recognize and respond appropriately to the many themes in obsessive-compulsive disorder?: data from a Pacific Island community. Ment Health Relig Cult.  Jan 20, 2022;25(1):33-46. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1080\u002F13674676.2021.2010037\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref108\"\u003ELevi-Belz Y, Gamliel E. The effect of perceived burdensomeness and thwarted belongingness on therapists' assessment of patients' suicide risk. Psychother Res.  Jul 09, 2016;26(4):436-445. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1080\u002F10503307.2015.1013161\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=25751580&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref109\"\u003EGarg M, Saxena C, Krishnan V, Joshi R, Saha S, Mago V. CAMS: an annotated corpus for causal analysis of mental health issues in social media posts. arXiv.  Preprint posted online on July 11, 2022.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2207.04674\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref110\"\u003ECoppersmith G, Dredze M, Harman C, Hollingshead K, Mitchell M. CLPsych 2015 shared task: depression and PTSD on Twitter. \n                        In: Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality. 2015. Presented at: CLPsych '15; June 5, 2015:31-39; Denver, CO. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002FW15-1204.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002FW15-1204.pdf\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.3115\u002Fv1\u002Fw15-1204\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref111\"\u003EChim J, Tsakalidis A, Gkoumas D, Atzil-Slonim D, Ophir Y, Zirikly A,  et al. Overview of the clpsych 2024 shared task: leveraging large language models to identify evidence of suicidality risk in online posts. \n                        In: Proceedings of the 9th Workshop on Computational Linguistics and Clinical Psychology. 2024. Presented at: CLPsych '24; March 21, 2024:177-190; St. Julians, Malta. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.15.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2024.clpsych-1.15.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref112\"\u003EGaur M, Alambo A, Sain JP, Kursuncu U, Thirunarayan K, Kavuluru R,  et al. Knowledge-aware assessment of severity of suicide risk for early intervention. \n                        In: Proceedings of the 2019 International Conference on the World Wide Web. 2019. Presented at: WWW '19; May 13-17, 2019:514-525; San Francisco, CA. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3308558.3313698\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3308558.3313698\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3308558.3313698\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref113\"\u003EPirina I, &#xC7;&#xF6;ltekin C. Identifying depression on reddit: the effect of training data. \n                        In: Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task. 2018. Presented at: EMNLP '18; October 31, 2018:9-12; Brussels, Belgium. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002FW18-5903.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002FW18-5903.pdf\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002Fw18-59\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref114\"\u003ENaseem U, Dunn AG, Kim J, Khushi M. Early identification of depression severity levels on reddit using ordinal classification. \n                        In: Proceedings of the 2022 International Conference on World Wide Web. 2022. Presented at: WWW '22; April 25-29, 2022:2563-2572; Virtual Event. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3485447.3512128\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3485447.3512128\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3485447.3512128\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref115\"\u003ETurcan E, McKeown K. Dreaddit: a reddit dataset for stress analysis in social media. arXiv.  Preprint posted online on October 31, 2019.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1911.00133\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002Fd19-6213\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref116\"\u003EGarg M, Shahbandegan A, Chadha A, Mago V. An annotated dataset for explainable interpersonal risk factors of mental disturbance in social media posts. arXiv.  Preprint posted online on May 30, 2023.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2305.18727\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2023.findings-acl.757\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref117\"\u003ESampath K, Durairaj T. Data set creation and empirical analysis for detecting signs of depression from social media postings. \n                        In: Proceedings of the 5th IFIP TC 12 International Conference on Computational Intelligence in Data Science. 2022. Presented at: ICCIDS '22; March 24-26, 2022:136-151; Virtual Event. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Flink.springer.com\u002Fchapter\u002F10.1007\u002F978-3-031-16364-7_11\"\u003Ehttps:\u002F\u002Flink.springer.com\u002Fchapter\u002F10.1007\u002F978-3-031-16364-7_11\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-031-16364-7_11\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref118\"\u003EHaque A, Reddi V, Giallanza T. Deep learning for suicide and depression identification with unsupervised label correction. \n                        In: Proceedings of the 30th International Conference on Artificial Neural Networks on Artificial Neural Networks and Machine Learning. 2021. Presented at: ICANN '21; September 14-17, 2021:436-447; Bratislava, Slovakia. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Flink.springer.com\u002Fchapter\u002F10.1007\u002F978-3-030-86383-8_35\"\u003Ehttps:\u002F\u002Flink.springer.com\u002Fchapter\u002F10.1007\u002F978-3-030-86383-8_35\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-030-86383-8_35\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref119\"\u003EJi S, Li X, Huang Z, Cambria E. Suicidal ideation and mental disorder detection with attentive relation networks. Neural Comput Appl.  Jun 24, 2021;34(13):10309-10319. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs00521-021-06208-y\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref120\"\u003EGui T, Zhu L, Zhang Q, Peng M, Zhou X, Ding K,  et al. Cooperative multimodal approach to depression detection in Twitter. AAAI Conf Artif Intell.  Jul 17, 2019;33(01):110-117. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v33i01.3301110\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref121\"\u003EShing HC, Nair S, Zirikly A, Friedenberg M, Daum&#xE9; IH, Resnik P. Expert, crowdsourced, and machine assessment of suicide risk via online postings. \n                        In: Proceedings of the 5th Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic. 2018. Presented at: CLPsych '18; June 5, 2018:25-36; New Orleans, LA. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002FW18-0603.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002FW18-0603.pdf\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002Fw18-0603\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref122\"\u003EZirikly A, Resnik P, Uzuner O, Hollingshead K. CLPsych 2019 shared task: predicting the degree of suicide risk in reddit posts. \n                        In: Proceedings of the 6th Workshop on Computational Linguistics and Clinical Psychology. 2019. Presented at: CLPsych '19; June 6, 2019:24-33; Minneapolis, MN. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002FW19-3003.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002FW19-3003.pdf\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002Fw19-3003\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref123\"\u003EWang Y, Wang Z, Li C, Zhang Y, Wang H. A multitask deep learning approach for user depression detection on Sina Weibo. arXiv.  Preprint posted online on August 26, 2020.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2008.11708\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref124\"\u003EElvev&#xE5;g B, Foltz PW, Weinberger DR, Goldberg TE. Quantifying incoherence in speech: an automated methodology and novel application to schizophrenia. Schizophr Res.  Jul 2007;93(1-3):304-316.  [\u003Ca href=\"https:\u002F\u002Feuropepmc.org\u002Fabstract\u002FMED\u002F17433866\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1016\u002Fj.schres.2007.03.001\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=17433866&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref125\"\u003EGratch J, Artstein R, Lucas GM, Stratou G, Scherer S, Nazarian A,  et al. The distress analysis interview corpus of human and computer interviews. \n                        In: Proceedings of the 9th International Conference on Language Resources and Evaluation. 2014. Presented at: LREC '14; May 26-31, 2014:3123-3128; Reykjavik, Iceland. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002FL14-1421\u002F\"\u003Ehttps:\u002F\u002Faclanthology.org\u002FL14-1421\u002F\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref126\"\u003EShen Y, Yang H, Lin L. Automatic depression detection: an emotional audio-textual corpus and a Gru\u002FBilstm-based model. \n                        In: Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing. 2022. Presented at: ICASSP '22; May 23-27, 2022:6247-6251; Singapore, Singapore. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F9746569\"\u003Ehttps:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F9746569\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Ficassp43922.2022.9746569\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref127\"\u003EDeVault D, Artstein R, Benn G, Dey T, Fast E, Gainer A,  et al. SimSensei kiosk: a virtual human interviewer for healthcare decision support. \n                        In: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. 2014. Presented at: AAMAS '14; May 5-9, 2014:1061-1068; Paris, France. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.5555\u002F2615731.2617415\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.5555\u002F2615731.2617415\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref128\"\u003EXu X, Zhang H, Sefidgar Y, Ren Y, Liu X, Seo W,  et al. GLOBEM dataset: multi-year datasets for longitudinal human behavior modeling generalization. arXiv.  preprint posted online on November 4, 2022.  [\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F2211.02733\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref129\"\u003ECimtay Y, Ekmekcioglu E, Caglar-Ozhan S. Cross-subject multimodal emotion recognition based on hybrid fusion. IEEE Access.   2020;8:168865-168878. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Faccess.2020.3023871\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref130\"\u003ECai H, Yuan Z, Gao Y, Sun S, Li N, Tian F,  et al. A multi-modal open dataset for mental-disorder analysis. Sci Data.  Apr 19, 2022;9(1):178.  [\u003Ca href=\"https:\u002F\u002Fdoi.org\u002F10.1038\u002Fs41597-022-01211-x\" target=\"_blank\"\u003EFREE Full text\u003C\u002Fa\u003E] [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1038\u002Fs41597-022-01211-x\"\u003ECrossRef\u003C\u002Fa\u003E] [\u003Ca href=\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fentrez\u002Fquery.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=35440583&amp;dopt=Abstract\" target=\"_blank\"\u003EMedline\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref131\"\u003EChen J, Ro T, Zhu Z. Emotion recognition with audio, video, EEG, and EMG: a dataset and baseline approaches. IEEE Access.   2022;10:13229-13242. [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1109\u002Faccess.2022.3146729\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref132\"\u003EMauriello ML, Lincoln T, Hon G, Simon D, Jurafsky D, Paredes P. SAD: a stress annotated dataset for recognizing everyday stressors in SMS-like conversational systems. \n                        In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 2021. Presented at: CHI EA '21; May 8-13, 2021:1-7; Yokohama, Japan. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3411763.3451799\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3411763.3451799\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3411763.3451799\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref133\"\u003EZeng G, Yang W, Ju Z, Yang Y, Wang S, Zhang R,  et al. MedDialog: large-scale medical dialogue datasets. \n                        In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. 2020. Presented at: EMNLP '20; November 16-20, 2020:9241-9250; Virtual Event. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002F2020.emnlp-main.743.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002F2020.emnlp-main.743.pdf\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2020.emnlp-main.743\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref134\"\u003EJamil Z, Inkpen D, Buddhitha P, White K. Monitoring tweets for depression to detect at-risk users. \n                        In: Proceedings of the 4th Workshop on Computational Linguistics and Clinical Psychology. 2017. Presented at: CLPsych '17; August 3, 2017:32-40; Vancouver, BC. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Faclanthology.org\u002FW17-3104.pdf\"\u003Ehttps:\u002F\u002Faclanthology.org\u002FW17-3104.pdf\u003C\u002Fa\u003E\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref135\"\u003EGemini. Google AI.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fwww.gemini.google.com\"\u003Ehttps:\u002F\u002Fwww.gemini.google.com\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref136\"\u003ELlama 2: open source, free for research and commercial use. Meta AI.  URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fwww.llama.com\u002Fllama2\u002F\"\u003Ehttps:\u002F\u002Fwww.llama.com\u002Fllama2\u002F\u003C\u002Fa\u003E [accessed 2025-05-29]\n                        \u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cspan id=\"ref137\"\u003ECesare N, Grant C, Nsoesie E. Understanding demographic bias and representation in social media health data. \n                        In: Proceedings of the 10th ACM Conference on Web Science. 2019. Presented at: WebSci '19; June 30-July 3, 2019:7-9; Boston, MA. URL: \u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3328413.3328415\"\u003Ehttps:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3328413.3328415\u003C\u002Fa\u003E [\u003Ca target=\"_blank\" href=\"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3328413.3328415\"\u003ECrossRef\u003C\u002Fa\u003E]\u003C\u002Fspan\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003C\u002Fdiv\u003E\u003Cbr\u003E\u003Chr\u003E\u003Ca name=\"Abbreviations\"\u003E&#x200E;\u003C\u002Fa\u003E\u003Ch4 class=\"navigation-heading\" id=\"Abbreviations\" data-label=\"Abbreviations\"\u003EAbbreviations\u003C\u002Fh4\u003E\u003Ctable width=\"80%\" border=\"0\" align=\"center\"\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003EBERT:\u003C\u002Fb\u003E Bidirectional Encoder Representations from Transformers\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003EEEG:\u003C\u002Fb\u003E electroencephalogram\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003EGenAI:\u003C\u002Fb\u003E generative artificial intelligence\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003ELGBTQ:\u003C\u002Fb\u003E lesbian, gay, bisexual, transgender, and queer\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003ELLaMA:\u003C\u002Fb\u003E large language model Meta AI\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003ELLM:\u003C\u002Fb\u003E large language model\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003EMI-CLAIM-GEN:\u003C\u002Fb\u003E Minimum Information about Clinical Artificial Intelligence for Generative Modeling Research\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003EPICOS:\u003C\u002Fb\u003E Population, Intervention, Comparison, Outcome, and Study\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003ERAG:\u003C\u002Fb\u003E retrieval-augmented generation\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003ESPIDER:\u003C\u002Fb\u003E Sample, Phenomenon of Interest, Design, Evaluation, and Research Type\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003Ctr\u003E\u003Ctd\u003E\u003Cb\u003ESVM:\u003C\u002Fb\u003E support vector machines\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\u003C\u002Ftable\u003E\u003Cbr\u003E\u003Chr\u003E\u003Cp style=\"font-style: italic\"\u003EEdited by  C Blease; submitted 27.12.24; peer-reviewed by B Lamichhane,  S Markham,  S Tayebi Arasteh,  G Huang; comments to author 18.02.25; revised version received 14.04.25; accepted 29.05.25; published 27.06.25.\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fsupport.jmir.org\u002Fhc\u002Fen-us\u002Farticles\u002F115002955531\" id=\"Copyright\" target=\"_blank\" class=\"navigation-heading h4 d-block\" aria-label=\"Copyright - what is a Creative Commons License?\" data-label=\"Copyright\"\u003ECopyright \u003Cspan class=\"fas fa-question-circle\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cp class=\"article-copyright\"\u003E&#xA9;Xi Wang, Yujia Zhou, Guangyu Zhou. Originally published in JMIR Mental Health (https:\u002F\u002Fmental.jmir.org), 27.06.2025.\u003C\u002Fp\u003E\u003Csmall class=\"article-license\"\u003E\u003Cp class=\"abstract-paragraph\"\u003EThis is an open-access article distributed under the terms of the Creative Commons Attribution License (https:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Mental Health, is properly cited. The complete bibliographic information, a link to the original publication on https:\u002F\u002Fmental.jmir.org\u002F, as well as this copyright and license information must be included.\u003C\u002Fp\u003E\u003C\u002Fsmall\u003E\u003Cbr\u003E\u003C\u002Fsection\u003E\u003C\u002Farticle\u003E\u003C\u002Fsection\u003E\u003C\u002Fsection\u003E\u003C\u002Fmain\u003E\n"}],fetch:{},error:a,state:{host:a,environment:c,journalPath:j,keys:{},domains:{},screensize:"desktop",currentCookieScriptId:"aa022ba6-b337-11ef-b288-3fb59f57942d",accessibility:{filter:"none","font-weight":"inherit","font-size":.625,"text-align":"initial"},announcements:{data:[{announcement_id:628,title:"Webinar - Enhancing Therapy Engagement Through Digital Tools for Clinicians and Clients",description_short:"\u003Cp\u003EThe Society of Digital Psychiatry, in collaboration with \u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E—an open-access, peer-reviewed journal published by JMIR Publications—invites you to a webinar titled “Enhancing Therapy Engagement Through Digital Tools for Clinicians and Clients.”\u003C\u002Fp\u003E",date_posted:"2025-11-11T16:44:02.000Z",journal_id:f},{announcement_id:626,title:"2025 Society of Digital Psychiatry Symposium: Advancing Digital Mental Health Through AI",description_short:"\u003Cp style=\"text-align: left;\"\u003EWe are excited to announce the annual virtual Society of Digital Psychiatry (SODP) Symposium, taking place on December 12, 2025, at 1 PM ET! Join us for an online event dedicated to \"Advancing Digital Mental Health Through AI.\"\u003C\u002Fp\u003E",date_posted:"2025-11-11T11:59:15.000Z",journal_id:f},{announcement_id:618,title:"Webinar - Co-Designing Digital Mental Health with Youth: From Global Principles to Local Platforms",description_short:"\u003Cp\u003EThe Society of Digital Psychiatry in collaboration with \u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E, an open access, peer-reviewed journal published by JMIR Publications, invites you to a webinar titled ‘Co-Designing Digital Mental Health with Youth: From Global Principles to Local Platforms’\u003C\u002Fp\u003E\u003Cp\u003E\u003Cbr\u003E\u003C\u002Fp\u003E",date_posted:"2025-10-15T08:40:58.000Z",journal_id:f},{announcement_id:610,title:"SODP Symposium: Live Virtually From Australia Advancing Digital Mental Health: AI, Youth Engagement, and Suicide Prevention",description_short:"\u003Cp\u003EThe \u003Ca href=\"https:\u002F\u002Fwww.sodpsych.com\u002F\" target=\"_blank\"\u003ESociety of Digital Psychiatry\u003C\u002Fa\u003E, in collaboration with \u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E, an open access, peer-reviewed journal published by JMIR Publications, invites you to an exclusive symposium-style webinar spotlighting innovative research from leading digital mental health scholars across Australia.\u003C\u002Fp\u003E",date_posted:"2025-08-27T09:56:07.000Z",journal_id:f},{announcement_id:609,title:"Webinar - From Signals to Solutions: Making Digital Phenotyping Clinically Meaningful",description_short:"\u003Cp\u003EThe \u003Ca href=\"https:\u002F\u002Fwww.sodpsych.com\u002F\" target=\"_blank\"\u003ESociety of Digital Psychiatry\u003C\u002Fa\u003E in collaboration with \u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E, an open access, peer-reviewed journal published by JMIR Publications, invites you to a webinar titled ‘\u003Ca href=\"https:\u002F\u002Flandingpage.jmirpublications.com\u002Ffrom-signals-to-solutions-making-digital-phenotyping-clinically-meaningful\" target=\"_blank\"\u003EFrom Signals to Solutions: Making Digital Phenotyping Clinically Meaningful\u003C\u002Fa\u003E’.\u003C\u002Fp\u003E",date_posted:"2025-08-13T09:58:51.000Z",journal_id:f},{announcement_id:604,title:"Webinar - Bringing Digital Mental Health to County Services: Challenges and Opportunities",description_short:"\u003Cp\u003EThe \u003Ca href=\"https:\u002F\u002Fwww.sodpsych.com\u002F\" target=\"_blank\"\u003ESociety of Digital Psychiatry\u003C\u002Fa\u003E in collaboration with \u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E, an open access, peer-reviewed journal published by JMIR Publications, invites you to a webinar titled ‘\u003Ca href=\"https:\u002F\u002Flandingpage.jmirpublications.com\u002Fbringing-digital-mental-health-to-county-services\" target=\"_blank\"\u003EBringing Digital Mental Health to County Services: Challenges and Opportunities\u003C\u002Fa\u003E’\u003C\u002Fp\u003E",date_posted:"2025-07-15T15:41:13.000Z",journal_id:f},{announcement_id:596,title:"JMIR Mental Health Sees a Rise in Journal Impact Factor in 2025",description_short:"\u003Cp\u003E\u003Cstrong\u003E(Toronto, ON, June 19, 2025)&nbsp;\u003C\u002Fstrong\u003EJMIR Publications is pleased to announce that&nbsp;\u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E&nbsp;has received a\u003Cstrong\u003E&nbsp;Journal Impact Factor of 5.8\u003C\u002Fstrong\u003E, as published in Journal Citation Reports 2025 from Clarivate. This represents an increase over last year.&nbsp;\u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E&nbsp;ranks in the first quartile (Q1) within the “Psychiatry” category, continuing to rise in rank year-over-year (ranked #25 of 288, 91st percentile).\u003C\u002Fp\u003E",date_posted:"2025-06-18T17:06:04.000Z",journal_id:f},{announcement_id:576,title:"Webinar - From Apps to AI: Rethinking Clinical Tools in Patient Care",description_short:"\u003Cp\u003EThe Society of Digital Psychiatry in collaboration with JMIR Mental Health, an open access, peer-reviewed journal published by JMIR Publications, invites you to a webinar titled ‘\u003Ca href=\"https:\u002F\u002Flandingpage.jmirpublications.com\u002Fsodp-from-apps-to-ai\" target=\"_blank\"\u003EFrom Apps to AI: Rethinking Clinical Tools in Patient Care\u003C\u002Fa\u003E’\u003C\u002Fp\u003E",date_posted:"2025-06-12T09:19:05.000Z",journal_id:f},{announcement_id:568,title:"Webinar - Technology in Eating Disorders Care: Where We Are and What Lies Ahead",description_short:"\u003Cp\u003EThe Society of Digital Psychiatry in collaboration with \u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E, an open access, peer-reviewed journal published by JMIR Publications, invites you to a webinar titled ‘\u003Ca href=\"https:\u002F\u002Flandingpage.jmirpublications.com\u002Ftechnology-in-eating-disorders-care-where-we-are-and-what-lies-ahead\" target=\"_blank\"\u003ETechnology in Eating Disorders Care: Where We Are and What Lies Ahead\u003C\u002Fa\u003E’\u003C\u002Fp\u003E",date_posted:"2025-05-15T10:27:30.000Z",journal_id:f},{announcement_id:567,title:"Call for Papers: AI-Powered Therapy Bots and Virtual Companions in Digital Mental Health",description_short:"\u003Cp\u003E\u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E invites submissions for a theme issue on \u003Cstrong\u003EAI-Powered Therapy Bots and Virtual Companions\u003C\u002Fstrong\u003E, with a focus on next-generation research that moves beyond proof of concept and addresses the real-world challenges, risks, and opportunities these technologies present in the context of digital psychiatry and mental health.\u003C\u002Fp\u003E",date_posted:"2025-05-08T11:13:51.000Z",journal_id:f}],pagination:{from:b,to:q,total:52,perPage:q,firstPage:b,lastPage:k}},article:{data:{article_id:y,published_at:"2025-06-27T16:30:02.000Z",submitted_at:aa,section_id:ab,journal_id:f,year:2025,issue:ac,volume:l,identifier:"70610",url:ad,pdf_url:"https:\u002F\u002Fmental.jmir.org\u002F2025\u002F1\u002Fe70610\u002FPDF",html_url:"https:\u002F\u002Fmental.jmir.org\u002F2025\u002F1\u002Fe70610",xml_url:"https:\u002F\u002Fmental.jmir.org\u002F2025\u002F1\u002Fe70610\u002FXML",title:"The Application and Ethical Implication of Generative AI in Mental Health: Systematic Review",public_id:"JMIR Ment Health 2025;12:e70610",thumbnail:"https:\u002F\u002Fasset.jmir.pub\u002Fassets\u002Fdae990f3af4bcdffe2910e552301457e.png",doi:"10.2196\u002F70610",pmid:40577783,pmcid:"12254713",issue_title:"Jan-Dec",pages:[],transfer:a,authors:[{first_name:"Xi",last_name:"Wang",full_name:"Xi Wang",degrees:"MA",deceased:a,orcid:"0009-0004-1090-6209",equal_contrib:m,matchedAffiliations:[b]},{first_name:"Yujia",last_name:z,full_name:"Yujia Zhou",degrees:"PhD",deceased:a,orcid:"0000-0002-3530-3787",equal_contrib:m,matchedAffiliations:[d]},{first_name:ae,last_name:z,full_name:af,degrees:ag,deceased:a,orcid:"0000-0003-2053-6737",equal_contrib:m,matchedAffiliations:[b]}],affiliations:[{aff_id:11239008,author_id:472941,phone:a,fax:e,corresp_aff:m,aff_type:a,seq:b,article_id:y,institution_line_1:ah,institution_line_2:ai,institution_line_3:aj,address_line_1:ak,address_line_2:al,city:A,prov_state:a,postal_code:am,country:B},{aff_id:11239009,author_id:472942,phone:a,fax:e,corresp_aff:m,aff_type:a,seq:b,article_id:y,institution_line_1:e,institution_line_2:"Department of Computer Science and Technology",institution_line_3:"Tsinghua University",address_line_1:a,address_line_2:a,city:A,prov_state:a,postal_code:a,country:B}],primaryAuthor:{first_name:ae,last_name:z,full_name:af,email:"gyzhou@pku.edu.cn",degrees:ag,primaryAffiliation:{fax:e,phone:"86 10 62767702",country:B,postal_code:am,prov_state:a,city:A,address_line_1:ak,address_line_2:al,institution_line_1:ah,institution_line_2:ai,institution_line_3:aj}},abstract:"Background: Mental health disorders affect an estimated 1 in 8 individuals globally, yet traditional interventions often face barriers, such as limited accessibility, high costs, and persistent stigma. Recent advancements in generative artificial intelligence (GenAI) have introduced AI systems capable of understanding and producing humanlike language in real time. These developments present new opportunities to enhance mental health care.\nObjective: We aimed to systematically examine the current applications of GenAI in mental health, focusing on 3 core domains: diagnosis and assessment, therapeutic tools, and clinician support. In addition, we identified and synthesized key ethical issues reported in the literature.\nMethods: We conducted a comprehensive literature search, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) 2020 guidelines, in PubMed, ACM Digital Library, Scopus, Embase, PsycInfo, and Google Scholar databases to identify peer-reviewed studies published from October 1, 2019, to September 30, 2024. After screening 783 records, 79 (10.1%) studies met the inclusion criteria.\nResults: The number of studies on GenAI applications in mental health has grown substantially since 2023. Studies on diagnosis and assessment (37\u002F79, 47%) primarily used GenAI models to detect depression and suicidality through text data. Studies on therapeutic applications (20\u002F79, 25%) investigated GenAI-based chatbots and adaptive systems for emotional and behavioral support, reporting promising outcomes but revealing limited real-world deployment and safety assurance. Clinician support studies (24\u002F79, 30%) explored GenAI’s role in clinical decision-making, documentation and summarization, therapy support, training and simulation, and psychoeducation. Ethical concerns were consistently reported across the domains. On the basis of these findings, we proposed an integrative ethical framework, GenAI4MH, comprising 4 core dimensions—data privacy and security, information integrity and fairness, user safety, and ethical governance and oversight—to guide the responsible use of GenAI in mental health contexts.\nConclusions: GenAI shows promise in addressing the escalating global demand for mental health services. They may augment traditional approaches by enhancing diagnostic accuracy, offering more accessible support, and reducing clinicians’ administrative burden. However, to ensure ethical and effective implementation, comprehensive safeguards—particularly around privacy, algorithmic bias, and responsible user engagement—must be established.\n",keywords:"mental health; large language models; generative ai; mental health detection and diagnosis; therapeutic chatbots",date_submitted:aa,title_html:a,sections:[{title:"Reviews in Digital Mental Health",section_id:ab,journal_id:f,colour:h,count:291},{title:"Methods and New Tools in Mental Health Research",section_id:232,journal_id:f,colour:h,count:562},{title:"AI-Powered Therapy Bots and Virtual Companions in Digital Mental Health",section_id:1660,journal_id:f,colour:h,count:r},{title:"Diagnostic Tools in Mental Health",section_id:388,journal_id:f,colour:h,count:328},{title:"Mobile Health in Psychiatry",section_id:253,journal_id:f,colour:h,count:an},{title:"Ethics, Privacy, and Legal Issues",section_id:ao,journal_id:b,colour:n,count:622},{title:"Responsible Health AI",section_id:1649,journal_id:C,colour:s,count:56},{title:"Generative Language Models Including ChatGPT",section_id:1437,journal_id:b,colour:n,count:725},{title:"Digital Mental Health Interventions, e-Mental Health and Cyberpsychology",section_id:64,journal_id:b,colour:n,count:2145},{title:"Artificial Intelligence",section_id:797,journal_id:b,colour:n,count:2162}],preprint:g,articleKD:i,isOldOjphiMigrated:i,isNewsArticle:i,isEocArticle:i,articleType:a}},articles:{recent:[],openReview:[]},articleTypes:{},authentication:{data:a,jwt:a},countries:{data:[]},departments:{data:[]},help:{data:{}},journal:{data:{journal_id:f,title:ap,tag:aq,description:e,path:j,slug:j,seq:t,enabled:b,environment:c,url:ar,batch:b,year:u,colour:h,impact:D,order:v,published:as,transfers:a,cite_score:at,settings:{aboutJournal:"\u003Cp style=\"text-align: justify;\"\u003E\u003Cem\u003EJMIR Mental Health \u003C\u002Fem\u003E(JMH, ISSN&nbsp;2368-7959\u003Cem\u003E,&nbsp;\u003C\u002Fem\u003E\u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F596\" target=\"_blank\"\u003EJournal Impact Factor 5.8,\u003C\u002Fa\u003E&nbsp;Journal Citation Reports 2025 from Clarivate)&nbsp;is a premier, open-access, peer-reviewed journal with a unique focus on digital health and Internet\u002Fmobile interventions, technologies, and electronic innovations (software and hardware) for mental health, addictions, online counseling, and behavior change. The journal publishes research on system descriptions, theoretical frameworks, review papers, viewpoint\u002Fvision papers, and rigorous evaluations that advance evidence-based care, improve accessibility, and enhance the effectiveness of digital mental health solutions. It also explores innovations in digital psychiatry, e-mental health, and clinical informatics in psychiatry and psychology, with an emphasis on improving patient outcomes and expanding access to care.\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\"\u003E\u003Cspan\u003EThe journal is indexed in PubMed Central and PubMed,&nbsp;\u003C\u002Fspan\u003E\u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F430\"\u003EMEDLINE\u003C\u002Fa\u003E\u003Cspan\u003E,&nbsp;\u003C\u002Fspan\u003E\u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F201\"\u003EScopus\u003C\u002Fa\u003E\u003Cspan\u003E, Sherpa\u002FRomeo,&nbsp;\u003C\u002Fspan\u003E\u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F253\"\u003EDOAJ\u003C\u002Fa\u003E\u003Cspan\u003E, EBSCO\u002FEBSCO Essentials, SCIE,&nbsp;\u003C\u002Fspan\u003E\u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F239\"\u003EPsycINFO\u003C\u002Fa\u003E\u003Cspan\u003E&nbsp;and&nbsp;\u003C\u002Fspan\u003E\u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F289\"\u003ECABI\u003C\u002Fa\u003E\u003Cspan\u003E.\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\" data-pm-slice=\"1 1 []\"\u003E\u003Cem\u003EJMIR Mental Health&nbsp;\u003C\u002Fem\u003Ereceived a \u003Ca href=\"..\u002F..\u002F..\u002F..\u002F..\u002Fannouncements\u002F596\" target=\"_blank\"\u003EJournal Impact Factor of 5.8&nbsp;\u003C\u002Fa\u003E(\u003Cspan\u003Eranked&nbsp;\u003C\u002Fspan\u003E\u003Cspan\u003EQ1&nbsp;\u003C\u002Fspan\u003E\u003Cspan\u003E#25\u002F288 journals in the category Psychiatry,&nbsp;\u003C\u002Fspan\u003EJournal Citation Reports 2025 from Clarivate).\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\" data-pm-slice=\"1 1 []\"\u003E\u003Cem\u003EJMIR Mental Health&nbsp;\u003C\u002Fem\u003Ereceived a Scopus&nbsp;CiteScore of \u003Ca href=\"https:\u002F\u002Fwww.jmir.org\u002Fannouncements\u002F572\"\u003E10.2\u003C\u002Fa\u003E&nbsp;(2024), placing it in the 93rd percentile (#35 of 580) as a Q1 journal in the field of Psychiatry and Mental Health.\u003C\u002Fp\u003E",announcementLink:"https:\u002F\u002Fmental.jmir.org\u002Fannouncements\u002F596",copyrightNotice:"Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F2.0\u002F), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work (\"first published in the Journal of Medical Internet Research...\") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http:\u002F\u002Fwww.jmir.org\u002F, as well as this copyright and license information must be included.",focusScopeDesc:"\u003Cp style=\"text-align: justify;\"\u003E\u003Cem\u003EJMIR Mental Health (\u003C\u002Fem\u003E\u003Ca href=\"..\u002F..\u002Fannouncements\u002F478\"\u003EImpact Factor 4.8\u003C\u002Fa\u003E, Editor-in-Chief: John Torous, MD, MBI) is a premier, open-access, peer-reviewed journal indexed in PubMed Central and PubMed, \u003Ca href=\"..\u002F..\u002Fannouncements\u002F430\"\u003EMEDLINE\u003C\u002Fa\u003E, \u003Ca href=\"..\u002F..\u002Fannouncements\u002F201\"\u003EScopus\u003C\u002Fa\u003E, Sherpa\u002FRomeo, \u003Ca href=\"..\u002F..\u002Fannouncements\u002F253\"\u003EDOAJ\u003C\u002Fa\u003E, EBSCO\u002FEBSCO Essentials, SCIE, \u003Ca href=\"..\u002F..\u002Fannouncements\u002F239\"\u003EPsycINFO\u003C\u002Fa\u003E and \u003Ca href=\"..\u002F..\u002Fannouncements\u002F289\"\u003ECABI\u003C\u002Fa\u003E. \u003Cem\u003E\u003C\u002Fem\u003E\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\"\u003EJMIR Mental Health has a unique focus on digital health and internet\u002Fmobile interventions, technologies, and electronic innovations (software and hardware) for mental health, addictions, online counseling, and behavior change. The journal publishes research on system descriptions, theoretical frameworks, review papers, viewpoint\u002Fvision papers, and rigorous evaluations that advance evidence-based care, improve accessibility, and enhance the effectiveness of digital mental health solutions. It also explores innovations in digital psychiatry, e-mental health, and clinical informatics in psychiatry and psychology, with an emphasis on improving patient outcomes and expanding access to care.\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\"\u003E\u003Ca href=\"..\u002F..\u002Fthemes\" target=\"_blank\" rel=\"noopener\"\u003EThe main themes\u002Ftopics covered by this journal can be found here.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\"\u003E\u003Ci\u003EJMIR Mental Health&nbsp;\u003C\u002Fi\u003Ehas an international author and readership and welcomes submissions from around the world.\u003C\u002Fp\u003E\r\n\u003Cp style=\"text-align: justify;\"\u003E\u003C\u002Fp\u003E",googleAnalyticsId:"UA-186918-17",impactFactor:D,journalDescription:"\u003Cp\u003EInternet interventions, technologies, and digital innovations for mental health and behavior change.\u003C\u002Fp\u003E\r\n\u003Cp\u003E\u003Cem\u003EJMIR Mental Health\u003C\u002Fem\u003E is the \u003Ca href=\"..\u002F..\u002Fannouncements\u002F366\"\u003Eofficial journal\u003C\u002Fa\u003E of the \u003Cem\u003E\u003Ca href=\"https:\u002F\u002Fwww.sodpsych.org\u002F\"\u003ESociety of Digital Psychiatry\u003C\u002Fa\u003E.&nbsp;\u003C\u002Fem\u003E\u003C\u002Fp\u003E",journalInitials:"JMH",footer:"\u003Cdiv\u003E\r\n\u003Cul style=\"display: flex; flex-wrap: wrap; list-style: none; justify-content: center;\"\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"http:\u002F\u002Fsearch.crossref.org\u002F?q=2368-7959\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002Fcrossref.jpg\" alt=\"Crossref\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002Fopen-access.jpg\" alt=\"Open Access\" \u002F\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"https:\u002F\u002Foaspa.org\u002Fmember-record-jmir-publications-inc\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002Foaspa.jpg\" alt=\"Open Access Scholarly Publishers Association\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"https:\u002F\u002Fwww.trendmd.com\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002Ftrend-MD.jpg\" alt=\"TrendMD Member\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"https:\u002F\u002Forcid.org\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002FORCID.jpg\" alt=\"ORCID Member\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"https:\u002F\u002Fdoaj.org\u002Ftoc\u002F2368-7959\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002FDOAJ.jpg\" alt=\"Directory of Open Access Journals\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"https:\u002F\u002Fdoaj.org\u002Ftoc\u002F2368-7959\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002Fdoaj_seal_logo_medium.png\" alt=\"DOAJ Seal\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli style=\"margin: 10px;\"\u003E\u003Ca href=\"https:\u002F\u002Fwww.cabdirect.org\u002Fglobalhealth\" target=\"_blank\" rel=\"noopener\"\u003E\u003Cimg src=\"https:\u002F\u002Fasset.jmir.pub\u002Fresources\u002Fimages\u002Fpartners\u002FCABILogo.png\" alt=\"CABI\" \u002F\u003E\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\r\n\u003C\u002Fdiv\u003E",onlineIssn:"2368-7959",searchDescription:"Journal of Medical Internet Research - International Scientific Journal for Medical Research, Information and Communication on the Internet",searchKeywords:"Medical, Medicine, Internet, Research, Journal, ehealth, JMIR,open access publishing, medical research, mental health, addictions, behavior change, digital health",submissionChecklist:[{order:ac,content:"\u003Cp\u003EThe submission has not been previously published nor is it before another journal for consideration; or an explanation has been provided in Comments to the Editor. Related\u002Foverlapping published or submitted work will be uploaded as supplementary files so reviewers and editors can determine the degree of overlap with previous\u002Fother papers under consideration. Salami slicing of research is discouraged.\u003C\u002Fp\u003E"},{order:au,content:"\u003Cp\u003EThe submission file is in Microsoft Word (.doc\u002F.docx) file format.\u003C\u002Fp\u003E"},{order:"3",content:"\u003Cp\u003EThe text meets this journal's formatting requirements, in particular those summarized in the \u003Ca href=\"http:\u002F\u002Fwww.jmir.org?Instructions_for_Authors:Instructions_for_Authors_of_JMIR#checklist\" target=\"_blank\"\u003EAuthor Checklist\u003C\u002Fa\u003E found in Instructions for Authors. The text employs \u003Cem\u003Eitalics\u003C\u002Fem\u003E, rather than \u003Cspan style=\"text-decoration: underline;\"\u003Eunderlining\u003C\u002Fspan\u003E or bold as emphasis; with figures and tables (portrait only, no landscape format) placed within the text, rather than at the end. Additional information has been put in separate files to be uploaded as Multimedia Appendix.\u003C\u002Fp\u003E"},{order:"5",content:"\u003Cp\u003EI have read and understood the \u003Ca href=\"..\u002F..\u002F\u002F?Instructions_for_Authors:Instructions_for_Authors_of_JMIR#Open_Access\" target=\"instr\"\u003Efee schedule\u003C\u002Fa\u003E. In particular, I understand and agree that unless my department\u002Forganization is a \u003Ca href=\"..\u002F..\u002Fsupport.htm\" target=\"member\"\u003Einstitutional member\u003C\u002Fa\u003E BEFORE submission (see dropdown-list in step 1 of the submission process), I\u002Fmy department will be billed for the article processing fee (see Instructions for authors) in case of acceptance. PLEASE MENTION IN THE COVER LETTER ON SUBMISSION THAT YOU 1) AGREE TO PAY THE APF, OR 2) IF YOU THINK THAT THE APF SHOULD BE WAIVED DUE TO MEMBERSHIP OR FOR ANY OTHER REASONS. Journal sections marked with * may be eligible for a fee waiver or reduction under certain circumstances (must be justified in the comments field for the editor on submission). APFs may not apply for article categories marked with * (check instructions for authors). ** Special fees (in particular a submission fee) apply for \u003Ca href=\"..\u002F..\u002F\u002F?Instructions_for_Authors:Protocol_review\"\u003Eresearch protocols and grant proposals\u003C\u002Fa\u003E. Note that the APF will also be billed if the author retracts the manuscript after acceptance, or if a case of scientific misconduct prevents us from publishing a manuscript after acceptance.&nbsp;\u003C\u002Fp\u003E\r\n\u003Cp\u003EPlease note the price increase for JMIR in July 2015.\u003C\u002Fp\u003E"},{order:"6",content:"\u003Cp\u003EAll cited webreferences (webpages, online available PDF reports) which are NOT journal articles or which do not have a DOI have been cached using WebCite (\u003Ca href=\"http:\u002F\u002Fwww.webcitation.org\"\u003Ewww.webcitation.org\u003C\u002Fa\u003E) . Instead of citing the \"live\" webpage\u002Fwebsite, the author should cite the WebCite archived webpage. No URLs in the body of the manuscript are allowed - all URLs are cited as references.\u003C\u002Fp\u003E"},{order:"8",content:"\u003Cp\u003E(please check this checkbox even if you do not wish to fast-track as an indication that you read this). I understand that if I wish to fast-track the paper, I will pay the Fast-Track-Fee immediately after submission (a payment link will be provided after submission) or at a later stage. The FTF guarantees an editorial decision within 15 working days (see website for further instructions)\u003C\u002Fp\u003E"},{order:"9",content:"\u003Cp\u003EI understand that all author names and their affiliations for the final publication will be taken from the database (metadata form), not the submitted manuscript, thus all author names must be entered in the metadata form during submission. Authors may remove author names from the manuscript if they prefer blind review. All coauthors have been\u002Fwill be entered in the metadata form, and all coauthors fulfill ICMJE criteria in that they made 1) substantial contributions to conception and design, or acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it critically for important intellectual content; and 3) final approval of the version to be published. Authors should meet conditions 1, 2, and 3.\u003C\u002Fp\u003E"},{order:"10",content:"\u003Cp\u003EP-values are reported in accordance with our \u003Ca href=\"..\u002F..\u002F\u002F?Instructions_for_Authors:Instructions_for_Authors_of_JMIR#p\" target=\"instr\"\u003Einstructions for authors\u003C\u002Fa\u003E.\u003C\u002Fp\u003E"},{order:av,content:"\u003Cp\u003ESince 26 Oct 2006, we now require payment of a \u003Cstrong\u003EUS$ 90 submission fee\u003C\u002Fstrong\u003E for ALL articles submitted to the \u003Cem\u003EJ Med Internet Res\u003C\u002Fem\u003E&nbsp;(=this journal) EXCEPT letters or invited articles (there is no submission fee for sister journals - please change the journal in the drop down list above before proceeding). You can use Paypal or a credit card immediately after submission. Authors will not be able to complete the submission process without payment. This fee cannot be waived (only exception: invited articles), needs to be paid also by institutional members, and is non-refundable. This fee is in addition to other potential fees such as the optional fast-track fee (FTF) and the article processing fee (APF) for non-members. Authors should understand that the submission fee is non-refundable, even if the manuscript is promptly rejected without peer-review (we do send out the majority of papers for peer-review, but we reserve the right to reject papers without peer-review for any reason, including the topic not being deemed interesting enough, which is a subjective decision by the editor).\u003C\u002Fp\u003E"},{order:"12",content:"\u003Cp\u003EAuthors agree that the manuscript and peer-review reports may be transferred to a JMIR sister\u002Fpartner journal (e.g. \u003Ca href=\"http:\u002F\u002Fwww.i-jmr.org\" target=\"_new\"\u003Ei-JMR\u003C\u002Fa\u003E, \u003Ca href=\"http:\u002F\u002Fwww.researchprotocols.org\" target=\"_new\"\u003EJMIR Res Protoc\u003C\u002Fa\u003E, JMIR mHealth, JMIR Human Factors and others), if the paper is not found suitable for publication in JMIR, but is publishable in another journal. The submission fee for that partner journal (if any) will be waived, and transfer of the peer-review reports may mean that the paper does not have to be re-reviewed. Authors will receive a notification when the manuscript is transferred, and at that time can decide if they want to pursue publication in a sister\u002Fpartner journal. If authors do NOT wish an automatic transfer to an alternative journal after rejection for JMIR, this should be noted in the cover letter.\u003C\u002Fp\u003E"}],articlesWidget:{enabled:g,count:l,label:"Recent Articles"},openReviewWidget:{enabled:g,count:l,label:"\u003Ca href=\"https:\u002F\u002Fpreprints.jmir.org\"\u003EPreprints\u003C\u002Fa\u003E Open for Peer-Review"},searchWidget:{enabled:g},partnershipsWidget:{enabled:g},submitButton:{enabled:g,label:"Submit Article"},editorInChief:"\u003Cp style=\"display: inline-block;\"\u003EJohn Torous, MD, MBI, Harvard Medical School, USA\u003C\u002Fp\u003E"}}},journals:{data:[{journal_id:b,title:"Journal of Medical Internet Research",tag:"The leading peer-reviewed journal for digital medicine and health and health care in the internet age. June 2025 - Journal Impact Factor 6.0. Q1 journal in Health Care Science & Services and Medical Informatics categories. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:a,path:aw,slug:aw,seq:b,enabled:b,environment:c,url:"https:\u002F\u002Fwww.jmir.org",batch:b,year:1999,colour:n,impact:"6.0",order:b,published:10756,transfers:a,cite_score:"11.7"},{journal_id:k,title:"JMIR Research Protocols",tag:"Ongoing trials, grant proposals, formative research, methods, early results. June 2025 - Journal Impact Factor 1.5. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"JMIR Res Protoc publishes research protocols, current and ongoing trials, and grant proposals in all areas of medicine (with an initial focus on ehealth\u002Fmhealth). Publish your work in this journal to let others know what you are working on, to facilitate collaboration and\u002For recruitment, to avoid duplication of efforts, to create a citable record of a research design idea, and to aid systematic reviewers in compiling evidence. Research protocols or grant proposals that are funded and have undergone peer-review will receive an expedited review if you upload peer-review reports as supplementary files.",path:"resprot",slug:"researchprotocols",seq:d,enabled:b,environment:c,url:"https:\u002F\u002Fwww.researchprotocols.org",batch:b,year:E,colour:"#837a7a",impact:"1.5",order:F,published:5264,transfers:a,cite_score:"2.4"},{journal_id:ax,title:"JMIR Formative Research",tag:"Process evaluations, early results and feasibility\u002Fpilot studies of digital and non-digital interventions. June 2025 - Journal Impact Factor 2.1. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:ay,slug:ay,seq:G,enabled:b,environment:c,url:"https:\u002F\u002Fformative.jmir.org",batch:d,year:H,colour:"#605959",impact:"2.1",order:I,published:4027,transfers:a,cite_score:"3.5"},{journal_id:J,title:"JMIR mHealth and uHealth",tag:"Internet interventions, technologies and digital innovations for mental health and behavior change. June 2025 - Journal Impact Factor 6.2. Q1 journal in Health Care Science & Services and Medical Informatics categories. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"JMIR mhealth and uhealth is a new journal focussing on mobile and ubiquitous health technologies, including smartphones, augmented reality (Google Glasses), intelligent domestic devices, implantable devices, and other technologies designed to maintain health and improve life.",path:az,slug:az,seq:K,enabled:b,environment:c,url:"https:\u002F\u002Fmhealth.jmir.org",batch:d,year:L,colour:aA,impact:"6.2",order:K,published:2959,transfers:a,cite_score:"11.6"},{journal_id:M,title:"JMIR Public Health and Surveillance",tag:"A multidisciplinary journal that focuses on the intersection of public health and technology, public health informatics, mass media campaigns, surveillance, participatory epidemiology, and innovation in public health practice and research. June 2025 - Journal Impact Factor 3.9. Q1 journal in Public, Environmental & Occupational Health category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"Innovations in Public Health practice and research",path:aB,slug:aB,seq:N,enabled:b,environment:c,url:"https:\u002F\u002Fpublichealth.jmir.org",batch:b,year:o,colour:"#01538A",impact:"3.9",order:aC,published:1867,transfers:a,cite_score:"6.3"},{journal_id:45,title:"Online Journal of Public Health Informatics",tag:"A leading peer-reviewed, open access journal dedicated to the dissemination of high-quality research and innovation in the field of public health informatics. June 2025 - Journal Impact Factor 1.1.  (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:a,path:aD,slug:aD,seq:O,enabled:b,environment:c,url:"https:\u002F\u002Fojphi.jmir.org",batch:a,year:aE,colour:"#3399FF",impact:"1.1",order:P,published:1751,transfers:a,cite_score:a},{journal_id:t,title:"JMIR Medical Informatics",tag:"Clinical informatics, decision support for health professionals, electronic health records, and ehealth infrastructures. June 2025 - Journal Impact Factor 3.8. Q2 journal in Medical Informatics category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"Clinical informatics",path:aF,slug:aF,seq:k,enabled:b,environment:c,url:"https:\u002F\u002Fmedinform.jmir.org",batch:d,year:L,colour:"#82ABB9",impact:"3.8",order:M,published:1734,transfers:a,cite_score:"7.7"},{journal_id:f,title:ap,tag:aq,description:e,path:j,slug:j,seq:t,enabled:b,environment:c,url:ar,batch:b,year:u,colour:h,impact:D,order:v,published:as,transfers:a,cite_score:at},{journal_id:N,title:"JMIR Human Factors",tag:"Making health care interventions and technologies usable, safe, and effective. June 2025 - Journal Impact Factor 3.0. Q2 journal in Health Care Sciences & Services category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"Usability Studies and Ergonomics",path:aG,slug:aG,seq:aC,enabled:b,environment:c,url:"https:\u002F\u002Fhumanfactors.jmir.org",batch:d,year:u,colour:"#008C9E",impact:aH,order:q,published:1078,transfers:a,cite_score:aI},{journal_id:Q,title:"JMIR Serious Games",tag:"A multidisciplinary journal on gaming and gamification including simulation and immersive virtual reality for health education\u002Fpromotion, teaching and social change. June 2025 - Journal Impact Factor 4.1. Q1 journal in Health Care Science & Services category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"Serious games for health and social change",path:aJ,slug:aJ,seq:M,enabled:b,environment:c,url:"https:\u002F\u002Fgames.jmir.org",batch:b,year:L,colour:"#4A5A67",impact:"4.1",order:N,published:780,transfers:a,cite_score:"8.6"},{journal_id:F,title:"JMIR Medical Education",tag:"Technology, innovation and openess in medical education in the information age. June 2025 - Journal Impact Factor 12.5. June 2025 - Journal Impact Factor 2.2. Q1 journal in Education, Scientific Disciplines category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:aK,slug:aK,seq:aL,enabled:b,environment:c,url:"https:\u002F\u002Fmededu.jmir.org",batch:d,year:o,colour:"#6678A6",impact:"12.5",order:d,published:740,transfers:a,cite_score:av},{journal_id:r,title:"JMIR Aging",tag:"Digital health technologies, apps, and informatics for patient education, medicine and nursing, preventative interventions, and clinical care \u002F home care for elderly populations. June 2025 - Journal Impact Factor 4.8. Q1 journal in Geriatrics & Gerontology and Gerontology categories. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:aM,slug:aM,seq:R,enabled:b,environment:c,url:"https:\u002F\u002Faging.jmir.org",batch:d,year:p,colour:"#979bc4",impact:aI,order:k,published:634,transfers:a,cite_score:"6.6"},{journal_id:S,title:"JMIRx Med",tag:aN,description:a,path:aO,slug:aO,seq:w,enabled:b,environment:c,url:"https:\u002F\u002Fxmed.jmir.org",batch:a,year:T,colour:"#3187df",impact:e,order:G,published:575,transfers:a,cite_score:a},{journal_id:P,title:"JMIR Cancer",tag:"Patient-centered innovations, education and technology for cancer care, cancer survivorship, and cancer research. June 2025 - Journal Impact Factor 2.7. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:aP,slug:aP,seq:l,enabled:b,environment:c,url:"https:\u002F\u002Fcancer.jmir.org",batch:d,year:o,colour:"#584677",impact:"2.7",order:f,published:539,transfers:a,cite_score:"5.9"},{journal_id:R,title:"JMIR Pediatrics and Parenting",tag:"Improving pediatric and adolescent health outcomes and empowering and educating parents. June 2025 - Journal Impact Factor 2.3. Q2 journal in Pediatrics category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:aQ,slug:aQ,seq:U,enabled:b,environment:c,url:"https:\u002F\u002Fpediatrics.jmir.org",batch:d,year:p,colour:"#d2a9ad",impact:aR,order:J,published:an,transfers:a,cite_score:"4.5"},{journal_id:K,title:"Interactive Journal of Medical Research",tag:"A new general medical journal for the 21st centrury, focusing on innovation in health and medical research. June 2025 - Journal Impact Factor 2.2. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:"ijmr",slug:"i-jmr",seq:v,enabled:b,environment:c,url:"https:\u002F\u002Fwww.i-jmr.org",batch:d,year:E,colour:"#22B2C1",impact:V,order:W,published:525,transfers:a,cite_score:a},{journal_id:v,title:"iProceedings",tag:"Electronic Proceedings, Presentations and Posters of Leading Conferences",description:e,path:aS,slug:aS,seq:J,enabled:b,environment:c,url:"https:\u002F\u002Fwww.iproc.org",batch:d,year:o,colour:"#6F7D80",impact:a,order:r,published:510,transfers:a,cite_score:a},{journal_id:U,title:"JMIR Dermatology",tag:"Technologies, devices, apps, and informatics applications for patient education in dermatology, including preventative interventions, and clinical care for dermatological populations",description:e,path:aT,slug:aT,seq:x,enabled:b,environment:c,url:"https:\u002F\u002Fderma.jmir.org",batch:d,year:p,colour:"#ecac7d",impact:a,order:x,published:361,transfers:a,cite_score:"1.8"},{journal_id:W,title:"JMIR Rehabilitation and Assistive Technologies",tag:"Development and Evaluation of Rehabilitation, Physiotherapy and Assistive Technologies, Robotics, Prosthetics and Implants, Mobility and Communication Tools, Home Automation and Telerehabilitation. June 2025 - Journal Impact Factor 3.0. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:aU,slug:aU,seq:aV,enabled:b,environment:c,url:"https:\u002F\u002Frehab.jmir.org",batch:d,year:u,colour:"#15638E",impact:aH,order:Q,published:347,transfers:a,cite_score:"5.7"},{journal_id:X,title:"JMIR Diabetes",tag:"Emerging Technologies, Medical Devices, Apps, Sensors, and Informatics to Help People with Diabetes. June 2025 - Journal Impact Factor 2.6. Q2 journal in Health Care Sciences & Services category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:e,path:aW,slug:aW,seq:Q,enabled:b,environment:c,url:"https:\u002F\u002Fdiabetes.jmir.org",batch:d,year:Y,colour:"#5c89c7",impact:aX,order:aL,published:322,transfers:a,cite_score:"4.7"},{journal_id:x,title:"JMIR Cardio",tag:"Electronic, mobile, digital health approaches in cardiology and for cardiovascular health. June 2025 - Journal Impact Factor 2.2. Q2 journal in Cardiac & Cardiovascular Systems category. (Source: Journal Citation Reports™ 2025 from Clarivate™)  \r",description:e,path:aY,slug:aY,seq:P,enabled:b,environment:c,url:"https:\u002F\u002Fcardio.jmir.org",batch:d,year:H,colour:"#791f20",impact:V,order:aV,published:258,transfers:a,cite_score:"4.3"},{journal_id:C,title:"JMIR AI",tag:"A new peer reviewed journal focused on research and applications for the health artificial intelligence (AI) community. June 2025 - Journal Impact Factor 2.0. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"JMIR AI is a new journal that focuses on the applications of AI in health settings. This includes contemporary developments as well as historical examples, with an emphasis on sound methodological evaluations of AI techniques and authoritative analyses. It is intended to be the main source of reliable information for health informatics professionals to learn about how AI techniques can be applied and evaluated.",path:aZ,slug:aZ,seq:O,enabled:b,environment:c,url:"https:\u002F\u002Fai.jmir.org",batch:d,year:a_,colour:s,impact:"2.0",order:Z,published:229,transfers:a,cite_score:"2.5"},{journal_id:38,title:"JMIR Infodemiology",tag:"Focusing on determinants and distribution of health information and misinformation on the internet, and its effect on public and individual health. June 2025 - Journal Impact Factor 2.3. Q2 journal in Health Care Sciences & Services and Public, Environmental & Occupational Health. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:"Focusing on determinants and distribution of health information and misinformation on the internet, and its effect on public and individual health.",path:a$,slug:a$,seq:ao,enabled:b,environment:c,url:"https:\u002F\u002Finfodemiology.jmir.org",batch:d,year:2021,colour:"#32A852",impact:aR,order:l,published:227,transfers:a,cite_score:"6.5"},{journal_id:w,title:"JMIR Nursing",tag:"Virtualizing care from hospital to community: Mobile mealth, telehealth and digital patient care. June 2025 - Journal Impact Factor 4.0. Q1 journal in Nursing category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ",description:a,path:ba,slug:ba,seq:_,enabled:b,environment:c,url:"https:\u002F\u002Fnursing.jmir.org",batch:d,year:p,colour:"#429a99",impact:"4.0",order:t,published:163,transfers:a,cite_score:"5.1"},{journal_id:bb,title:"Journal of Participatory Medicine",tag:"The Journal of Participatory Medicine is a peer-reviewed, open access journal with the mission to advance the understanding and practice of participatory medicine among health care professionals and patients.\n\nIt is the Official Journal of the Society for Participatory Medicine.",description:e,path:bc,slug:bc,seq:X,enabled:b,environment:c,url:"https:\u002F\u002Fjopm.jmir.org",batch:d,year:aE,colour:"#2ea3f2",impact:a,order:bd,published:148,transfers:a,cite_score:"3.1"},{journal_id:_,title:"JMIR Perioperative Medicine",tag:"Technologies for pre- and post-operative education, preventative interventions and clinical care for surgery and anaesthesiology patients, as well as informatics applications in anesthesia, surgery, critical care and pain medicine",description:e,path:be,slug:be,seq:r,enabled:b,environment:c,url:"https:\u002F\u002Fperiop.jmir.org",batch:d,year:p,colour:"#187662",impact:a,order:U,published:135,transfers:a,cite_score:au},{journal_id:bf,title:"JMIR Biomedical Engineering",tag:"Engineering for health technologies, medical devices, and innovative medical treatments and procedures.",description:e,path:bg,slug:bg,seq:Z,enabled:b,environment:c,url:"https:\u002F\u002Fbiomedeng.jmir.org",batch:d,year:Y,colour:bh,impact:a,order:ax,published:106,transfers:a,cite_score:a},{journal_id:Z,title:"JMIR Bioinformatics and Biotechnology",tag:"Methods, devices, web-based platforms, open data and open software tools for big data analytics, understanding biological\u002Fmedical data, and information retrieval in biology and medicine.",description:e,path:bi,slug:bi,seq:I,enabled:b,environment:c,url:"https:\u002F\u002Fbioinform.jmir.org",batch:d,year:T,colour:bh,impact:a,order:bb,published:72,transfers:a,cite_score:V},{journal_id:43,title:"Asian\u002FPacific Island Nursing Journal",tag:"The official journal of the Asian American \u002F Pacific Islander Nurses Association (AAPINA), devoted to the exchange of knowledge in relation to Asian and Pacific Islander health and nursing care. Created to fill the gap between nursing science and behavioral\u002Fsocial sciences, the journal offers a forum for empirical, theoretical and methodological issues related to Asian American \u002F Pacific Islander ethnic, cultural values and beliefs and biological and physiological phenomena that can affect nursing care.",description:"The official journal of the Asian American \u002F Pacific Islander Nurses Association, this is a peer-reviewed, open access journal for the exchange of knowledge in relation to Asian and Pacific Islander health and nursing care. It will serve as a voice for nursing and other health care providers for research, education, and practice.",path:bj,slug:bj,seq:C,enabled:b,environment:c,url:"https:\u002F\u002Fapinj.jmir.org",batch:d,year:H,colour:bk,impact:e,order:bl,published:49,transfers:a,cite_score:aX},{journal_id:46,title:"JMIR XR and Spatial Computing (JMXR)",tag:"A new peer-reviewed journal for extended reality and spatial computing in health and health care. ",description:a,path:bm,slug:bm,seq:$,enabled:b,environment:c,url:"https:\u002F\u002Fxr.jmir.org",batch:d,year:2024,colour:"#887ECB",impact:a,order:$,published:$,transfers:a,cite_score:a},{journal_id:35,title:"JMIRx Bio",tag:aN,description:e,path:bn,slug:bn,seq:S,enabled:b,environment:c,url:"https:\u002F\u002Fxbio.jmir.org",batch:d,year:2023,colour:"#bf2433",impact:a,order:X,published:O,transfers:a,cite_score:a},{journal_id:42,title:"JMIR Neurotechnology",tag:"Cross-disciplinary journal that connects the broad domains of clinical neuroscience and all related technologies. The journal provides a space for the publication of research exploring how technologies can be applied in clinical neuroscience (e.g., neurology, neurosurgery, neuroradiology) to prevent, diagnose, and treat neurological disorders.",description:"JMIR Neuro is an innovative new journal which aims to bridge clinical neurology & neurosurgery with advances in the web space and digital technologies",path:bo,slug:bo,seq:bl,enabled:b,environment:c,url:"https:\u002F\u002Fneuro.jmir.org",batch:d,year:a_,colour:bk,impact:a,order:bf,published:w,transfers:a,cite_score:a},{journal_id:d,title:"Medicine 2.0",tag:"Official proceedings publication of the Medicine 2.0 Congress",description:a,path:"med20",slug:"medicine20",seq:q,enabled:b,environment:c,url:"https:\u002F\u002Fwww.medicine20.com",batch:b,year:E,colour:aA,impact:a,order:_,published:x,transfers:a,cite_score:a},{journal_id:bd,title:"JMIR Data",tag:"A muldisciplinary journal to publish open datasets for analysis and re-analysis",description:e,path:bp,slug:bp,seq:F,enabled:b,environment:c,url:"https:\u002F\u002Fdata.jmir.org",batch:d,year:T,colour:"#5A6672",impact:a,order:S,published:k,transfers:a,cite_score:a},{journal_id:G,title:"JMIR Challenges",tag:"JMIR Challenges is a new platform connecting \"solution-seekers\" (sponsors such as companies or other researchers) with \"solution-providers\" (entrants, such as innovators, researchers, or developers in the ehealth space)",description:e,path:bq,slug:bq,seq:W,enabled:b,environment:c,url:"https:\u002F\u002Fchallenges.jmir.org",batch:d,year:Y,colour:s,impact:a,order:w,published:d,transfers:a,cite_score:a},{journal_id:I,title:"JMIR Preprints",tag:"A preprint server for pre-publication\u002Fpre-peer-review preprints intended for community review as well as ahead-of-print (accepted) manuscripts",description:"Publish your paper for open peer-review.",path:br,slug:br,seq:f,enabled:b,environment:c,url:"https:\u002F\u002Fpreprints.jmir.org",batch:b,year:o,colour:s,impact:a,order:R,published:b,transfers:a,cite_score:a}]},license:{},page:{data:a},pages:{data:{}},preprint:{data:a},searchArticles:{data:{}},searchAuthors:{data:[],pagination:{}},searchHelp:{data:{},error:a},sections:{data:[],allSections:[],journalSections:[]},submission:{data:{id:a,files:{toc:[],figures:[],appendicies:[],other:[]}},file:a,affiliation:a,suggestedAffiliations:a,events:{extraction:[]},errors:{}},subscription:{},theme:{data:[]},themes:{random:a,data:[],sortType:"pubDate",sortOrder:"desc"}},serverRendered:g,routePath:ad,config:{environment:c,adsEnabled:g,adsDebug:i,_app:{basePath:"\u002F",assetsPath:"\u002F_nuxt\u002F",cdnURL:a}}}}(null,1,"production",2,"",16,true,"#45936C",false,"mental",5,12,0,"#247CB3",2015,2018,10,31,"#666666",7,2014,4,33,26,70610,"Zhou","Beijing","China",41,"5.8",2012,20,22,2017,18,13,3,2013,9,6,39,21,15,30,34,2020,29,"2.2",17,23,2016,19,32,40,"2024-12-27T01:26:41.000Z",243,"1","\u002F2025\u002F1\u002Fe70610","Guangyu","Guangyu Zhou","Prof Dr","School of Psychological and Cognitive Sciences","Beijing Key Laboratory of Behavior and Mental Health, Key Laboratory of Machine Perception (Ministry of Education)","Peking University","Philosophy Building, 2nd Fl.","No. 5 Yiheyuan Road, Haidian District","100871",530,37,"JMIR Mental Health","Internet interventions, technologies and digital innovations for mental health and behavior change. June 2025 - Journal Impact Factor 5.8. Q1 journal in Psychiatry category. (Source: Journal Citation Reports™ 2025 from Clarivate™) ","https:\u002F\u002Fmental.jmir.org",1227,"10.2","2","11","jmir",27,"formative","mhealth","#65AD8C","publichealth",8,"ojphi",2009,"medinform","humanfactors","3.0","4.8","games","mededu",11,"aging","Overlay journal for preprints with post-review manuscript marketplace","xmed","cancer","pediatrics","2.3","iproc","derma","rehab",14,"diabetes","2.6","cardio","ai",2022,"infodemiology","nursing",28,"jopm",25,"periop",24,"biomedeng","#474760","bioinform","apinj","#3399ff",36,"xr","xbio","neuro","data","challenges","preprints"));</script><script src="/_nuxt/9c7c1e4.js" defer></script><script src="/_nuxt/b70fdb7.js" defer></script><script src="/_nuxt/4916c14.js" defer></script><script src="/_nuxt/8043cc4.js" defer></script><script src="/_nuxt/797995a.js" defer></script><script src="/_nuxt/d965382.js" defer></script><script src="/_nuxt/4927325.js" defer></script><script src="/_nuxt/2bde5b8.js" defer></script><script src="/_nuxt/1326996.js" defer></script><script src="/_nuxt/09b1d65.js" defer></script><script src="/_nuxt/366ad09.js" defer></script><script src="/_nuxt/95fde3b.js" defer></script><script src="/_nuxt/d0c41c5.js" defer></script><script src="/_nuxt/e6dad64.js" defer></script><script src="/_nuxt/84066b5.js" defer></script><script src="/_nuxt/fc6cf93.js" defer></script><script src="/_nuxt/14f78e7.js" defer></script><script src="/_nuxt/aa08135.js" defer></script><script src="/_nuxt/75832c0.js" defer></script><script src="/_nuxt/4436098.js" defer></script><script src="/_nuxt/bcc9d63.js" defer></script><script src="/_nuxt/3d6a000.js" defer></script><script src="/_nuxt/98a1aa4.js" defer></script><script src="/_nuxt/89ba270.js" defer></script><script src="/_nuxt/bc9dad6.js" defer></script><script src="/_nuxt/fc087e0.js" defer></script><script src="/_nuxt/b643d0c.js" defer></script><script src="/_nuxt/68827e8.js" defer></script><script src="/_nuxt/421b4ad.js" defer></script><script src="/_nuxt/bbb2307.js" defer></script><script src="/_nuxt/d9f2686.js" defer></script><script src="/_nuxt/255fd1e.js" defer></script><script src="/_nuxt/0bceff3.js" defer></script><script src="/_nuxt/7b27d0b.js" defer></script><script src="/_nuxt/e8be31d.js" defer></script><script src="/_nuxt/70e9da6.js" defer></script><script src="/_nuxt/beec627.js" defer></script><script src="/_nuxt/f98275c.js" defer></script><script src="/_nuxt/4c795dc.js" defer></script><script src="/_nuxt/08b91c7.js" defer></script><script src="/_nuxt/3df9693.js" defer></script><script src="/_nuxt/9a1b312.js" defer></script><script src="/_nuxt/21899a6.js" defer></script><script src="/_nuxt/bf91a67.js" defer></script><script src="/_nuxt/e7d56e2.js" defer></script><script src="/_nuxt/9c711d7.js" defer></script><script src="/_nuxt/a116589.js" defer></script><script src="/_nuxt/e8da5fa.js" defer></script><script data-n-head="ssr" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js" data-body="true" defer></script><script data-n-head="ssr" src="https://badge.dimensions.ai/badge.js" data-body="true" defer></script><script data-n-head="ssr" type="text/javascript" charset="utf-8" data-body="true">_linkedin_partner_id = "3149908"; window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || []; window._linkedin_data_partner_ids.push(_linkedin_partner_id);</script><script data-n-head="ssr" type="text/javascript" charset="utf-8" data-body="true">(function(l) { if (!l){window.lintrk = function(a,b){window.lintrk.q.push([a,b])}; window.lintrk.q=[]} var s = document.getElementsByTagName("script")[0]; var b = document.createElement("script"); b.type = "text/javascript";b.async = true; b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js"; s.parentNode.insertBefore(b, s);})(window.lintrk);</script><script data-n-head="ssr" type="text/javascript" charset="utf-8" data-body="true">
                    (function(w, d) {
                                var s = d.createElement('script');
                                s.src = '//cdn.adpushup.com/48547/adpushup.js';
                                s.crossOrigin='anonymous'; 
                                s.type = 'text/javascript'; s.async = true;
                                (d.getElementsByTagName('head')[0] || d.getElementsByTagName('body')[0]).appendChild(s);
                                w.adpushup = w.adpushup || {que:[]};
                        })(window, document);
                    </script><script data-n-head="ssr" type="text/javascript" charset="utf-8" data-body="true">
(function(w, d) {
    var s = d.createElement('script');
    s.src = '//cdn.adpushup.com/48547/adpushup.js';
    s.crossOrigin='anonymous'; 
    s.type = 'text/javascript'; s.async = true;
    (d.getElementsByTagName('head')[0] || d.getElementsByTagName('body')[0]).appendChild(s);
    w.adpushup = w.adpushup || {que:[]};
})(window, document);

var adpushup = window.adpushup = window.adpushup || {};
adpushup.que = adpushup.que || [];

// Note: Individual ad triggering is now handled by each component
// when they mount and verify they should be displayed (desktop only)
</script></body></html>