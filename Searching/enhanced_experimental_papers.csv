search_cluster,title,abstract,url,year,predicted_cluster,predicted_label,confidence
0,Customizing emotional support: How do individuals construct and interact with LLM-powered chatbots,"Personalized support is essential to fulfill individuals’ emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.",https://www.semanticscholar.org/paper/a4b5e544c8bb18f844f2c8836d3a8c5f8fc9ffc7,2025,0,Cluster 0: Social & Collaboration,0.8111
0,Utilizing artificial intelligence to enhance social connections–the alleviating effect of emotionally intelligent chatbots on loneliness,"OBJECTIVE
To enhance social connections using artificial intelligence and explore the alleviating effect of emotionally intelligent chatbots on loneliness.


METHOD
A stratified sampling method was used to distribute the Emotional Social Loneliness Inventory (ESLI) to full-time college students. Based on the ESLI assessment results, 120 young people with severe loneliness were selected as the research subjects. Regularly interact with Rplika chatbot to obtain psychological support and academic assistance (continuous intervention for 1 month). Compare the scores of the ESLI scale, SASS CS scale, IES scale, and CD-RISC scale in young individuals with severe loneliness before and 1, 3, and 5 months after intervention.


RESULTS
There was no statistically significant difference in ESLI scores between the two groups of college students before intervention (P>0.05). After 1, 3, and 5 months of intervention, the ESLI scores of the experimental group were lower than those of the control group (P<0.05). There was no statistically significant difference in social anxiety scores between the two groups of college students before intervention (P>0.05). After 1, 3, and 5 months of intervention, the social anxiety scores of the experimental group were lower than those of the control group (P<0.05). There was no statistically significant difference in social self-efficacy and psychological resilience levels between the two groups of college students before intervention (P>0.05). After intervention, the social self-efficacy and CD-RISC scores of the experimental group were higher than those of the control group (P<0.05).


CONCLUSION
High emotional intelligence AI chatbots can significantly improve and alleviate feelings of loneliness and enhance social skills, opening up new avenues for technology assisted psychological intervention.",https://www.semanticscholar.org/paper/f2d90302efca8f43139fb795bd7f558ecef70d8b,2025,0,Cluster 0: Social & Collaboration,0.7849
0,""" My Boyfriend is AI"": A Computational Analysis of Human-AI Companionship in Reddit's AI Community","The emergence of AI companion applications has created novel forms of intimate human-AI relationships, yet empirical research on these communities remains limited. We present the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community (27,000+ members). Using exploratory qualitative analysis and quantitative analysis employing classifiers, we identify six primary conversation themes, with visual sharing of couple pictures and ChatGPT-specific discussions dominating the discourse of the most viewed posts. Through analyzing the top posts in the community, our findings reveal how community members'AI companionship emerges unintentionally through functional use rather than deliberate seeking, with users reporting therapeutic benefits led by reduced loneliness, always-available support, and mental health improvements. Our work covers primary concerns about human intimacy with AIs such as emotional dependency, reality dissociation, and grief from model updates. We observe users materializing relationships following traditional human-human relationship customs, such as wedding rings. Community dynamics indicate active resistance to stigmatization through advocacy and mutual validation. This work contributes an empirical understanding of AI companionship as an emerging sociotechnical phenomenon.",https://www.semanticscholar.org/paper/52c3519ae79c841fbd898a23bc292d011bb40620,2025,0,Cluster 0: Social & Collaboration,0.8753
0,Artificial intelligence chatbots as a source of virtual social support: Implications for loneliness and anxiety management,"Loneliness, social isolation, and anxiety affect millions of people across the world. Communication technologies, including artificial intelligence (AI) chatbots, can potentially offer support to those experiencing mental health challenges by providing companionship and support. Specifically, social AI can mimic human interaction, which may help alleviate loneliness and anxiety through person‐centered messaging. Despite growing AI usage, there is limited research on the effectiveness of specific message types in this context. Thus, this study employed a 2 (person‐centered message: high vs. low) × 2 (context: loneliness vs. anxiety) between‐subjects design to test how different supportive messages from social AI chatbots impact subsequent outcomes. Results revealed that high person‐centered messages are associated with increased emotional validation. Furthermore, the quality of social support and interpersonal warmth (IW) mediated the relationship between high person‐centered messages and emotional validation. Finally, the mediation effect between high person‐centered messages and emotional validation via the quality of emotional support was moderated by social presence, but not the mediation effect between high person‐centered messages and emotional validation via IW. These results demonstrate the importance of developing social AI chatbots that employ messages high in person‐centeredness, as these messages are most important for addressing mental health concerns.",https://www.semanticscholar.org/paper/df71cde0ba65a75523ae673fbe91c7b6292b958f,2025,0,Cluster 0: Social & Collaboration,0.8194
0,The role of ChatGPT in mitigating loneliness among older adults: An exploratory study,"Purpose: This exploratory study aims to investigate the potential of ChatGPT in mitigating loneliness among older adults.
Design/methodology/approach: 20 participants aged 60 and above engaged in three conversational sessions with ChatGPT over two weeks. Data collection involved pre- and post-intervention assessments using the UCLA Loneliness Scale, analysis of conversation transcripts, and semi-structured interviews.
Findings: Our findings indicate that ChatGPT shows promise in alleviating loneliness among older adults. Participants found the tool easy to use, engaging, and emotionally supportive. They established an emotional connection with ChatGPT, suggesting its potential to provide comfort and companionship to those experiencing loneliness.
Conclusion: ChatGPT demonstrates potential as a tool to address loneliness in older adults, offering emotional support and engagement. However, it should be viewed as a complement rather than a replacement for human interaction. Future research should explore its long-term efficacy and its integration with other interventions.",https://www.semanticscholar.org/paper/78754675c91b505914a26233976deaf3a6e3bed4,2024,0,Cluster 0: Social & Collaboration,0.5662
0,How AI and Human Behaviors Shape Psychosocial Effects of Extended Chatbot Use: A Longitudinal Randomized Controlled Study,"As people increasingly seek emotional support and companionship from AI chatbots, understanding how such interactions impact mental well-being becomes critical. We conducted a four-week randomized controlled experiment (n=981,>300k messages) to investigate how interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence four psychosocial outcomes: loneliness, social interaction with real people, emotional dependence on AI, and problematic AI usage. No significant effects were detected from experimental conditions, despite conversation analyses revealing differences in AI and human behavioral patterns across the conditions. Instead, participants who voluntarily used the chatbot more, regardless of assigned condition, showed consistently worse outcomes. Individuals'characteristics, such as higher trust and social attraction towards the AI chatbot, are associated with higher emotional dependence and problematic use. These findings raise deeper questions about how artificial companions may reshape the ways people seek, sustain, and substitute human connections.",https://www.semanticscholar.org/paper/fea512e6abaf780cffcb240e77aa0f285f60239e,2025,0,Cluster 0: Social & Collaboration,0.8879
0,""" I Like Sunnie More Than I Expected!"": Exploring User Expectation and Perception of an Anthropomorphic LLM-based Conversational Agent for Well-Being Support","user study that compared participants’ expectations and perceptions of two different systems:  Sunnie, an anthropomorphic LLM- of LLM-based conversational agents in mental health is",https://arxiv.org/abs/2405.13803,2024,0,Cluster 0: Social & Collaboration,0.705
0,More than just a chat: A taxonomy of consumers' relationships with conversational AI agents and their well-being implications,"
Purpose
This paper aims to study the role of self-concept in consumer relationships with anthropomorphised conversational artificially intelligent (AI) agents. First, the authors investigate how the self-congruence between consumer self-concept and AI and the integration of the conversational AI agent into consumer self-concept might influence such relationships. Second, the authors examine whether these links with self-concept have implications for mental well-being.


Design/methodology/approach
This study conducted in-depth interviews with 20 consumers who regularly use popular conversational AI agents for functional or emotional tasks. Based on a thematic analysis and an ideal-type analysis, this study derived a taxonomy of consumer–AI relationships, with self-congruence and self–AI integration as the two axes.


Findings
The findings unveil four different relationships that consumers forge with their conversational AI agents, which differ in self-congruence and self–AI integration. Both dimensions are prominent in replacement and committed relationships, where consumers rely on conversational AI agents for companionship and emotional tasks such as personal growth or as a means for overcoming past traumas. These two relationships carry well-being risks in terms of changing expectations that consumers seek to fulfil in human-to-human relationships. Conversely, in the functional relationship, the conversational AI agents are viewed as an important part of one’s professional performance; however, consumers maintain a low sense of self-congruence and distinguish themselves from the agent, also because of the fear of losing their sense of uniqueness and autonomy. Consumers in aspiring relationships rely on their agents for companionship to remedy social exclusion and loneliness, but feel this is prevented because of the agents’ technical limitations.


Research limitations/implications
Although this study provides insights into the dynamics of consumer relationships with conversational AI agents, it comes with limitations. The sample of this study included users of conversational AI agents such as Siri, Google Assistant and Replika. However, future studies should also investigate other agents, such as ChatGPT. Moreover, the self-related processes studied here could be compared across public and private contexts. There is also a need to examine such complex relationships with longitudinal studies. Moreover, future research should explore how consumers’ self-concept could be negatively affected if the support provided by AI is withdrawn. Finally, this study reveals that in some cases, consumers are changing their expectations related to human-to-human relationships based on their interactions with conversational AI agents.


Practical implications
This study enables practitioners to identify specific anthropomorphic cues that can support the development of different types of consumer–AI relationships and to consider their consequences across a range of well-being aspects.


Originality/value
This research equips marketing scholars with a novel understanding of the role of self-concept in the relationships that consumers forge with popular conversational AI agents and the associated well-being implications.
",https://www.semanticscholar.org/paper/2c8162873c93bd96f6df68b8d9df9453eb0197ad,2024,0,Cluster 0: Social & Collaboration,0.7393
0,"Role of synchronous, moderated, and anonymous peer support chats on reducing momentary loneliness in older adults: retrospective observational study","Background Older adults have a high rate of loneliness, which contributes to increased psychosocial risk, medical morbidity, and mortality. Digital emotional support interventions provide a convenient and rapid avenue for additional support. Digital peer support interventions for emotional struggles contrast the usual provider-based clinical care models because they offer more accessible, direct support for empowerment, highlighting the users’ autonomy, competence, and relatedness. Objective This study aims to examine a novel anonymous and synchronous peer-to-peer digital chat service facilitated by trained human moderators. The experience of a cohort of 699 adults aged ≥65 years was analyzed to determine (1) if participation, alone, led to measurable aggregate change in momentary loneliness and optimism and (2) the impact of peers on momentary loneliness and optimism. Methods Participants were each prompted with a single question: “What’s your struggle?” Using a proprietary artificial intelligence model, the free-text response automatched the respondent based on their self-expressed emotional struggle to peers and a chat moderator. Exchanged messages were analyzed to quantitatively measure the change in momentary loneliness and optimism using a third-party, public, natural language processing model (GPT-4 [OpenAI]). The sentiment change analysis was initially performed at the individual level and then averaged across all users with similar emotion types to produce a statistically significant (P<.05) collective trend per emotion. To evaluate the peer impact on momentary loneliness and optimism, we performed propensity matching to align the moderator+single user and moderator+small group chat cohorts and then compare the emotion trends between the matched cohorts. Results Loneliness and optimism trends significantly improved after 8 (P=.02) to 9 minutes (P=.03) into the chat. We observed a significant improvement in the momentary loneliness and optimism trends between the moderator+small group compared to the moderator+single user chat cohort after 19 (P=.049) and 21 minutes (P=.04) for optimism and loneliness, respectively. Conclusions Chat-based peer support may be a viable intervention to help address momentary loneliness in older adults and present an alternative to traditional care. The promising results support the need for further study to expand the evidence for such cost-effective options.",https://www.semanticscholar.org/paper/d7eb7694734795686dd09d5cfcaca83ff6f296bd,2024,0,Cluster 0: Social & Collaboration,0.6983
0,What People Share With a Robot When Feeling Lonely and Stressed and How It Helps Over Time,"Loneliness and stress are prevalent among young adults and are linked to significant psychological and health-related consequences. Social robots may offer a promising avenue for emotional support, especially when considering the ongoing advancements in conversational AI. This study investigates how repeated interactions with a social robot influence feelings of loneliness and perceived stress, and how such feelings are reflected in the themes of user disclosures towards the robot. Participants engaged in a five-session robot-led intervention, where a LLM-powered QTrobot facilitated structured conversations designed to support cognitive reappraisal. Results from linear mixed-effects models show significant reductions in both loneliness and perceived stress over time. Additionally, semantic clustering of 560 user disclosures towards the robot revealed six distinct conversational themes. Results from Kruskal-Wallis H-test demonstrate that participants reporting higher loneliness and stress, more frequently engaged in socially focused disclosures, such as friendship and connection, whereas lower distress was associated with introspective and goal-oriented themes (e.g., academic ambitions). By exploring both how the intervention affects well-being, as well as how well-being shapes the content of robot-directed conversations, we aim to capture the dynamic nature of emotional support in human–robot interaction.",https://www.semanticscholar.org/paper/5f3cbdc7b1f37473e0f9d66d7f2d22482a48c79c,2025,0,Cluster 0: Social & Collaboration,0.763
1,Persuasion with large language models: a survey,"The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.",https://www.semanticscholar.org/paper/55ee883d33ce4e434495921d153f2fda536b154f,2024,1,Cluster 1: Psychology & Persuasion,0.7891
1,Deceptive explanations by large language models lead people to change their beliefs about misinformation more often than honest explanations,"Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and discredit true information. We examined the impact of deceptive AI generated explanations on individuals’ beliefs in a pre-registered online experiment with 11,780 observations from 589 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that logically invalid explanations are deemed less credible - diminishing the effects of deception. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.",https://www.semanticscholar.org/paper/18d7eb4b41df1647819cd465047a09201c9aff5e,2025,1,Cluster 1: Psychology & Persuasion,0.8177
1,A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies,"In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the""receive""and""accept""steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.",https://www.semanticscholar.org/paper/0e9d76c93819c94cf6e9d7356ef90eefe7f25258,2025,1,Cluster 1: Psychology & Persuasion,0.7827
1,Testing theories of political persuasion using AI,"Significance We leverage recent advances in AI in two original research studies regarding persuasion about controversial political issues in the United States. These studies make both methodological and substantive contributions to the study of persuasion. First, we demonstrate that large language models (LLMs) can be used to efficiently test theories of persuasion by overcoming many practical and logistical challenges faced by persuasion researchers. Second, we use LLMs to isolate and test the persuasive gains offered by two key hypothesized mechanisms of attitude change—message customization and elaboration. We show that LLMs are approximately equally persuasive across conditions and that LLM-generated messages with microtargeted customization and messages that promote elaboration through interaction are not clearly more persuasive than a generic message.",https://www.semanticscholar.org/paper/7387ec1e4a2a7b00e47fcccb406715cad1bc936d,2025,1,Cluster 1: Psychology & Persuasion,0.7978
1,More human than human: measuring ChatGPT political bias,"We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.",https://www.semanticscholar.org/paper/3d8a3517231643c1df79bc32c8c2664a4cba3a41,2024,1,Cluster 1: Psychology & Persuasion,0.7715
1,"Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts","Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.",https://www.semanticscholar.org/paper/873bc4aaa1f8430723221efcc6c6b44a7551b79d,2025,1,Cluster 1: Psychology & Persuasion,0.7909
1,The levers of political persuasion with conversational AI,"There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs'unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.",https://www.semanticscholar.org/paper/89a7bae8aac5ff4dd1fe31c20094d4610f878866,2025,1,Cluster 1: Psychology & Persuasion,0.8513
1,The levers of political persuasion with conversational artificial intelligence,"There are widespread fears that conversational artificial intelligence (AI) could soon exert unprecedented influence over human beliefs. In this work, in three large-scale experiments (N = 76,977 participants), we deployed 19 large language models (LLMs)-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. We show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51 and 27%, respectively-than from personalization or increasing model scale, which had smaller effects. We further show that these methods increased persuasion by exploiting LLMs' ability to rapidly access and strategically deploy information and that, notably, where they increased AI persuasiveness, they also systematically decreased factual accuracy.",https://www.semanticscholar.org/paper/113fc327c4a337208d4e7729c4f3e9f37b5edbc6,2025,1,Cluster 1: Psychology & Persuasion,0.8657
1,"Large language models are as persuasive as humans, but how? About the cognitive effort and moral-emotional language of LLM arguments","Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about how they do it. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuasion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.",https://www.semanticscholar.org/paper/415f33b88476688b5f375a8276919d51320f9dbc,2024,1,Cluster 1: Psychology & Persuasion,0.7699
1,Citations and trust in llm generated responses,"Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.",https://www.semanticscholar.org/paper/927451a9310f98f7fc1ea4bf0f4bbce30db3185a,2025,1,Cluster 1: Psychology & Persuasion,0.6615
2,Can LLM-Powered Multi-Agent Systems Augment Human Creativity? Evidence from Brainstorming Tasks,"This paper investigates whether LLM-powered multi-agent systems can effectively augment human creativity in collaborative brainstorming tasks. Traditional brainstorming methods face persistent challenges including evaluation apprehension and free riding, while existing LLM-based discussion systems suffer from premature convergence and homogeneous perspectives that limit creative exploration. To address these dual challenges, we develop a novel multi-agent brainstorming framework that integrates three key innovations: (1) an extended Issue-Based Information System (IBIS) that adds ""Theme"" nodes and ""Idea-to-Idea"" transformation paths to prevent premature convergence and enable systematic idea expansion; (2) dynamic role-playing where multiple distinct agent types each dynamically select from topic-specific personas to ensure diverse perspectives while maintaining coherent discussion structure; and (3) evidence-based information suggestion capabilities using web search to stimulate discussion with objective facts rather than subjective criticism. Our experimental evaluation employed a rigorous counter-balanced design comparing human-only, human-agent collaboration, and agent-only conditions across both general topics and domain-specific topics. Results suggest striking domain-dependent effectiveness. For general topics, agent collaboration appears to enhance human creativity, with improvements in originality scores and participants reporting increased agreement that diverse ideas were generated. BERT embedding visualization suggests enhanced semantic dispersion, indicating stronger divergent thinking as agent-assisted posts spread into previously unexplored conceptual territories. The system shows promise in addressing traditional brainstorming challenges: evaluation apprehension appears to decrease as participants report reduced social pressure, and free riding diminishes with the majority of participants selecting agent contributions as the most stimulating ideas. However, for domain-specific topics requiring specialized knowledge, agent effectiveness appears to diminish substantially. Human response rates to agent posts drop noticeably, while creativity metrics suggest decreases rather than improvements. Expert evaluation reveals that a substantial proportion of agent contributions on domain-specific topics are deemed irrelevant by domain knowledge holders, suggesting limitations in current LLM capabilities for specialized contexts where agents lack access to tacit organizational knowledge and domain expertise. This work contributes the first systematic evaluation of structured multi-agent systems for human creativity augmentation, demonstrating both significant potential and important considerations for system design. Our findings provide essential insights for developing effective human-AI collaborative creativity systems: while the combination of IBIS structure, dynamic role-playing, and information suggestion shows promise for general domains, future systems should consider domain-adaptive architectures with specialized knowledge integration to optimize effectiveness across diverse knowledge domains. While our preliminary findings suggest meaningful effects, larger-scale studies will be necessary to establish statistical significance and generalizability across diverse populations and contexts. These results establish initial design principles for next-generation collaborative intelligence systems that can appropriately balance human expertise with AI capabilities.",https://www.semanticscholar.org/paper/9c17fd5d1302bdc0bb7c66a1c767ab6d3b23a4bc,2025,2,Cluster 2: Creativity & Ideation,0.7445
2,Human‐AI Co‐Creativity: Does ChatGPT Make Us More Creative?,", ChatGPT assistance they received is evidence of lower expectations that have  -creativity  assistance provided by ChatGPT, suggesting that participants may tend to see generative AI",https://onlinelibrary.wiley.com/doi/abs/10.1002/jocb.70022,2025,2,Cluster 2: Creativity & Ideation,0.8157
2,AI and the Future of Collaborative Work: Group Ideation with an LLM in a Virtual Canvas,"The introduction of generative AI into multi-user applications raises novel considerations for the future of collaborative work. How might collaborative work practices change? How might we incorporate generative AI into shared tools with users’ needs at the forefront? We examine these questions in the context of a remote team conducting ideation tasks – an example of collaborative work enabled by a shared digital workspace. We conducted a user study with 17 professionals experienced with virtual group ideation workshops. Our study examined their use of the Collaborative Canvas, a virtual canvas tool with integrated generative AI capabilities that we created as a probe. Participants saw value in using generative AI to assist with group facilitation and to augment perspectives and ideas. However, they worried about losing human perspectives and critical thinking, as well as reputational harms resulting from harmful AI outputs. Participants shared suggestions for appropriate ways to incorporate generative AI capabilities within multi-user applications and identified needs for transparency of content ownership, private digital spaces, and specialized AI capabilities. Based on participants’ insights, we share implications and opportunities for the incorporation of generative AI into collaborative work in ways that place user needs at the forefront.",https://www.semanticscholar.org/paper/ee4c1921d60daceec5c02e1e15a157060aa33422,2024,2,Cluster 2: Creativity & Ideation,0.64
2,Supermind ideator: How scaffolding human-AI collaboration can increase creativity,"Previous efforts to support creative problem-solving have included (a) techniques such as brainstorming and design thinking to stimulate creative ideas, and (b) software tools to record and share these ideas. Now, generative AI technologies can suggest new ideas that might never have occurred to the users, and users can then select from these ideas or use them to stimulate even more ideas. To explore these possibilities, we developed a system called Supermind Ideator that uses a large language model (LLM) and adds prompts, fine tuning, and a specialized user interface in order to help users reformulate their problem statements and generate possible solutions. This provides scaffolding to guide users through a set of creative problem-solving techniques, including some techniques specifically intended to help generate innovative ideas about designing groups of people and/or computers (“superminds”). In an experimental study, we found that people using Supermind Ideator generated significantly more innovative ideas than those generated by people using ChatGPT or people working alone. Thus our results suggest that the benefits of using LLMs for creative problem-solving can be substantially enhanced by scaffolding designed specifically for this purpose.",https://www.semanticscholar.org/paper/880ad51160400cf0ef278722875cc6a9d237a583,2024,2,Cluster 2: Creativity & Ideation,0.7967
2,Human creativity in the age of llms: Randomized experiments on divergent and convergent thinking,"Our findings reveal that while LLM assistance can provide short-term boosts in creativity   experimental conditions, participants exhibited a reduction in selfreported creativity ratings from",https://dl.acm.org/doi/abs/10.1145/3706598.3714198,2025,2,Cluster 2: Creativity & Ideation,0.746
2,An empirical investigation of the impact of ChatGPT on creativity,"Across five experiments, we asked participants to use ChatGPT ( We found that using  ChatGPT increased the creativity of the  Furthermore, ChatGPT was most effective at generating",https://www.semanticscholar.org/paper/c403fa6edfd563431f0692c749dedbd364e734fe,2024,2,Cluster 2: Creativity & Ideation,0.7338
2,How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent,"Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry. The system’s source is available at: https://github.com/yiren-liu/coquest.",https://www.semanticscholar.org/paper/b4db5c8d8f3f0cb3e37305314fb7edbabaca43f1,2024,2,Cluster 2: Creativity & Ideation,0.7735
2,The Roles of Idea Generation and Elaboration in Human-AI Collaborative Creativity,"When provided AI assistance, we allowed participants to freely interact with the LLM chatbot,  thus allowing us to investigate whether any property of these interactions may be driving",https://www.researchgate.net/profile/Simone-Luchini/publication/392122488_The_Roles_of_Idea_Generation_and_Elaboration_in_Human-AI_Collaborative_Creativity/links/684412196a754f72b5910290/The-Roles-of-Idea-Generation-and-Elaboration-in-Human-AI-Collaborative-Creativity.pdf,2025,2,Cluster 2: Creativity & Ideation,0.7857
2,When AI Joins the Brainstorm: Impacts of Generative Language Models on Collaborative Divergent Thinking,"the collaboration, participants’ experiences, and creativity of outcomes  OCSAI is a Large  Language Model (LLM) fine-tuned on  of creativity assessment, particularly in divergent thinking",https://aisel.aisnet.org/icis2025/gen_ai/gen_ai/24/,2025,2,Cluster 2: Creativity & Ideation,0.8044
2,Effects of ChatGPT use on undergraduate students' creativity: a threat to creative thinking?,"Concerns have arisen regarding the potential negative impact of long-term ChatGPT use on creative thinking. This study investigates these effects using a one-group, pretest–posttest design involving 31 undergraduates over a 10-week intervention period. Participants used ChatGPT to complete academic assignments focused on designing teaching units on bacterial growth, chronic disease, and the human body. Creativity was assessed using the CREA-Creative Intelligence test, a valid and reliable instrument. Data were analyzed using both frequentist and Bayesian methods. The results showed that 25% of participants experienced decreased creativity, 21.43% remained unchanged, and 53.57% showed improvement. Overall, the study found no evidence that ChatGPT negatively impacts creative thinking.",https://www.semanticscholar.org/paper/891e9d4dec15bd19ca469820a20298877cccf1d8,2024,2,Cluster 2: Creativity & Ideation,0.6432
3,"Integrating large language models into EFL writing instruction: effects on performance, self-regulated learning strategies, and motivation","randomized controlled trial, 65 elementary school students were divided into an experimental  group receiving CALLA-LLM  CALLA-LLM model embodies this approach, integrating LLM",https://www.semanticscholar.org/paper/d502564f6cf919013da1300489f002331bc6e156,2024,3,Cluster 3: Education & Productivity,0.6252
3,Enhancing university level English proficiency with generative AI: Empirical insights into automated feedback and learning outcomes,"This paper investigates the effects of large language model (LLM) based feedback on the essay writing proficiency of university students in Hong Kong. It focuses on exploring the potential improvements that generative artificial intelligence (AI) can bring to student essay revisions, its effect on student engagement with writing tasks, and the emotions students experience while undergoing the process of revising written work. Utilizing a randomized controlled trial, it draws comparisons between the experiences and performance of 918 language students at a Hong Kong university, some of whom received generated feedback (GPT-3.5-turbo LLM) and some of whom did not. The impact of AI-generated feedback is assessed not only through quantifiable metrics, entailing statistical analysis of the impact of AI feedback on essay grading, but also through subjective indices, student surveys that captured motivational levels and emotional states, as well as thematic analysis of interviews with participating students. The incorporation of AI-generated feedback into the revision process demonstrated significant improvements in the caliber of students’ essays. The quantitative data suggests notable effect sizes of statistical significance, while qualitative feedback from students highlights increases in engagement and motivation as well as a mixed emotional experience during revision among those who received AI feedback.",https://www.semanticscholar.org/paper/717670161041dac08825fd6c9e4de168ad05bf36,2024,3,Cluster 3: Education & Productivity,0.8243
3,Enhancing critical thinking skills in ChatGPT-human interaction: A scoping review,"Introduction: The rapid integration of generative artificial intelligence (GenAI) technologies, including ChatGPT, into educational environments has introduced both opportunities and challenges for learners and educators. While GenAI can support advanced learning practices, it also raises concerns about critical engagement and the accuracy of generated content. Previous systematic reviews have explored GenAI’s relationship with critical thinking (CT) and self-regulated learning, but a focused synthesis of recent empirical evidence on GenAI’s impact on university students’ CT skills remains lacking. 
Method: This scoping review followed the PRISMA-ScR guidelines and applied the Arksey and O’Malley framework alongside the Population – Concept –  Context (PCC) model. Studies were identified via the Scopus database, using inclusion criteria limited to the years 2024–2025, English language, and the Social Sciences subject area. Thirty eligible empirical studies were analysed and visualised using VOSviewer to identify thematic clusters and categories in the literature. 
Results: The reviewed studies were grouped into seventeen thematic clusters by the VOSviewer and then manually synthesized into six categories based on semantic interpretation: cognitive and metacognitive development, pedagogical innovation and learning design, academic writing and language learning, AI literacy and learner perception, evaluation and assessment technologies, global and ethical dimensions of GenAI use. The findings were analysed as (1) direct enhancement of CT, (2) metacognitive and reflective gains, (3) contextual factors shaping CT, (4) risks of cognitive offloading, and (5) instructional strategies mediating AI’s effect. 21 publications showed predominantly positive impact of GenAI on CT (idea generation, conceptual understanding, construction of arguments, literature review, academic writing, etc.) whereas reported found mixed impact. 
Conclusion: The review concludes that GenAI holds substantial potential to support CT development, particularly when pedagogically integrated to promote active reasoning, metacognitive monitoring, and critical autonomy. However, the evidence base is still emerging and is limited by its short temporal scope, narrow database coverage, and reliance on self-reported data. Future research should focus on long-term effects, discipline-specific instructional models, and robust theoretical frameworks linking AI use to cognitive outcomes.",https://www.semanticscholar.org/paper/3d623dcd06d21cca9e2af6c7454fb81886dbf190,2025,3,Cluster 3: Education & Productivity,0.7599
3,GPT-4 as a homework tutor can improve student engagement and learning outcomes,"This work contributes to the scarce empirical literature on LLM-based interactive homework in real-world educational settings and offers a practical, scalable solution for improving homework in schools. Homework is an important part of education in schools across the world, but in order to maximize benefit, it needs to be accompanied with feedback and followup questions. We developed a prompting strategy that enables GPT-4 to conduct interactive homework sessions for high-school students learning English as a second language. Our strategy requires minimal efforts in content preparation, one of the key challenges of alternatives like home tutors or ITSs. We carried out a Randomized Controlled Trial (RCT) in four high-school classes, replacing traditional homework with GPT-4 homework sessions for the treatment group. We observed significant improvements in learning outcomes, specifically a greater gain in grammar, and student engagement. In addition, students reported high levels of satisfaction with the system and wanted to continue using it after the end of the RCT.",https://www.semanticscholar.org/paper/c659e7ae5bf4a848a60cf4fa95bad24bea23c6e0,2025,3,Cluster 3: Education & Productivity,0.7141
3,"Empowering ChatGPT with guidance mechanism in blended learning: Effect of self-regulated learning, higher-order thinking skills, and knowledge construction","In the evolving landscape of higher education, challenges such as the COVID-19 pandemic have underscored the necessity for innovative teaching methodologies. These challenges have catalyzed the integration of technology into education, particularly in blended learning environments, to bolster self-regulated learning (SRL) and higher-order thinking skills (HOTS). However, increased autonomy in blended learning can lead to learning disruptions if issues are not promptly addressed. In this context, OpenAI's ChatGPT, known for its extensive knowledge base and immediate feedback capability, emerges as a significant educational resource. Nonetheless, there are concerns that students might become excessively dependent on such tools, potentially hindering their development of HOTS. To address these concerns, this study introduces the Guidance-based ChatGPT-assisted Learning Aid (GCLA). This approach modifies the use of ChatGPT in educational settings by encouraging students to attempt problem-solving independently before seeking ChatGPT assistance. When engaged, the GCLA provides guidance through hints rather than direct answers, fostering an environment conducive to the development of SRL and HOTS. A randomized controlled trial (RCT) was employed to examine the impact of the GCLA compared to traditional ChatGPT use in a foundational chemistry course within a blended learning setting. This study involved 61 undergraduate students from a university in Taiwan. The findings reveal that the GCLA enhances SRL, HOTS, and knowledge construction compared to traditional ChatGPT use. These results directly align with the research objective to improve learning outcomes through providing guidance rather than answers by ChatGPT. In conclusion, the introduction of the GCLA has not only facilitated more effective learning experiences in blended learning environments but also ensured that students engage more actively in their educational journey. The implications of this study highlight the potential of ChatGPT-based tools in enhancing the quality of higher education, particularly in fostering essential skills such as self-regulation and HOTS. Furthermore, this research offers insights regarding the more effective use of ChatGPT in education.",https://www.semanticscholar.org/paper/364a69991cfd4fa6431d7cb6f4b850a3fb999f3b,2024,3,Cluster 3: Education & Productivity,0.6463
3,Exploring the landscape of generative AI (ChatGPT)-powered writing instruction in English as a foreign language education: A scoping review,"Recent advancements in artificial intelligence (AI)-powered technologies, particularly ChatGPT, have sparked significant interest in English as a Foreign Language (EFL) education. This review aims to explore the landscape of ChatGPT's application in EFL writing. Given the nascence of this field, the study conducts a scoping review by analyzing 16 empirical studies published before December 2023 to investigate the role of ChatGPT in EFL writing. The review explores the current and potential uses of ChatGPT in EFL writing, highlighting its dual role as both a writing assistant and an assessment tool. On one hand, ChatGPT is widely acknowledged for providing real-time feedback that enhances writing quality and efficiency. On the other hand, challenges and concerns remain prevalent. The findings reveal key gaps in the literature, such as the need for more interdisciplinary research, the adaptation of AI models to meet the linguistic and cultural needs of EFL learners, and the integration of multimodal AI tools. The review emphasizes the importance of critical thinking and information literacy training for educators and students while addressing ethical considerations. These insights offer a roadmap for future research and the practical implementation of AI in language education, providing valuable guidance for different stakeholders.",https://www.semanticscholar.org/paper/9c2dcb66fbf8735541b0ec2ba2b0bd4daa7bd9a1,2025,3,Cluster 3: Education & Productivity,0.7934
3,"Outcomes, perceptions, and interaction strategies of novice programmers studying with ChatGPT","Large Language Model (LLM) conversational agents are increasingly used in programming education, yet we still lack insight into how novices engage with them for conceptual learning compared with human tutoring. This mixed‑methods study compared learning outcomes and interaction strategies of novices using ChatGPT or human tutors. A controlled lab study with 20 students enrolled in introductory programming courses revealed that students employ markedly different interaction strategies with AI versus human tutors: ChatGPT users relied on brief, zero‑shot prompts and received lengthy, context‑rich responses but showed minimal prompt refinement, while those working with human tutors provided more contextual information and received targeted explanations. Although students distrusted ChatGPT’s accuracy, they paradoxically preferred it for basic conceptual questions due to reduced social anxiety. We offer empirically grounded recommendations for developing AI literacy in computer science education and designing learning‑focused conversational agents that balance trust‑building with maintaining the social safety that facilitates uninhibited inquiry.",https://www.semanticscholar.org/paper/03118033376626c8acc2afa91266a4e78c778f77,2025,3,Cluster 3: Education & Productivity,0.7192
3,"Herald of Advancement, Disruption, or Both: A Systematic Literature Review on Student-Facing LLM Tools in Undergraduate Computing Education","Grade and Subject We categorized the papers into two groups based on their user study   empirical evidence on how LLM-driven tools impact student learning outcomes, behaviors, and",https://www.techrxiv.org/doi/full/10.36227/techrxiv.176463808.80840600,NA,3,Cluster 3: Education & Productivity,0.5154
3,The impact of a large language model-based programming learning environment on students' motivation and programming ability,"These constraints can impede the effective delivery of programming education and realizing   (2023b), which was developed through a comprehensive user study and expert validation.",https://www.semanticscholar.org/paper/5daf5bc9ba265a63361c8dcaba9575b5e5f3c6a2,2025,3,Cluster 3: Education & Productivity,0.5513
3,Assessment of the LLM-based Chatbots on Student Engagement and Learning Outcomes in Afghanistan,"The integration of Generative AI (GenAI) technologies, such as ChatGPT, into online education is accelerating; however, their effectiveness in under-resourced contexts remains insufficiently studied. This paper investigates the impact of a Large Language Model (LLM)-based conversational agent on student engagement and learning outcomes in Afghanistan, where access to formal education—particularly for women—is severely restricted or banned. We conducted an experimental study involving 80 undergraduate computer science students (40 male, 40 female) in Afghanistan, randomly assigned to control and treatment groups. All participants attended identical 50-minute online lectures followed by 40-minute post-lecture discussions moderated by a human instructor, and completed a follow-up self-report questionnaire. The treatment group additionally engaged in AI-facilitated discussions using a GPT-4-based chatbot during post-lecture discussion. Analysis of discussion logs and post-intervention surveys revealed that the treatment group demonstrated significantly higher participation rates, with more posts and replies, during post-lecture discussion and reported greater confidence in their understanding of the course material. These findings highlight the potential of LLM-based chatbots to enhance interactive learning and foster educational inclusion, particularly for marginalized populations in low-resource environments.",https://www.semanticscholar.org/paper/27f5277f0fc950168b9dbc648c6d50f80e95c925,2025,3,Cluster 3: Education & Productivity,0.7988
