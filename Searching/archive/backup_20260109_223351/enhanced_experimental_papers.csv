search_cluster,title,abstract,url,year,predicted_cluster,predicted_label,confidence
0,Customizing emotional support: How do individuals construct and interact with LLM-powered chatbots,"Personalized support is essential to fulfill individuals’ emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.",https://www.semanticscholar.org/paper/a4b5e544c8bb18f844f2c8836d3a8c5f8fc9ffc7,2025,0,Cluster 0: Social & Collaboration,0.8111
0,Utilizing artificial intelligence to enhance social connections–the alleviating effect of emotionally intelligent chatbots on loneliness,", but also provides new ideas and methods for solving the mental health problems faced  by contemporary youth. Recent studies indicate that generative AI tutoring can enhance",https://www.tandfonline.com/doi/abs/10.1080/17483107.2025.2540494,2025,0,Cluster 0: Social & Collaboration,0.7362
0,Artificial intelligence chatbots as a source of virtual social support: Implications for loneliness and anxiety management,"Loneliness, social isolation, and anxiety affect millions of people across the world. Communication technologies, including artificial intelligence (AI) chatbots, can potentially offer support to those experiencing mental health challenges by providing companionship and support. Specifically, social AI can mimic human interaction, which may help alleviate loneliness and anxiety through person‐centered messaging. Despite growing AI usage, there is limited research on the effectiveness of specific message types in this context. Thus, this study employed a 2 (person‐centered message: high vs. low) × 2 (context: loneliness vs. anxiety) between‐subjects design to test how different supportive messages from social AI chatbots impact subsequent outcomes. Results revealed that high person‐centered messages are associated with increased emotional validation. Furthermore, the quality of social support and interpersonal warmth (IW) mediated the relationship between high person‐centered messages and emotional validation. Finally, the mediation effect between high person‐centered messages and emotional validation via the quality of emotional support was moderated by social presence, but not the mediation effect between high person‐centered messages and emotional validation via IW. These results demonstrate the importance of developing social AI chatbots that employ messages high in person‐centeredness, as these messages are most important for addressing mental health concerns.",https://www.semanticscholar.org/paper/df71cde0ba65a75523ae673fbe91c7b6292b958f,2025,0,Cluster 0: Social & Collaboration,0.8194
0,The role of ChatGPT in mitigating loneliness among older adults: An exploratory study,"Purpose: This exploratory study aims to investigate the potential of ChatGPT in mitigating loneliness among older adults.
Design/methodology/approach: 20 participants aged 60 and above engaged in three conversational sessions with ChatGPT over two weeks. Data collection involved pre- and post-intervention assessments using the UCLA Loneliness Scale, analysis of conversation transcripts, and semi-structured interviews.
Findings: Our findings indicate that ChatGPT shows promise in alleviating loneliness among older adults. Participants found the tool easy to use, engaging, and emotionally supportive. They established an emotional connection with ChatGPT, suggesting its potential to provide comfort and companionship to those experiencing loneliness.
Conclusion: ChatGPT demonstrates potential as a tool to address loneliness in older adults, offering emotional support and engagement. However, it should be viewed as a complement rather than a replacement for human interaction. Future research should explore its long-term efficacy and its integration with other interventions.",https://www.semanticscholar.org/paper/78754675c91b505914a26233976deaf3a6e3bed4,2024,0,Cluster 0: Social & Collaboration,0.5662
0,How AI and Human Behaviors Shape Psychosocial Effects of Extended Chatbot Use: A Longitudinal Randomized Controlled Study,"As people increasingly seek emotional support and companionship from AI chatbots, understanding how such interactions impact mental well-being becomes critical. We conducted a four-week randomized controlled experiment (n=981,>300k messages) to investigate how interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence four psychosocial outcomes: loneliness, social interaction with real people, emotional dependence on AI, and problematic AI usage. No significant effects were detected from experimental conditions, despite conversation analyses revealing differences in AI and human behavioral patterns across the conditions. Instead, participants who voluntarily used the chatbot more, regardless of assigned condition, showed consistently worse outcomes. Individuals'characteristics, such as higher trust and social attraction towards the AI chatbot, are associated with higher emotional dependence and problematic use. These findings raise deeper questions about how artificial companions may reshape the ways people seek, sustain, and substitute human connections.",https://www.semanticscholar.org/paper/fea512e6abaf780cffcb240e77aa0f285f60239e,2025,0,Cluster 0: Social & Collaboration,0.8879
0,What People Share With a Robot When Feeling Lonely and Stressed and How It Helps Over Time,"Loneliness and stress are prevalent among young adults and are linked to significant psychological and health-related consequences. Social robots may offer a promising avenue for emotional support, especially when considering the ongoing advancements in conversational AI. This study investigates how repeated interactions with a social robot influence feelings of loneliness and perceived stress, and how such feelings are reflected in the themes of user disclosures towards the robot. Participants engaged in a five-session robot-led intervention, where a LLM-powered QTrobot facilitated structured conversations designed to support cognitive reappraisal. Results from linear mixed-effects models show significant reductions in both loneliness and perceived stress over time. Additionally, semantic clustering of 560 user disclosures towards the robot revealed six distinct conversational themes. Results from Kruskal-Wallis H-test demonstrate that participants reporting higher loneliness and stress, more frequently engaged in socially focused disclosures, such as friendship and connection, whereas lower distress was associated with introspective and goal-oriented themes (e.g., academic ambitions). By exploring both how the intervention affects well-being, as well as how well-being shapes the content of robot-directed conversations, we aim to capture the dynamic nature of emotional support in human–robot interaction.",https://www.semanticscholar.org/paper/5f3cbdc7b1f37473e0f9d66d7f2d22482a48c79c,2025,0,Cluster 0: Social & Collaboration,0.763
0,Therapeutic potential of social chatbots in alleviating loneliness and social anxiety: Quasi-experimental mixed methods study,"Background Artificial intelligence (AI) social chatbots represent a major advancement in merging technology with mental health, offering benefits through natural and emotional communication. Unlike task-oriented chatbots, social chatbots build relationships and provide social support, which can positively impact mental health outcomes like loneliness and social anxiety. However, the specific effects and mechanisms through which these chatbots influence mental health remain underexplored. Objective This study explores the mental health potential of AI social chatbots, focusing on their impact on loneliness and social anxiety among university students. The study seeks to (i) assess the impact of engaging with an AI social chatbot in South Korea, ""Luda Lee,"" on these mental health outcomes over a 4-week period and (ii) analyze user experiences to identify perceived strengths and weaknesses, as well as the applicability of social chatbots in therapeutic contexts. Methods A single-group pre-post study was conducted with university students who interacted with the chatbot for 4 weeks. Measures included loneliness, social anxiety, and mood-related symptoms such as depression, assessed at baseline, week 2, and week 4. Quantitative measures were analyzed using analysis of variance and stepwise linear regression to identify the factors affecting change. Thematic analysis was used to analyze user experiences and assess the perceived benefits and challenges of chatbots. Results A total of 176 participants (88 males, average age=22.6 (SD 2.92)) took part in the study. Baseline measures indicated slightly elevated levels of loneliness (UCLA Loneliness Scale, mean 27.97, SD (11.07)) and social anxiety (Liebowitz Social Anxiety Scale, mean 25.3, SD (14.19)) compared to typical university students. Significant reductions were observed as loneliness decreasing by week 2 (t175=2.55, P=.02) and social anxiety decreasing by week 4 (t175=2.67, P=.01). Stepwise linear regression identified baseline loneliness (β=0.78, 95% CI 0.67 to 0.89), self-disclosure (β=–0.65, 95% CI –1.07 to –0.23) and resilience (β=0.07, 95% CI 0.01 to 0.13) as significant predictors of week 4 loneliness (R2=0.64). Baseline social anxiety (β=0.92, 95% CI 0.81 to 1.03) significantly predicted week 4 anxiety (R2=0.65). These findings indicate higher baseline loneliness, lower self-disclosure to the chatbot, and higher resilience significantly predicted higher loneliness at week 4. Additionally, higher baseline social anxiety significantly predicted higher social anxiety at week 4. Qualitative analysis highlighted the chatbot's empathy and support as features for reliability, though issues such as inconsistent responses and excessive enthusiasm occasionally disrupted user immersion. Conclusions Social chatbots may have the potential to mitigate feelings of loneliness and social anxiety, indicating their possible utility as complementary resources in mental health interventions. User insights emphasize the importance of empathy, accessibility, and structured conversations in achieving therapeutic goals. Trial Registration Clinical Research Information Service (CRIS) KCT0009288; https://tinyurl.com/hxrznt3t",https://www.semanticscholar.org/paper/3e89ef095a7a2e239b29e116ac85c9750680d656,2025,0,Cluster 0: Social & Collaboration,0.8092
0,"Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support","robot (QTrobot; LuxAI) powered by a large language model (LLM; GPT 3.5). Using sentence   socio-emotional support (H2R). Rather than conducting high-risk user study prematurely,",https://arxiv.org/abs/2506.16473,2025,0,Cluster 0: Social & Collaboration,0.6695
0,… of a cognitive behavioral therapy–based AI chatbot on depression and loneliness in chinese university students: Randomized controlled trial with financial …,"Abstract Background Mental health concerns are prevalent among university students, with financial stress further compounding these issues. While cognitive behavioral therapy (CBT) is effective for these conditions, its delivery through artificial intelligence (AI) chatbots represents a promising approach, especially in non-Western contexts. Objective This study aims to investigate the efficacy of a culturally adapted, CBT-based AI chatbot for improving the well-being of Chinese university students and to examine whether financial stress moderates its effectiveness. Methods In this randomized controlled trial, 100 university students (mean age 20.8, SD 2.2 years; 62/100, 62% female) were allocated to either an intervention (n=50) or a waitlist control group (n=50). The intervention group interacted with a CBT-based AI chatbot for 7 consecutive days. Depression (Center for Epidemiologic Studies Depression Scale), anxiety (Generalized Anxiety Disorder-7 scale), and loneliness (UCLA Loneliness Scale) were assessed at baseline, day 3, and day 7. Financial stress was measured using the Psychological Inventory of Financial Scarcity. Results Significant group×time interactions were found for depression (F2,196=8.63; P<.001; η²p=.08) and loneliness (F2,196=5.57; P=.004; η²p=.05), but not for anxiety (F2,196=1.31; P=.27; η²p=.01). Post hoc comparisons showed significant reductions in both depression (t=3.85; P<.001) and loneliness (t=4.28; P<.001) from baseline to postintervention in the intervention group, with corresponding effect sizes of Cohen d=0.71 (95% CI 0.30‐1.12) and Cohen d=0.60 (95% CI 0.20‐1.00), respectively. No significant changes were observed in the waitlist control group. Exploratory subgroup analyses revealed that participants with high financial stress demonstrated significantly greater improvements in depression (F2,52=11.56; P<.001; η²p=.31) and loneliness (F2,52=11.18; P<.001; η²p=.30) compared to those with low financial stress. Conclusions The culturally adapted, CBT-based AI chatbot effectively reduced depression and loneliness in Chinese university students, with stronger effects among those experiencing high financial stress. These findings highlight the potential of AI-driven interventions to provide accessible mental health support, particularly for financially stressed students.",https://www.semanticscholar.org/paper/0c8d7490b69eaec2947a28d66f8806471ef71044,2025,0,Cluster 0: Social & Collaboration,0.6757
0,"The efficacy, feasibility, and technical outcomes of a GPT-4o-based chatbot Amanda for relationship support: a randomized controlled trial","This randomized controlled trial evaluated the efficacy, feasibility, and technical outcomes of Amanda, a GPT-4-based chatbot, in delivering single-session relationship interventions. A total of 258 participants were randomly assigned to engage with either Amanda (n = 130) or a writing task (n = 128) focused on conflict reappraisal. Findings demonstrated significant improvements across 13 of 14 outcome variables—including relationship satisfaction, communication patterns, dyadic coping, problem-specific confidence, and individual well-being—over time in both conditions. Improvements emerged immediately after the intervention and were sustained or continued to improve at the two-week follow-up. However, there were no significant group differences for most outcomes, suggesting that both interventions were comparably effective. One significant group-by-time interaction emerged: participants in the chatbot condition reported lower levels of the partner-demand/self-withdraw communication pattern immediately post-intervention. The writing condition was also associated with lower overall distress about the issue. Feasibility outcomes indicated strong participant engagement with Amanda. Usability was rated highly (M = 4.19/5), as were therapeutic skills (M = 3.99/5) and working alliance (M = 4.75/6). Technical evaluation of interaction transcripts supported these findings, with high coder agreement on Amanda’s empathy, therapeutic questioning, and coherence. However, limitations were noted: Amanda occasionally produced repetitive or generic responses and did not consistently identify potential safety concerns. Overall, results suggest that Amanda provides a feasible and effective single-session relationship intervention, comparable in impact to an evidence-based writing task. This study highlights the potential for large language model-based chatbots to deliver scalable, accessible relationship support. Future research should assess Amanda’s use in multi-session interventions, explore performance in clinical populations, and enhance risk detection capabilities to ensure safe deployment in real-world settings.",https://www.semanticscholar.org/paper/2ea2d6f0cac3ba51e9d91398876984cf2638e722,2025,0,Cluster 0: Social & Collaboration,0.721
1,Persuasion with large language models: a survey,"The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.",https://www.semanticscholar.org/paper/55ee883d33ce4e434495921d153f2fda536b154f,2024,1,Cluster 1: Psychology & Persuasion,0.7891
1,Deceptive explanations by large language models lead people to change their beliefs about misinformation more often than honest explanations,"Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and discredit true information. We examined the impact of deceptive AI generated explanations on individuals’ beliefs in a pre-registered online experiment with 11,780 observations from 589 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that logically invalid explanations are deemed less credible - diminishing the effects of deception. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.",https://www.semanticscholar.org/paper/18d7eb4b41df1647819cd465047a09201c9aff5e,2025,1,Cluster 1: Psychology & Persuasion,0.8177
1,A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies,"In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the""receive""and""accept""steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.",https://www.semanticscholar.org/paper/0e9d76c93819c94cf6e9d7356ef90eefe7f25258,2025,1,Cluster 1: Psychology & Persuasion,0.7827
1,Testing theories of political persuasion using AI,"Significance We leverage recent advances in AI in two original research studies regarding persuasion about controversial political issues in the United States. These studies make both methodological and substantive contributions to the study of persuasion. First, we demonstrate that large language models (LLMs) can be used to efficiently test theories of persuasion by overcoming many practical and logistical challenges faced by persuasion researchers. Second, we use LLMs to isolate and test the persuasive gains offered by two key hypothesized mechanisms of attitude change—message customization and elaboration. We show that LLMs are approximately equally persuasive across conditions and that LLM-generated messages with microtargeted customization and messages that promote elaboration through interaction are not clearly more persuasive than a generic message.",https://www.semanticscholar.org/paper/7387ec1e4a2a7b00e47fcccb406715cad1bc936d,2025,1,Cluster 1: Psychology & Persuasion,0.7978
1,More human than human: measuring ChatGPT political bias,"We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.",https://www.semanticscholar.org/paper/3d8a3517231643c1df79bc32c8c2664a4cba3a41,2024,1,Cluster 1: Psychology & Persuasion,0.7715
1,"Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts","Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.",https://www.semanticscholar.org/paper/873bc4aaa1f8430723221efcc6c6b44a7551b79d,2025,1,Cluster 1: Psychology & Persuasion,0.7909
1,The levers of political persuasion with conversational AI,"There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs'unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.",https://www.semanticscholar.org/paper/89a7bae8aac5ff4dd1fe31c20094d4610f878866,2025,1,Cluster 1: Psychology & Persuasion,0.8513
1,The levers of political persuasion with conversational artificial intelligence,"There are widespread fears that conversational artificial intelligence (AI) could soon exert unprecedented influence over human beliefs. In this work, in three large-scale experiments (N = 76,977 participants), we deployed 19 large language models (LLMs)-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. We show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51 and 27%, respectively-than from personalization or increasing model scale, which had smaller effects. We further show that these methods increased persuasion by exploiting LLMs' ability to rapidly access and strategically deploy information and that, notably, where they increased AI persuasiveness, they also systematically decreased factual accuracy.",https://www.semanticscholar.org/paper/113fc327c4a337208d4e7729c4f3e9f37b5edbc6,2025,1,Cluster 1: Psychology & Persuasion,0.8657
1,"Large language models are as persuasive as humans, but how? About the cognitive effort and moral-emotional language of LLM arguments","Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about how they do it. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuasion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.",https://www.semanticscholar.org/paper/415f33b88476688b5f375a8276919d51320f9dbc,2024,1,Cluster 1: Psychology & Persuasion,0.7699
1,Citations and trust in llm generated responses,"Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.",https://www.semanticscholar.org/paper/927451a9310f98f7fc1ea4bf0f4bbce30db3185a,2025,1,Cluster 1: Psychology & Persuasion,0.6615
2,Can LLM-Powered Multi-Agent Systems Augment Human Creativity? Evidence from Brainstorming Tasks,"This paper investigates whether LLM-powered multi-agent systems can effectively augment human creativity in collaborative brainstorming tasks. Traditional brainstorming methods face persistent challenges including evaluation apprehension and free riding, while existing LLM-based discussion systems suffer from premature convergence and homogeneous perspectives that limit creative exploration. To address these dual challenges, we develop a novel multi-agent brainstorming framework that integrates three key innovations: (1) an extended Issue-Based Information System (IBIS) that adds ""Theme"" nodes and ""Idea-to-Idea"" transformation paths to prevent premature convergence and enable systematic idea expansion; (2) dynamic role-playing where multiple distinct agent types each dynamically select from topic-specific personas to ensure diverse perspectives while maintaining coherent discussion structure; and (3) evidence-based information suggestion capabilities using web search to stimulate discussion with objective facts rather than subjective criticism. Our experimental evaluation employed a rigorous counter-balanced design comparing human-only, human-agent collaboration, and agent-only conditions across both general topics and domain-specific topics. Results suggest striking domain-dependent effectiveness. For general topics, agent collaboration appears to enhance human creativity, with improvements in originality scores and participants reporting increased agreement that diverse ideas were generated. BERT embedding visualization suggests enhanced semantic dispersion, indicating stronger divergent thinking as agent-assisted posts spread into previously unexplored conceptual territories. The system shows promise in addressing traditional brainstorming challenges: evaluation apprehension appears to decrease as participants report reduced social pressure, and free riding diminishes with the majority of participants selecting agent contributions as the most stimulating ideas. However, for domain-specific topics requiring specialized knowledge, agent effectiveness appears to diminish substantially. Human response rates to agent posts drop noticeably, while creativity metrics suggest decreases rather than improvements. Expert evaluation reveals that a substantial proportion of agent contributions on domain-specific topics are deemed irrelevant by domain knowledge holders, suggesting limitations in current LLM capabilities for specialized contexts where agents lack access to tacit organizational knowledge and domain expertise. This work contributes the first systematic evaluation of structured multi-agent systems for human creativity augmentation, demonstrating both significant potential and important considerations for system design. Our findings provide essential insights for developing effective human-AI collaborative creativity systems: while the combination of IBIS structure, dynamic role-playing, and information suggestion shows promise for general domains, future systems should consider domain-adaptive architectures with specialized knowledge integration to optimize effectiveness across diverse knowledge domains. While our preliminary findings suggest meaningful effects, larger-scale studies will be necessary to establish statistical significance and generalizability across diverse populations and contexts. These results establish initial design principles for next-generation collaborative intelligence systems that can appropriately balance human expertise with AI capabilities.",https://www.semanticscholar.org/paper/9c17fd5d1302bdc0bb7c66a1c767ab6d3b23a4bc,2025,2,Cluster 2: Creativity & Ideation,0.7445
2,Human‐AI Co‐Creativity: Does ChatGPT Make Us More Creative?,"Much has been made of the apparent capacity for creativity of generative AI. However, as research expands the knowledge base regarding the capabilities and performance of this technology, the prevailing view is shifting away from “AI is creative” and towards a more balanced model of Human‐AI co‐creativity. Nevertheless, even this paradigm may be impacted by untested assumptions: for example, that generative AI will boost human performance on idea generation tasks. To test that assumption this study primed subjects with lists of words purportedly either from a human or from ChatGPT, and of varying degrees of creativity. Subjects then completed the Divergent Association Task (DAT). The results of this study found no evidence of any difference in divergent thinking resulting from either the source of priming (Human/ChatGPT) or from the level of creativity of the priming (low, typical, and high), with one exception: a low‐creativity prime, believed to be from ChatGPT, resulted in lower scores on the DAT compared to other priming conditions. A subsequent regression analysis supported this result, finding only the perceived creativity of the prime to be a weak predictor of DAT scores (in addition to the expected trait of Openness). The consequences of these findings for Human‐AI co‐creativity are discussed.",https://www.semanticscholar.org/paper/b6ef399529e3b5c90f6adab9ce69ae2a4f04750e,2025,2,Cluster 2: Creativity & Ideation,0.8419
2,AI and the Future of Collaborative Work: Group Ideation with an LLM in a Virtual Canvas,"The introduction of generative AI into multi-user applications raises novel considerations for the future of collaborative work. How might collaborative work practices change? How might we incorporate generative AI into shared tools with users’ needs at the forefront? We examine these questions in the context of a remote team conducting ideation tasks – an example of collaborative work enabled by a shared digital workspace. We conducted a user study with 17 professionals experienced with virtual group ideation workshops. Our study examined their use of the Collaborative Canvas, a virtual canvas tool with integrated generative AI capabilities that we created as a probe. Participants saw value in using generative AI to assist with group facilitation and to augment perspectives and ideas. However, they worried about losing human perspectives and critical thinking, as well as reputational harms resulting from harmful AI outputs. Participants shared suggestions for appropriate ways to incorporate generative AI capabilities within multi-user applications and identified needs for transparency of content ownership, private digital spaces, and specialized AI capabilities. Based on participants’ insights, we share implications and opportunities for the incorporation of generative AI into collaborative work in ways that place user needs at the forefront.",https://www.semanticscholar.org/paper/ee4c1921d60daceec5c02e1e15a157060aa33422,2024,2,Cluster 2: Creativity & Ideation,0.64
2,Supermind ideator: How scaffolding human-AI collaboration can increase creativity,"Supermind Ideator that uses a large language model (LLM) and adds prompts,  participants  copied and pasted responses directly from the two treatment systems (Ideator and ChatGPT",https://dl.acm.org/doi/abs/10.1145/3643562.3672611,2024,2,Cluster 2: Creativity & Ideation,0.809
2,AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation,"the use of AI to creativity in drawing [8], and in this work we explore using LLM to evaluate  the  Participants are encouraged to iterate on their LLM prompts and are exposed, prior to the",https://dl.acm.org/doi/abs/10.1145/3613904.3642414,2024,2,Cluster 2: Creativity & Ideation,0.74
2,Human creativity in the age of llms: Randomized experiments on divergent and convergent thinking,"Our findings reveal that while LLM assistance can provide short-term boosts in creativity   experimental conditions, participants exhibited a reduction in selfreported creativity ratings from",https://dl.acm.org/doi/abs/10.1145/3706598.3714198,2025,2,Cluster 2: Creativity & Ideation,0.746
2,An empirical investigation of the impact of ChatGPT on creativity,"Across five experiments, we asked participants to use ChatGPT ( We found that using  ChatGPT increased the creativity of the  Furthermore, ChatGPT was most effective at generating",https://www.semanticscholar.org/paper/c403fa6edfd563431f0692c749dedbd364e734fe,2024,2,Cluster 2: Creativity & Ideation,0.7338
2,How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent,"In this study, we propose and examine a new agent LLM  LLM-based co-creation systems  can help facilitate the process of information gathering and idea evolution using user study",https://dl.acm.org/doi/abs/10.1145/3613904.3642698,2024,2,Cluster 2: Creativity & Ideation,0.7135
2,The Roles of Idea Generation and Elaboration in Human-AI Collaborative Creativity,"When provided AI assistance, we allowed participants to freely interact with the LLM chatbot,  thus allowing us to investigate whether any property of these interactions may be driving",https://www.researchgate.net/profile/Simone-Luchini/publication/392122488_The_Roles_of_Idea_Generation_and_Elaboration_in_Human-AI_Collaborative_Creativity/links/684412196a754f72b5910290/The-Roles-of-Idea-Generation-and-Elaboration-in-Human-AI-Collaborative-Creativity.pdf,2025,2,Cluster 2: Creativity & Ideation,0.7857
2,When AI Joins the Brainstorm: Impacts of Generative Language Models on Collaborative Divergent Thinking,"the collaboration, participants’ experiences, and creativity of outcomes  OCSAI is a Large  Language Model (LLM) fine-tuned on  of creativity assessment, particularly in divergent thinking",https://aisel.aisnet.org/icis2025/gen_ai/gen_ai/24/,2025,2,Cluster 2: Creativity & Ideation,0.8044
3,"Integrating large language models into EFL writing instruction: effects on performance, self-regulated learning strategies, and motivation","randomized controlled trial, 65 elementary school students were divided into an experimental  group receiving CALLA-LLM  CALLA-LLM model embodies this approach, integrating LLM",https://www.semanticscholar.org/paper/d502564f6cf919013da1300489f002331bc6e156,2024,3,Cluster 3: Education & Productivity,0.6252
3,Enhancing university level English proficiency with generative AI: Empirical insights into automated feedback and learning outcomes,"This paper investigates the effects of large language model (LLM) based feedback on the essay writing proficiency of university students in Hong Kong. It focuses on exploring the potential improvements that generative artificial intelligence (AI) can bring to student essay revisions, its effect on student engagement with writing tasks, and the emotions students experience while undergoing the process of revising written work. Utilizing a randomized controlled trial, it draws comparisons between the experiences and performance of 918 language students at a Hong Kong university, some of whom received generated feedback (GPT-3.5-turbo LLM) and some of whom did not. The impact of AI-generated feedback is assessed not only through quantifiable metrics, entailing statistical analysis of the impact of AI feedback on essay grading, but also through subjective indices, student surveys that captured motivational levels and emotional states, as well as thematic analysis of interviews with participating students. The incorporation of AI-generated feedback into the revision process demonstrated significant improvements in the caliber of students’ essays. The quantitative data suggests notable effect sizes of statistical significance, while qualitative feedback from students highlights increases in engagement and motivation as well as a mixed emotional experience during revision among those who received AI feedback.",https://www.semanticscholar.org/paper/717670161041dac08825fd6c9e4de168ad05bf36,2024,3,Cluster 3: Education & Productivity,0.8243
3,GPT-4 as a homework tutor can improve student engagement and learning outcomes,"This work contributes to the scarce empirical literature on LLM-based interactive homework in real-world educational settings and offers a practical, scalable solution for improving homework in schools. Homework is an important part of education in schools across the world, but in order to maximize benefit, it needs to be accompanied with feedback and followup questions. We developed a prompting strategy that enables GPT-4 to conduct interactive homework sessions for high-school students learning English as a second language. Our strategy requires minimal efforts in content preparation, one of the key challenges of alternatives like home tutors or ITSs. We carried out a Randomized Controlled Trial (RCT) in four high-school classes, replacing traditional homework with GPT-4 homework sessions for the treatment group. We observed significant improvements in learning outcomes, specifically a greater gain in grammar, and student engagement. In addition, students reported high levels of satisfaction with the system and wanted to continue using it after the end of the RCT.",https://www.semanticscholar.org/paper/c659e7ae5bf4a848a60cf4fa95bad24bea23c6e0,2025,3,Cluster 3: Education & Productivity,0.7141
3,"Empowering ChatGPT with guidance mechanism in blended learning: Effect of self-regulated learning, higher-order thinking skills, and knowledge construction","In the evolving landscape of higher education, challenges such as the COVID-19 pandemic have underscored the necessity for innovative teaching methodologies. These challenges have catalyzed the integration of technology into education, particularly in blended learning environments, to bolster self-regulated learning (SRL) and higher-order thinking skills (HOTS). However, increased autonomy in blended learning can lead to learning disruptions if issues are not promptly addressed. In this context, OpenAI's ChatGPT, known for its extensive knowledge base and immediate feedback capability, emerges as a significant educational resource. Nonetheless, there are concerns that students might become excessively dependent on such tools, potentially hindering their development of HOTS. To address these concerns, this study introduces the Guidance-based ChatGPT-assisted Learning Aid (GCLA). This approach modifies the use of ChatGPT in educational settings by encouraging students to attempt problem-solving independently before seeking ChatGPT assistance. When engaged, the GCLA provides guidance through hints rather than direct answers, fostering an environment conducive to the development of SRL and HOTS. A randomized controlled trial (RCT) was employed to examine the impact of the GCLA compared to traditional ChatGPT use in a foundational chemistry course within a blended learning setting. This study involved 61 undergraduate students from a university in Taiwan. The findings reveal that the GCLA enhances SRL, HOTS, and knowledge construction compared to traditional ChatGPT use. These results directly align with the research objective to improve learning outcomes through providing guidance rather than answers by ChatGPT. In conclusion, the introduction of the GCLA has not only facilitated more effective learning experiences in blended learning environments but also ensured that students engage more actively in their educational journey. The implications of this study highlight the potential of ChatGPT-based tools in enhancing the quality of higher education, particularly in fostering essential skills such as self-regulation and HOTS. Furthermore, this research offers insights regarding the more effective use of ChatGPT in education.",https://www.semanticscholar.org/paper/364a69991cfd4fa6431d7cb6f4b850a3fb999f3b,2024,3,Cluster 3: Education & Productivity,0.6463
3,Exploring the landscape of generative AI (ChatGPT)-powered writing instruction in English as a foreign language education: A scoping review,"Recent advancements in artificial intelligence (AI)-powered technologies, particularly ChatGPT, have sparked significant interest in English as a Foreign Language (EFL) education. This review aims to explore the landscape of ChatGPT's application in EFL writing. Given the nascence of this field, the study conducts a scoping review by analyzing 16 empirical studies published before December 2023 to investigate the role of ChatGPT in EFL writing. The review explores the current and potential uses of ChatGPT in EFL writing, highlighting its dual role as both a writing assistant and an assessment tool. On one hand, ChatGPT is widely acknowledged for providing real-time feedback that enhances writing quality and efficiency. On the other hand, challenges and concerns remain prevalent. The findings reveal key gaps in the literature, such as the need for more interdisciplinary research, the adaptation of AI models to meet the linguistic and cultural needs of EFL learners, and the integration of multimodal AI tools. The review emphasizes the importance of critical thinking and information literacy training for educators and students while addressing ethical considerations. These insights offer a roadmap for future research and the practical implementation of AI in language education, providing valuable guidance for different stakeholders.",https://www.semanticscholar.org/paper/9c2dcb66fbf8735541b0ec2ba2b0bd4daa7bd9a1,2025,3,Cluster 3: Education & Productivity,0.7934
3,"Outcomes, perceptions, and interaction strategies of novice programmers studying with ChatGPT","Large Language Model (LLM) conversational agents are increasingly used in programming education, yet we still lack insight into how novices engage with them for conceptual learning compared with human tutoring. This mixed‑methods study compared learning outcomes and interaction strategies of novices using ChatGPT or human tutors. A controlled lab study with 20 students enrolled in introductory programming courses revealed that students employ markedly different interaction strategies with AI versus human tutors: ChatGPT users relied on brief, zero‑shot prompts and received lengthy, context‑rich responses but showed minimal prompt refinement, while those working with human tutors provided more contextual information and received targeted explanations. Although students distrusted ChatGPT’s accuracy, they paradoxically preferred it for basic conceptual questions due to reduced social anxiety. We offer empirically grounded recommendations for developing AI literacy in computer science education and designing learning‑focused conversational agents that balance trust‑building with maintaining the social safety that facilitates uninhibited inquiry.",https://www.semanticscholar.org/paper/03118033376626c8acc2afa91266a4e78c778f77,2025,3,Cluster 3: Education & Productivity,0.7192
3,"Herald of Advancement, Disruption, or Both: A Systematic Literature Review on Student-Facing LLM Tools in Undergraduate Computing Education","Grade and Subject We categorized the papers into two groups based on their user study   empirical evidence on how LLM-driven tools impact student learning outcomes, behaviors, and",https://www.techrxiv.org/doi/full/10.36227/techrxiv.176463808.80840600,NA,3,Cluster 3: Education & Productivity,0.5154
3,The impact of a large language model-based programming learning environment on students' motivation and programming ability,"These constraints can impede the effective delivery of programming education and realizing   (2023b), which was developed through a comprehensive user study and expert validation.",https://www.semanticscholar.org/paper/5daf5bc9ba265a63361c8dcaba9575b5e5f3c6a2,2025,3,Cluster 3: Education & Productivity,0.5513
3,Assessment of the LLM-based Chatbots on Student Engagement and Learning Outcomes in Afghanistan,"The integration of Generative AI (GenAI) technologies, such as ChatGPT, into online education is accelerating; however, their effectiveness in under-resourced contexts remains insufficiently studied. This paper investigates the impact of a Large Language Model (LLM)-based conversational agent on student engagement and learning outcomes in Afghanistan, where access to formal education—particularly for women—is severely restricted or banned. We conducted an experimental study involving 80 undergraduate computer science students (40 male, 40 female) in Afghanistan, randomly assigned to control and treatment groups. All participants attended identical 50-minute online lectures followed by 40-minute post-lecture discussions moderated by a human instructor, and completed a follow-up self-report questionnaire. The treatment group additionally engaged in AI-facilitated discussions using a GPT-4-based chatbot during post-lecture discussion. Analysis of discussion logs and post-intervention surveys revealed that the treatment group demonstrated significantly higher participation rates, with more posts and replies, during post-lecture discussion and reported greater confidence in their understanding of the course material. These findings highlight the potential of LLM-based chatbots to enhance interactive learning and foster educational inclusion, particularly for marginalized populations in low-resource environments.",https://www.semanticscholar.org/paper/27f5277f0fc950168b9dbc648c6d50f80e95c925,2025,3,Cluster 3: Education & Productivity,0.7988
3,Generative AI and Essay Writing: Impacts of Automated Feedback on Revision Performance and Engagement.,"This study investigates the impact of feedback generated by large language models (LLMs) on improving the essay-writing skills of first-year university students in Hong Kong. Specifically, it examines how generative AI supports students in revising their essays, enhances engagement with writing tasks, and influences their emotional responses during the revision process. The study followed a randomized controlled trial design, with one group of students receiving AI-generated feedback on their essay drafts while a control group did not. A mixed-methods approach was used to evaluate the feedback's effectiveness, combining statistical analysis of essay grades with student surveys and interviews. Quantitative results demonstrated that students who received AI feedback achieved significant improvements in essay quality, while qualitative findings revealed higher levels of engagement, increased motivation, and mixed emotional responses to the feedback process. These findings highlight the potential of generative AI as a tool for enhancing essay revision performance and fostering student engagement in higher education. However, further research is needed to explore its long-term impacts and applicability across diverse educational contexts.",https://www.semanticscholar.org/paper/635ff30b252db4765fde61d0fb20459def4d43d6,2024,3,Cluster 3: Education & Productivity,0.8487
